{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#大纲\" data-toc-modified-id=\"大纲-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>大纲</a></span></li><li><span><a href=\"#为什么集成方法有效\" data-toc-modified-id=\"为什么集成方法有效-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>为什么集成方法有效</a></span><ul class=\"toc-item\"><li><span><a href=\"#一个类比（投票）\" data-toc-modified-id=\"一个类比（投票）-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>一个类比（投票）</a></span></li><li><span><a href=\"#一个例子（信号编码传输）\" data-toc-modified-id=\"一个例子（信号编码传输）-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>一个例子（信号编码传输）</a></span></li></ul></li><li><span><a href=\"#集成方法\" data-toc-modified-id=\"集成方法-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>集成方法</a></span><ul class=\"toc-item\"><li><span><a href=\"#提交结果的集成\" data-toc-modified-id=\"提交结果的集成-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>提交结果的集成</a></span><ul class=\"toc-item\"><li><span><a href=\"#投票集成(Voting-ensemble)\" data-toc-modified-id=\"投票集成(Voting-ensemble)-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>投票集成(Voting ensemble)</a></span><ul class=\"toc-item\"><li><span><a href=\"#投票集成的基本思路\" data-toc-modified-id=\"投票集成的基本思路-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>投票集成的基本思路</a></span></li><li><span><a href=\"#代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_vote.py)\" data-toc-modified-id=\"代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_vote.py)-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>代码展示(<a href=\"https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_vote.py\" target=\"_blank\">https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_vote.py</a>)</a></span></li><li><span><a href=\"#适用问题\" data-toc-modified-id=\"适用问题-3.1.1.3\"><span class=\"toc-item-num\">3.1.1.3&nbsp;&nbsp;</span>适用问题</a></span></li></ul></li><li><span><a href=\"#算术平均数集成(Arithmetic-mean-based-ensemble)\" data-toc-modified-id=\"算术平均数集成(Arithmetic-mean-based-ensemble)-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>算术平均数集成(Arithmetic mean based ensemble)</a></span><ul class=\"toc-item\"><li><span><a href=\"#算术平均数集成的基本思路\" data-toc-modified-id=\"算术平均数集成的基本思路-3.1.2.1\"><span class=\"toc-item-num\">3.1.2.1&nbsp;&nbsp;</span>算术平均数集成的基本思路</a></span></li><li><span><a href=\"#代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_avg.py)\" data-toc-modified-id=\"代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_avg.py)-3.1.2.2\"><span class=\"toc-item-num\">3.1.2.2&nbsp;&nbsp;</span>代码展示(<a href=\"https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_avg.py\" target=\"_blank\">https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_avg.py</a>)</a></span></li><li><span><a href=\"#适用问题\" data-toc-modified-id=\"适用问题-3.1.2.3\"><span class=\"toc-item-num\">3.1.2.3&nbsp;&nbsp;</span>适用问题</a></span></li></ul></li><li><span><a href=\"#几何平均数集成(Geometric-mean-based-ensemble)\" data-toc-modified-id=\"几何平均数集成(Geometric-mean-based-ensemble)-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>几何平均数集成(Geometric mean based ensemble)</a></span><ul class=\"toc-item\"><li><span><a href=\"#几何平均数集成的基本思路\" data-toc-modified-id=\"几何平均数集成的基本思路-3.1.3.1\"><span class=\"toc-item-num\">3.1.3.1&nbsp;&nbsp;</span>几何平均数集成的基本思路</a></span></li><li><span><a href=\"#代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_geomean.py)\" data-toc-modified-id=\"代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_geomean.py)-3.1.3.2\"><span class=\"toc-item-num\">3.1.3.2&nbsp;&nbsp;</span>代码展示(<a href=\"https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_geomean.py\" target=\"_blank\">https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_geomean.py</a>)</a></span></li><li><span><a href=\"#适用问题\" data-toc-modified-id=\"适用问题-3.1.3.3\"><span class=\"toc-item-num\">3.1.3.3&nbsp;&nbsp;</span>适用问题</a></span></li></ul></li><li><span><a href=\"#线上结果加权集成(Online-scroe-based-ensemble)\" data-toc-modified-id=\"线上结果加权集成(Online-scroe-based-ensemble)-3.1.4\"><span class=\"toc-item-num\">3.1.4&nbsp;&nbsp;</span>线上结果加权集成(Online scroe based ensemble)</a></span><ul class=\"toc-item\"><li><span><a href=\"#线上结果加权集成的基本思路\" data-toc-modified-id=\"线上结果加权集成的基本思路-3.1.4.1\"><span class=\"toc-item-num\">3.1.4.1&nbsp;&nbsp;</span>线上结果加权集成的基本思路</a></span></li><li><span><a href=\"#代码展示\" data-toc-modified-id=\"代码展示-3.1.4.2\"><span class=\"toc-item-num\">3.1.4.2&nbsp;&nbsp;</span>代码展示</a></span></li><li><span><a href=\"#适用问题\" data-toc-modified-id=\"适用问题-3.1.4.3\"><span class=\"toc-item-num\">3.1.4.3&nbsp;&nbsp;</span>适用问题</a></span></li></ul></li><li><span><a href=\"#排序均值集成(Rank-averaging-ensemble)\" data-toc-modified-id=\"排序均值集成(Rank-averaging-ensemble)-3.1.5\"><span class=\"toc-item-num\">3.1.5&nbsp;&nbsp;</span>排序均值集成(Rank averaging ensemble)</a></span><ul class=\"toc-item\"><li><span><a href=\"#排序均值集成的基本思路\" data-toc-modified-id=\"排序均值集成的基本思路-3.1.5.1\"><span class=\"toc-item-num\">3.1.5.1&nbsp;&nbsp;</span>排序均值集成的基本思路</a></span></li><li><span><a href=\"#代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_rankavg.py)\" data-toc-modified-id=\"代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_rankavg.py)-3.1.5.2\"><span class=\"toc-item-num\">3.1.5.2&nbsp;&nbsp;</span>代码展示(<a href=\"https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_rankavg.py\" target=\"_blank\">https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_rankavg.py</a>)</a></span></li><li><span><a href=\"#适用问题\" data-toc-modified-id=\"适用问题-3.1.5.3\"><span class=\"toc-item-num\">3.1.5.3&nbsp;&nbsp;</span>适用问题</a></span></li></ul></li><li><span><a href=\"#log集成变种(log-ensemble-version2)\" data-toc-modified-id=\"log集成变种(log-ensemble-version2)-3.1.6\"><span class=\"toc-item-num\">3.1.6&nbsp;&nbsp;</span>log集成变种(log ensemble version2)</a></span><ul class=\"toc-item\"><li><span><a href=\"#log集成变种\" data-toc-modified-id=\"log集成变种-3.1.6.1\"><span class=\"toc-item-num\">3.1.6.1&nbsp;&nbsp;</span>log集成变种</a></span></li><li><span><a href=\"#适用问题\" data-toc-modified-id=\"适用问题-3.1.6.2\"><span class=\"toc-item-num\">3.1.6.2&nbsp;&nbsp;</span>适用问题</a></span></li></ul></li></ul></li><li><span><a href=\"#Stacking/Blending\" data-toc-modified-id=\"Stacking/Blending-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Stacking/Blending</a></span><ul class=\"toc-item\"><li><span><a href=\"#简单单层stacking方法(传统的方法Simple-stacking-with-logistic-regression/nonlinear-algorithm)\" data-toc-modified-id=\"简单单层stacking方法(传统的方法Simple-stacking-with-logistic-regression/nonlinear-algorithm)-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>简单单层stacking方法(传统的方法Simple stacking with logistic regression/nonlinear algorithm)</a></span><ul class=\"toc-item\"><li><span><a href=\"#单层stacking方法基本思路\" data-toc-modified-id=\"单层stacking方法基本思路-3.2.1.1\"><span class=\"toc-item-num\">3.2.1.1&nbsp;&nbsp;</span>单层stacking方法基本思路</a></span></li><li><span><a href=\"#代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/blend_proba.py)\" data-toc-modified-id=\"代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/blend_proba.py)-3.2.1.2\"><span class=\"toc-item-num\">3.2.1.2&nbsp;&nbsp;</span>代码展示(<a href=\"https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/blend_proba.py\" target=\"_blank\">https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/blend_proba.py</a>)</a></span></li><li><span><a href=\"#适用问题\" data-toc-modified-id=\"适用问题-3.2.1.3\"><span class=\"toc-item-num\">3.2.1.3&nbsp;&nbsp;</span>适用问题</a></span></li></ul></li><li><span><a href=\"#单层stacking方法(融入多项式等特征)\" data-toc-modified-id=\"单层stacking方法(融入多项式等特征)-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>单层stacking方法(融入多项式等特征)</a></span><ul class=\"toc-item\"><li><span><a href=\"#单层stacking方法(融入多项式等特征)\" data-toc-modified-id=\"单层stacking方法(融入多项式等特征)-3.2.2.1\"><span class=\"toc-item-num\">3.2.2.1&nbsp;&nbsp;</span>单层stacking方法(融入多项式等特征)</a></span></li><li><span><a href=\"#代码展示\" data-toc-modified-id=\"代码展示-3.2.2.2\"><span class=\"toc-item-num\">3.2.2.2&nbsp;&nbsp;</span>代码展示</a></span></li><li><span><a href=\"#适用问题\" data-toc-modified-id=\"适用问题-3.2.2.3\"><span class=\"toc-item-num\">3.2.2.3&nbsp;&nbsp;</span>适用问题</a></span></li></ul></li><li><span><a href=\"#两层stacking或者多层stacking\" data-toc-modified-id=\"两层stacking或者多层stacking-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>两层stacking或者多层stacking</a></span></li><li><span><a href=\"#stacking与其他技术的融合\" data-toc-modified-id=\"stacking与其他技术的融合-3.2.4\"><span class=\"toc-item-num\">3.2.4&nbsp;&nbsp;</span>stacking与其他技术的融合</a></span><ul class=\"toc-item\"><li><span><a href=\"#融入无监督特征\" data-toc-modified-id=\"融入无监督特征-3.2.4.1\"><span class=\"toc-item-num\">3.2.4.1&nbsp;&nbsp;</span>融入无监督特征</a></span></li><li><span><a href=\"#回归与分类的结合(Binning-技术)\" data-toc-modified-id=\"回归与分类的结合(Binning-技术)-3.2.4.2\"><span class=\"toc-item-num\">3.2.4.2&nbsp;&nbsp;</span>回归与分类的结合(Binning 技术)</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#补充说明\" data-toc-modified-id=\"补充说明-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>补充说明</a></span></li><li><span><a href=\"#参考文献：\" data-toc-modified-id=\"参考文献：-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>参考文献：</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  大纲\n",
    "在实际机器学习应用中,亦或最近热门的数据科学竞赛中,掌握基本的集成方法是所有数据科学家都必备的一门绝杀技能，尤其是在大数据竞赛中,几乎所有的选手在最后阶段都会使用集成方法.\n",
    "\n",
    "虽然集成方法并不是100%每次使用都会有很大的效果,但是从平时的比赛经验中,我们得到的一条结论是:**集成方法在绝大多数情况下都会带来或多或少的帮助,而这在比赛中,尤其是最终成绩相差不大的情况下,集成的方法往往会成为取胜的关键之一**.在不同类型的比赛中,我们也很难保证哪一种集成方法一定会比另外一种要好,评价的指标往往还得从线上的结果来看,只是说了解的集成方法越多一点,我们的胜率就会越高一些.\n",
    "\n",
    "本篇notebook我们把集成方法分为两大类进行阐述,第一类从提交的文件进行集成(可以认为是预测结果的集成);第二类则是stacking/blending技术,每一类都会介绍一些非常有效实用的集成技术.\n",
    "\n",
    "本篇notebook介绍顺序为:第二部分,我们会先用简单的例子直观的阐述集成方法有效的原因.之后我们会在第三部分分门别类重点介绍目前大部分比赛中经常采用的几种集成方法.第四部分我们会给出参考文献,大家如果感兴趣可以取相关的网站进行浏览阅读,最后我们会陈列出一些最新的集成技巧(很多都是脑洞打开的技巧,在特定比赛中却是能带来较大的提升). <br  /><br  />\n",
    "\n",
    "<font color = red>这一篇notebook中大部分内容都摘自网上一些非常好的分享资料,同时加入了我们在比赛中的一些汇总,参考的资料我们都会在第4部分进行汇总</font>.<br  /> <br  />\n",
    "\n",
    "这边我们重点介绍一些简单而又实用的集成技巧,当然后续会不断补充增加最新的集成方法,如果大家觉得本文中有未提及相关方法,欢迎随时在下面进行评论,抑或直接联系我们,非常感谢. <br  />\n",
    "<br  />\n",
    "\n",
    "<font color = red> **注意:此处我们所介绍的集成方法和大家在学习随机森林等模型中提到的集成略有不同之处,我们这边的集成是将大量的模型进行集成,而随机森林我们将其看做是一个模型.** </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  为什么集成方法有效\n",
    "\n",
    "集成方法,从字面的理解就是集大家之成,以大家共同的决定来进行最终的决策.而放到我们的问题中就是,集多个分类器的结果,给出我们最终的结论.\n",
    "\n",
    "## 一个类比（投票）\n",
    "\n",
    "一个非常简单的例子,就是选举制度,一个人是否有能力成为某个小团队的老大,往往会由一群人进行投票来表决,如果赞成票占50%以上,我们就认为有能力当选，否则认为不符合当选条件,这可以认为是集成方法的一个特例.每个人都是一个小小的分类器,而我们最终的决策就由所有分类器最终判定的结果.\n",
    "\n",
    "*为了方便加深理解,我们再例举一个信号传播的例子.*\n",
    "\n",
    "##  一个例子（信号编码传输）\n",
    "\n",
    "在信号编码的传输过程中,我们需要传输一堆01编码,但是传输的过程往往不会一帆风顺,有时会遇到一些干扰噪音,使得最终我们接收到的编码与我们当初传输的编码有一丝偏差,例如我们起初发送的编码为_010101_ 但是我们最终接受到的编码可能为 _010001_,有一位编码出现了错误. 那么如何解决这个问题了？\n",
    "\n",
    "**一个简单的思路就是\"集成\".**\n",
    "\n",
    "\n",
    "假设我们传输的真实信号编码为:\n",
    "<font color = blue>\n",
    "1110110011 <br />\n",
    "</font>\n",
    "\n",
    " 我们将同一编码分三次进行发送,最终我们接受到的信号编码为:\n",
    "\n",
    "<font color = blue>\n",
    "①:1010110011 <br />\n",
    "</font>\n",
    "\n",
    "<font color = blue>\n",
    "②:1110110011 <br />\n",
    "</font>\n",
    "\n",
    "<font color = blue>\n",
    "③:1110110011 <br />\n",
    "</font>\n",
    "\n",
    "\n",
    "我们发现接收到的三个编码或多或少都出现了一点小小的问题,例如①中第二位编码的接收就出错了.但是②③次的接收第二位是对的,所以很容易地我们就想到:如果在多次不同的接收过程中,对于某一特定位置的编码,我们接收到的结果中10次有8次或者9次都为某一个值,那么我们就有很大的信心确定该位的值.**(这个就是我们常说的投票机制的集成)**,而对于本问题,我们投票的结果就是:\n",
    "<font color = blue>\n",
    "1110110011 <br />\n",
    "</font>\n",
    "基本全部正确,在实际中,编码的传输只会在少数情况下出现错误,所以加上投票的机制,基本是可以保证最终的结果是99.9%的概率是没有问题的.集成方法是不是很有用？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  集成方法 \n",
    "<br />\n",
    "<font color = red> **再次提示:此处我们所介绍的集成方法和大家在学习随机森林等模型中提到的集成方法略有不同,我们这边的集成是将大量的模型进行集成,而随机森林 $\\underline{它的集成由内部的多棵决策树构成}$. 但此处我们将其看做是一个\"单模型\".** </font>\n",
    "\n",
    "本处关于集成方法的介绍我们会将其拆分为两大块进行介绍,一块是基于提交结果文件的集成(预测结果的集成);另外一块则是stacking/blending,每一块我们都会介绍一些非常有效的集成技术.此处我们将每一块拆分为两部分进行介绍,第一部分是简单的实现,第二部分则是相应的源代码展示(虽然很多集成方法现在已经有对应的包可以直接进行调用,不过为了方便大家理解,我们选择从Github上面将其拷贝到我们的notebook中来)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提交结果的集成\n",
    "关于提交结果的集成,我们主要介绍下面几种技术.\n",
    "- 3.1.1 投票集成(Voting ensemble)\n",
    "- 3.1.2 算术平均数集成(Arithmetic mean based ensemble)\n",
    "- 3.1.3 几何均值集成(Geometric mean based ensemble)\n",
    "- 3.1.4 线上结果加权集成(Online scroe based ensemble)\n",
    "- 3.1.5 排序均值集成(Rank averaging ensemble)\n",
    "- 3.1.6 log集成变种(log ensemble version2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  投票集成(Voting ensemble)\n",
    "#### 投票集成的基本思路\n",
    "\n",
    "在第二部分:为什么集成方法有效？中我们已经给出了一个投票集成方法的案例,而它的实现思路也十分的简单.\n",
    "\n",
    "**算法基本思路:**\n",
    "\n",
    "> **输入**:多个不同分类器的分类结果;<br />\n",
    "> **输出**:最终的集成结果 <br />\n",
    "> **基本步骤**:\n",
    ">    1. 统计每个样本每个预测结果(常见于分类问题)出现的次数;\n",
    ">    2. 将每个样本出现的次数最多的那一个(众数)作为我们最终的集成结果. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_vote.py)\n",
    "此处我们不再进行代码的分析,具体的语法大家可以参考python类的学习网站.(后面的代码如无必要也都不会分析)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from glob import glob\n",
    "import sys\n",
    "import re\n",
    "\n",
    "glob_files = sys.argv[1]\n",
    "loc_outfile = sys.argv[2]\n",
    "weights_strategy = \"uniform\"\n",
    "if len(sys.argv) == 4:\n",
    "    weights_strategy = sys.argv[3]\n",
    "\n",
    "def kaggle_bag(glob_files, loc_outfile, method=\"average\", weights=\"uniform\"):\n",
    "    pattern = re.compile(r\"(.)*_[w|W](\\d*)_[.]*\")\n",
    "    if method == \"average\":\n",
    "        scores = defaultdict(list)\n",
    "    with open(loc_outfile,\"wb\") as outfile:\n",
    "        #weight_list may be usefull using a different method\n",
    "        weight_list = [1]*len(glob(glob_files))\n",
    "        for i, glob_file in enumerate( glob(glob_files) ):\n",
    "            print(\"parsing:\", glob_file)\n",
    "        if weights == \"weighted\":\n",
    "            weight = pattern.match(glob_file)\n",
    "            if weight and weight.group(2):\n",
    "                print(\"Using weight: \",int(weight.group(2)))\n",
    "                weight_list[i] = weight_list[i]*int(weight.group(2))\n",
    "            else:\n",
    "                print(\"Using weight: 1\")\n",
    "        # sort glob_file by first column, ignoring the first line\n",
    "        lines = open(glob_file).readlines()\n",
    "        lines = [lines[0]] + sorted(lines[1:])\n",
    "        for e, line in enumerate( lines ):\n",
    "            if i == 0 and e == 0:\n",
    "                outfile.write(line)\n",
    "            if e > 0:\n",
    "                row = line.strip().split(\",\")\n",
    "            for l in range(1,weight_list[i]+1):\n",
    "                scores[(e,row[0])].append(row[1])\n",
    "    for j,k in sorted(scores):\n",
    "        outfile.write(\"%s,%s\\n\"%(k,Counter(scores[(j,k)]).most_common(1)[0][0]))\n",
    "    print(\"wrote to %s\"%loc_outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 适用问题\n",
    "该方法常见于<font color = red>分类问题,Top-K推荐等预测结果是hard prediction的情况</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 算术平均数集成(Arithmetic mean based ensemble)\n",
    "#### 算术平均数集成的基本思路\n",
    "\n",
    " **算术平均数**：设一组数据为$X_1，X_2，...，X_n$，则简单的算术平均数的计算公式为：$\\frac{x_1 + x_2 + ... + x_N}{N}$\n",
    "\n",
    "基于算术平均数的集成方法是在算法中用的最多的,因为它不仅简单,而且基本每次使用都有较大概率取得很好的效果.\n",
    "\n",
    "**算法基本思路:**\n",
    "\n",
    "> **输入**:多个不同分类器的分类结果;<br />\n",
    "> **输出**:最终的集成结果 <br />\n",
    "> **基本步骤**: <br />\n",
    ">      $~~~~$ 计算每个样本对应的所有结果的算术平均数,并将此每个样本的算术平均数作为我们最终的集成结果. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_avg.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "glob_files = sys.argv[1]\n",
    "loc_outfile = sys.argv[2]\n",
    "\n",
    "def kaggle_bag(glob_files, loc_outfile, method=\"average\", weights=\"uniform\"):\n",
    "    if method == \"average\":\n",
    "        scores = defaultdict(float)\n",
    "    with open(loc_outfile,\"wb\") as outfile:\n",
    "        for i, glob_file in enumerate( glob(glob_files) ):\n",
    "            print(\"parsing:\", glob_file)\n",
    "        # sort glob_file by first column, ignoring the first line\n",
    "        lines = open(glob_file).readlines()\n",
    "        lines = [lines[0]] + sorted(lines[1:])\n",
    "        for e, line in enumerate( lines ):\n",
    "            if i == 0 and e == 0:\n",
    "                outfile.write(line)\n",
    "            if e > 0:\n",
    "                row = line.strip().split(\",\")\n",
    "                scores[(e,row[0])] += float(row[1])\n",
    "    for j,k in sorted(scores):\n",
    "        outfile.write(\"%s,%f\\n\"%(k,scores[(j,k)]/(i+1)))\n",
    "    print(\"wrote to %s\"%loc_outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 适用问题\n",
    "该方法常见于<font color = red>回归问题,分类问题中预测结果为概率的情况</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 几何平均数集成(Geometric mean based ensemble)\n",
    "#### 几何平均数集成的基本思路\n",
    "\n",
    "**几何平均数**：设一组数据为$X_1，X_2，...，X_n$，简单的算术平均数的计算公式为：$\\sqrt[n]{X_1 X_2...X_n}$\n",
    "\n",
    "基于几何平均数的集成方法在算法中使用的还不是很多,很多选手的分享都还是基于均值的较多,但是在实际情况中,有些时候基于几何平均数的效果要稍好于基于算术平均数的集成.\n",
    "\n",
    "**算法基本思路:**\n",
    "\n",
    "> **输入**:多个不同分类器的分类结果;<br />\n",
    "> **输出**:最终的集成结果 <br />\n",
    "> **基本步骤**: <br />\n",
    ">      $~~~~$ 计算每个样本对应的所有结果的几何平均数,并将每个样本的算术平均数作为我们最终的集成结果.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_geomean.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import sys\n",
    "import math\n",
    "\n",
    "glob_files = sys.argv[1]\n",
    "loc_outfile = sys.argv[2]\n",
    "\n",
    "def kaggle_bag(glob_files, loc_outfile, method=\"average\", weights=\"uniform\"):\n",
    "    if method == \"average\":\n",
    "        scores = defaultdict(float)\n",
    "    with open(loc_outfile,\"wb\") as outfile:\n",
    "        for i, glob_file in enumerate( glob(glob_files) ):\n",
    "            print(\"parsing:\", glob_file)\n",
    "        # sort glob_file by first column, ignoring the first line\n",
    "        lines = open(glob_file).readlines()\n",
    "        lines = [lines[0]] + sorted(lines[1:])\n",
    "        for e, line in enumerate( lines ):\n",
    "            if i == 0 and e == 0:\n",
    "                outfile.write(line)\n",
    "            if e > 0:\n",
    "                row = line.strip().split(\",\")\n",
    "                if scores[(e,row[0])] == 0:\n",
    "                    scores[(e,row[0])] = 1\n",
    "                scores[(e,row[0])] *= float(row[1])\n",
    "    for j,k in sorted(scores):\n",
    "        outfile.write(\"%s,%f\\n\"%(k,math.pow(scores[(j,k)],1/(i+1))))\n",
    "    print(\"wrote to %s\"%loc_outfile) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 适用问题\n",
    "该方法常见于<font color = red>回归问题,分类问题中预测结果为概率的情况</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  线上结果加权集成(Online scroe based ensemble)\n",
    "\n",
    "<br />\n",
    "<font color=red>大家平时一直会说trust local CV,其实这是非常正确的一种做法,80%的实践中都是有效的,但是有一些情况却并非如此,有很多时序问题,变化非常快(而且不存在明显周期性),线上线下的分布可能差异较大,同时线上的测试集又是均匀采样的,那么这个时候拿线上作为验证集,也是一个选择.</font>\n",
    "\n",
    "#### 线上结果加权集成的基本思路\n",
    " \n",
    "线上结果加权集成的方法也是实战中80%概率都会有效果的一种方法,该方法可以理解为一种拟合线上分布的方法.这种方法在数据竞赛这一块基本所有的老选手都会首选的方法之一.在实际工业生产中如果要使用这种方法的话,也可以通过线下验证集的结果进行加权.\n",
    "\n",
    "**算法基本思路:**\n",
    "\n",
    "> **输入**:多个不同分类器的分类结果以及对应的线上成绩;<br />\n",
    "> **输出**:最终的集成结果 <br />\n",
    "> **基本步骤**: <br />\n",
    ">      $~~~~$ 通过每个结果的线上成绩进行加权,线上分数高的权重就高,线上成绩相对低的权重就低,最终将加权的结果作为最终的结果(需保证所有的结果的 $~~~~$ 权重之和为1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码展示\n",
    "略.\n",
    "\n",
    "#### 适用问题\n",
    "该方法常见于<font color = red>回归问题,分类问题中预测结果为概率的情况</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 排序均值集成(Rank averaging ensemble)\n",
    "####  排序均值集成的基本思路\n",
    " \n",
    " 排序均值集成的基本思路在很多AUC为指标的比赛中都取得了非常不错的成绩,使用相对顺序取代原先的概率值,例如:<br />\n",
    " \n",
    " 我们预测某件事情发生的概率为:<br />\n",
    " \n",
    " <font color = blue>\n",
    " 0.30015,0.30011,0.30012 <br />\n",
    " </font> \n",
    " 我们将其转化为对应的rank的值,这样更加能体现预测顺序的重要性.<br />\n",
    " <font color = blue>\n",
    " 3,1,2 <br />\n",
    " </font>\n",
    " \n",
    " ** 算法思路: **\n",
    "\n",
    "> **输入**:多个不同分类器的分类结果以及对应的线上成绩;<br />\n",
    "> **输出**:最终的集成结果 <br />\n",
    "> **基本步骤**: <br />\n",
    ">    $~~~~$ 1.对每个分类器中分类的概率进行排序,然后用每个样本排序之后的得到的排名的值(rank)作为新的结果;<br />\n",
    ">    $~~~~$ 2.对每个分类器的rank的值求算术均值作为最终的结果;     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/kaggle_rankavg.py) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "import sys\n",
    "\n",
    "glob_files = sys.argv[1]\n",
    "loc_outfile = sys.argv[2]\n",
    "\n",
    "def kaggle_bag(glob_files, loc_outfile):\n",
    "    with open(loc_outfile,\"wb\") as outfile:\n",
    "        all_ranks = defaultdict(list)\n",
    "    for i, glob_file in enumerate(glob(glob_files) ):\n",
    "        file_ranks = []\n",
    "        print(\"parsing:\", glob_file)\n",
    "        # sort glob_file by first column, ignoring the first line\n",
    "        lines = open(glob_file).readlines()\n",
    "        lines = [lines[0]] + sorted(lines[1:])\n",
    "        for e, line in enumerate(lines):\n",
    "            if e == 0 and i == 0:\n",
    "                outfile.write(line)\n",
    "            elif e > 0:\n",
    "                r = line.strip().split(\",\")\n",
    "                file_ranks.append((float(r[1]), e, r[0]) )\n",
    "        for rank, item in enumerate(sorted(file_ranks) ):\n",
    "            all_ranks[(item[1],item[2])].append(rank)\n",
    "            \n",
    "    average_ranks = []\n",
    "    for k in sorted(all_ranks):\n",
    "        average_ranks.append((sum(all_ranks[k])/len(all_ranks[k]),k))\n",
    "    ranked_ranks = []\n",
    "    for rank, k in enumerate(sorted(average_ranks)):\n",
    "        ranked_ranks.append((k[1][0],k[1][1],rank/(len(average_ranks)-1)))\n",
    "    for k in sorted(ranked_ranks):\n",
    "        outfile.write(\"%s,%s\\n\"%(k[1],k[2]))\n",
    "    print(\"wrote to %s\"%loc_outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 适用问题\n",
    "该方法常见于<font color = red>结果的评估指标与预测结果顺序有关问题</font>,例如AUC等."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  log集成变种(log ensemble version2)\n",
    "<br />\n",
    "\n",
    "这是一种很少有人知道的集成方法,效果亲测极佳。常见的问题是推荐问题,大家知道很多推荐问题是给用户推荐N个产品,然后再从N个产品计算有多少个产品被用户浏览或者点击来给与模型一定的评价，预测的N个产品有先后顺序,而且越在前面的产品的权重越大，也就是说第一个产品如果预测出错比第N个产品预测出错带来的loss更大。大家可能发现这个时候上面的所有方法都是可以用的，基于概率的基本都适用于这类问题。\n",
    "\n",
    "但是有的时候可能数据可能比较大,这个时候大家没时间再去跑之前的model,但是我们手上有10份预测结果(非概率文件,而是每条记录都对应N个预测产品的结果文件),这个时候怎么样的集成方法能最有效呢？这个时候送大家一种神集成！\n",
    "\n",
    "\n",
    "#### log集成变种\n",
    "  \n",
    "\n",
    "**算法基本思路:**\n",
    "\n",
    "> **输入**:M个分数类似的预测结果文件,没条记录对应N个预测结果;<br />\n",
    "> **输出**:最终的输出N个预测结果 <br />\n",
    "> **基本步骤**: <br />\n",
    ">      $~~~~$ 1.统计每条记录中,每个推荐商品(N个商品)的出现位置,例如商品A,在第一份文件的推荐位置为1,在第二个文件的推荐位置是3,在第三个文件中未出现,此时我们计算商品A的得分为log1 + log3 + log(N+1),为出现我们用N+1表示;<br />\n",
    ">      $~~~~$ 2.找出所有没条记录中出现的产品的值并由小到大排序,取topN作为最终结果.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 适用问题\n",
    "\n",
    "该方法常见于<font color = red>推荐TopK的问题,尤其是TopK的顺序权重也不一样的问题</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stacking/Blending\n",
    "\n",
    "这一块我们介绍stacking方法,也是我们比赛中尝试较多的方法.stacking背后的基本思路就是,使用一群基分类器(第一层),然后使用另外一个分类器(第二层)来组合前面第一层的预测结果,stacking的目的最直观的理解就是减小泛化误差.在1992年《STACKED GENERALIZATION》一文中还有一句关于Stacking的理解:\n",
    "\n",
    "> Stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation’s crude winner-takes-all for combining the individual generalizers.    \n",
    "\n",
    "至于stacking在做什么,为什么有效,我们此处也直接引用University of Notre Dame的ensemble的ppt中的两句话进行阐述.\n",
    "> - If a particular base-classifier incorrectly learned a certain region of the feature space, the second tier (meta) classifier\n",
    "may be able to detect this undesired behavior.\n",
    "> - Along with the learned behaviors of other classifiers, it can correct such improper training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red>**关于stacking与blending的区别**:网上关于blending和stacking的说法有很多,很多时候大家已经把blending和stacking混为一谈,这边我们以台大林老师的课件作为参考,如果对细节感兴趣,建议大家去网上学习一下台大林老师相应的课程章节.</font><font color = blue>_此处我们以台大为主,把stacking当做是blending的一个特例,后续的介绍我们一律用stacking._</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pic/Stacking_and_blending.png\" width=\"75%\" height=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "至于更深层次的理解和讨论,还是建议大伙查看相应文献和技术文档,此处不再详述,接下来,我们回归到我们的实际应用,着重介绍几种常见的stacking/blending技术.\n",
    "\n",
    "- 3.2.1 简单单层stacking方法(传统的方法Simple stacking with logistic regression/nonlinear algorithm)\n",
    "- 3.2.2 单层stacking方法(融入多项式特征)\n",
    "- 3.2.3 两层stacking或者多层stacking\n",
    "- 3.2.4 stacking与其他技术的融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  简单单层stacking方法(传统的方法Simple stacking with logistic regression/nonlinear algorithm)\n",
    "####  单层stacking方法基本思路\n",
    "此处我们以两折(2-fold)stacking为例,N-fold stacking的思路类似:\n",
    "\n",
    "** 2-fold stacking算法思路: **\n",
    "\n",
    "> **输入**:训练集train,测试集test,m个第一层模型,1个第二层模型;<br />\n",
    "> **输出**:最终的集成结果 <br />\n",
    "> **基本步骤**: <br />\n",
    ">    $~~~~$ 1.将训练集拆分为2部分:train_a,train_b,m个第一层模型,1个第二层模型;<br />\n",
    ">    $~~~~$ 2.对于第一层中的每一个模型model_1,model_2,...,model_m;<br />\n",
    ">    $~~~~~~~~$ 2.1 在train_a数据集合上面拟合第一层的模型并且对train_b做预测;<br />\n",
    ">    $~~~~~~~~$ 2.2.在train_b数据集合上面拟合第一层模型并且对train_a做预测; <br />\n",
    ">    $~~~~~~~~$ 2.3.将2.1,2.2中的结果进行合并获得一个新的特征train_level1_model_i; <br />\n",
    ">    $~~~~~~~~$ 2.4.在train数据集合上面拟合第一层模型并且对test做预测,获得新的特征test_level1_model_i<br />\n",
    ">    $~~~~$ 3.用第一层中获得的m个特征(train_level1_model_1,train_level1_model_2,...,train_level1_model_m)进行训练,再在新的测试集   $~~~~~~~~$(test_level1_model_1,test_level1_model_2,...,test_level1_model_m)上进行测试得到我们最终的提交结果;<br />\n",
    "\n",
    "第二层通常会采用LR模型,不过很多非线性的模型例如XGB等等也都在一些比赛中取得了非常好的效果.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码展示(https://github.com/MLWave/Kaggle-Ensemble-Guide/blob/master/blend_proba.py)\n",
    "其他的一些stacking另外两个非常棒的代码链接(注意下面的两个链接有些许差别,大家注意可以细细阅读代码,不过在实际比赛中建议可以都尝试一下):\n",
    "> ①.https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python <br />\n",
    "> ②.https://github.com/ndemir/stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import md5\n",
    "import json\n",
    "\n",
    "def blend_proba(clf, X_train, y, X_test, nfolds=5, save_preds=\"\",save_test_only=\"\", seed=300373, save_params=\"\",\n",
    "                clf_name=\"XX\", generalizers_params=[], minimal_loss=0,return_score=False, minimizer=\"log_loss\"):\n",
    "    print(\"\\nBlending with classifier:\\n\\t%s\"%(clf))\n",
    "    folds = list(cross_validation.StratifiedKFold(y, nfolds,shuffle=True,random_state=seed))\n",
    "    print(X_train.shape)\n",
    "    dataset_blend_train = np.zeros((X_train.shape[0],np.unique(y).shape[0]))\n",
    "\n",
    "    #iterate through train set and train - predict folds\n",
    "    loss = 0\n",
    "    for i, (train_index, test_index) in enumerate( folds ):\n",
    "        print(\"Train Fold %s/%s\"%(i+1,nfolds))\n",
    "        fold_X_train = X_train[train_index]\n",
    "        fold_y_train = y[train_index]\n",
    "        fold_X_test = X_train[test_index]\n",
    "        fold_y_test = y[test_index]\n",
    "        clf.fit(fold_X_train, fold_y_train)\n",
    "\n",
    "        fold_preds = clf.predict_proba(fold_X_test)\n",
    "        print(\"Logistic loss: %s\"%log_loss(fold_y_test,fold_preds))\n",
    "        dataset_blend_train[test_index] = fold_preds\n",
    "        if minimizer == \"log_loss\":\n",
    "            loss += log_loss(fold_y_test,fold_preds)\n",
    "        if minimizer == \"accuracy\":\n",
    "            fold_preds_a = np.argmax(fold_preds, axis=1)\n",
    "            loss += accuracy_score(fold_y_test,fold_preds_a)\n",
    "        #fold_preds = clf.predict(fold_X_test)\n",
    "\n",
    "        #loss += accuracy_score(fold_y_test,fold_preds)\n",
    "\n",
    "        if minimal_loss > 0 and loss > minimal_loss and i == 0:\n",
    "            return False, False\n",
    "        fold_preds = np.argmax(fold_preds, axis=1)\n",
    "        print(\"Accuracy:      %s\"%accuracy_score(fold_y_test,fold_preds))\n",
    "    avg_loss = loss / float(i+1)\n",
    "    print(\"\\nAverage:\\t%s\\n\"%avg_loss)\n",
    "    #predict test set (better to take average on all folds, but this is quicker)\n",
    "    print(\"Test Fold 1/1\")\n",
    "    clf.fit(X_train, y)\n",
    "    dataset_blend_test = clf.predict_proba(X_test)\n",
    "\n",
    "    if clf_name == \"XX\":\n",
    "        clf_name = str(clf)[1:3]\n",
    "\n",
    "    if len(save_preds)>0:\n",
    "        id = md5.new(\"%s\"%str(clf.get_params())).hexdigest()\n",
    "        print(\"storing meta predictions at: %s\"%save_preds)\n",
    "        np.save(\"%s%s_%s_%s_train.npy\"%(save_preds,clf_name,avg_loss,id),dataset_blend_train)\n",
    "        np.save(\"%s%s_%s_%s_test.npy\"%(save_preds,clf_name,avg_loss,id),dataset_blend_test)\n",
    "\n",
    "    if len(save_test_only)>0:\n",
    "        id = md5.new(\"%s\"%str(clf.get_params())).hexdigest()\n",
    "        print(\"storing meta predictions at: %s\"%save_test_only)\n",
    "\n",
    "        dataset_blend_test = clf.predict(X_test)\n",
    "        np.savetxt(\"%s%s_%s_%s_test.txt\"%(save_test_only,clf_name,avg_loss,id),dataset_blend_test)\n",
    "        d = {}\n",
    "        d[\"stacker\"] = clf.get_params()\n",
    "        d[\"generalizers\"] = generalizers_params\n",
    "        with open(\"%s%s_%s_%s_params.json\"%(save_test_only,clf_name,avg_loss, id), 'wb') as f:\n",
    "          json.dump(d, f)\n",
    "\n",
    "    if len(save_params)>0:\n",
    "        id = md5.new(\"%s\"%str(clf.get_params())).hexdigest()\n",
    "        d = {}\n",
    "        d[\"name\"] = clf_name\n",
    "        d[\"params\"] = { k:(v.get_params() if \"\\n\" in str(v) or \"<\" in str(v) else v) for k,v in clf.get_params().items()}\n",
    "        d[\"generalizers\"] = generalizers_params\n",
    "        with open(\"%s%s_%s_%s_params.json\"%(save_params,clf_name,avg_loss, id), 'wb') as f:\n",
    "          json.dump(d, f)\n",
    "\n",
    "    if np.unique(y).shape[0] == 2: # when binary classification only return positive class proba\n",
    "        if return_score:\n",
    "            return dataset_blend_train[:,1], dataset_blend_test[:,1], avg_loss\n",
    "        else:\n",
    "            return dataset_blend_train[:,1], dataset_blend_test[:,1]\n",
    "    else:\n",
    "        if return_score:\n",
    "            return dataset_blend_train, dataset_blend_test, avg_loss\n",
    "        else:\n",
    "            return dataset_blend_train, dataset_blend_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 适用问题\n",
    "stacking方法基本适用于目前90%的问题,包括<font color = red>分类,回归,推荐</font>等等. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  单层stacking方法(融入多项式等特征)\n",
    "该方法与上面的方法的一个比较大的区别是,在第一层特征中间我们加入**多项式等其他特征**.当然其他的组合也都可以进行尝试.\n",
    "\n",
    "####  单层stacking方法(融入多项式等特征)\n",
    "** 2-fold stacking算法思路<font color = red>(融入多项式特征)</font>: **\n",
    "\n",
    "> **输入**:训练集train,测试集test,m个第一层模型,1个第二层模型;<br />\n",
    "> **输出**:最终的集成结果 <br />\n",
    "> **基本步骤**: <br />\n",
    ">    $~~~~$ 1.将训练集拆分为2部分:train_a,train_b,m个第一层模型,1个第二层模型;<br />\n",
    ">    $~~~~$ 2.对于第一层中的每一个模型model_1,model_2,...,model_m;<br />\n",
    ">    $~~~~~~~~$ 2.1 在train_a数据集合上面拟合第一层的模型并且对train_b做预测;<br />\n",
    ">    $~~~~~~~~$ 2.2.在train_b数据集合上面拟合第一层模型并且对train_a做预测; <br />\n",
    ">    $~~~~~~~~$ 2.3.将2.1,2.2中的结果进行合并获得一个新的特征train_level1_model_i; <br />\n",
    ">    $~~~~~~~~$ 2.4.在train数据集合上面拟合第一层模型并且对test做预测,获得新的特征test_level1_model_i<br />\n",
    ">    $~~~~$ 3.用第一层中获得的m个特征(train_level1_model_1,train_level1_model_2,...,train_level1_model_m)<font color = red>(对m个特征两两相乘(相除或者其他操作)<br />\n",
    "$~~~~~~~$ 形成新特征)</font>行训练,再在新的测试集上<font color = red>(对m个测试集的特征进行与训练集相同的操作形成新的测试集特征)</font>进行测试得到我们最终的提交结果<br />\n",
    "$~~~~~~~$ (test_level1_model_1,test_level1_model_2,...,test_level1_model_m);<br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 代码展示\n",
    "略.\n",
    "\n",
    "#### 适用问题\n",
    "此处的stacking方法算是3.2.1的一个简单扩展,所以同样适用于上述3.2.1中的所有问题,即<font color = red>分类,回归,推荐</font>等等. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  两层stacking或者多层stacking \n",
    "如果说3.2.2是在<font color=red>宽度</font>上对传统stacking的一种扩展,那么本节要介绍的两层或多层stacking就是对传统stacking技术的一种<font color=red>深度</font>上的扩展,一个直观的例子就是(下图摘自论文《Feature-Weighted Linear Stackingtwo-layer-stacking》):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pic/two-layer-stacking.png\" width=\"60%\" height=\"40%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至于多层stacking的话就是继续在上面的基础上继续往后加层数,最新的中国机器学习界的大牛周老师的论文gcForest很多人也认为是一个改版的多层stacking.大家也可以关注一下."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  stacking与其他技术的融合\n",
    "\n",
    "####  融入无监督特征\n",
    "从上面的介绍中我们已经发现stacking的中间层是通过一堆model结果的输出来作为新的特征用作下一层的输入.既然如此,那么只要是有输出的model就都可以融入进来,所以我们在每一层就又可以加入一些无监督的方法,例如K-means以及其他各种聚类方法.\n",
    "\n",
    "####  回归与分类的结合(Binning 技术)\n",
    "确切地说,该部分属于一种特征工程的技巧,经常用于中间数据的处理,例如将预测概率为0-0.1设置为1,预测概率为0.1-0.5的设置为2等等.可以认为嵌在层间的特征工程(个人理解)......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  补充说明\n",
    "本文notebook的初衷是希望帮助大家实践或者进行数据相关的比赛,所以本文提到的方法仅仅给出直观的解释和代码以及适应的情况,偏工程与实战,舍去了理论的部分,如果对理论感兴趣的可以去网上google集成方法相关的工作,当然要用好这些方法也有很多讲究,里面要注意的细节也较多,例如预测结果之间的相关性,如果所有结果都是一样的,那么集成就没有任何效果.不过这些知识就留给大家实战去总结和归纳吧."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  参考文献：\n",
    "1.kaggle-ensembling-guide (https://mlwave.com/kaggle-ensembling-guide/) <br />\n",
    "2.kaggle-ensembling-guide对应的代码Github (https://github.com/MLWave/Kaggle-Ensemble-Guide) <br />\n",
    "3.Stacking slides(https://www3.nd.edu/~rjohns15/cse40647.sp14/www/content/lectures/32%20-%20Stacking.pdf) <br />\n",
    "4.Quora:What-is-stacking-in-machine-learning? (https://www.quora.com/What-is-stacking-in-machine-learning) <br />\n",
    "5.Stacked generation(http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf) <br />\n",
    "6.Feature-Weighted Linear Stackingtwo-layer-stacking(https://arxiv.org/pdf/0911.0460.pdf) <br />\n",
    "7.林老师的课件(http://www.csie.ntu.edu.tw/~htlin/mooc/) <br />\n",
    "8.ndemir关于Stacking的Github(https://github.com/ndemir/stacking) <br />\n",
    "9.Introduction-to-ensembling-stacking-in-python(https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
