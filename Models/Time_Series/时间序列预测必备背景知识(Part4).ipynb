{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#线性回归模型\" data-toc-modified-id=\"线性回归模型-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>线性回归模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#简单的线性回归\" data-toc-modified-id=\"简单的线性回归-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>简单的线性回归</a></span></li><li><span><a href=\"#最小平方估计\" data-toc-modified-id=\"最小平方估计-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>最小平方估计</a></span></li><li><span><a href=\"#一些有用的预测因子\" data-toc-modified-id=\"一些有用的预测因子-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>一些有用的预测因子</a></span></li><li><span><a href=\"#回归模型的评估\" data-toc-modified-id=\"回归模型的评估-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>回归模型的评估</a></span></li><li><span><a href=\"#变量选择\" data-toc-modified-id=\"变量选择-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>变量选择</a></span></li><li><span><a href=\"#回归预测\" data-toc-modified-id=\"回归预测-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>回归预测</a></span></li><li><span><a href=\"#矩阵形式\" data-toc-modified-id=\"矩阵形式-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>矩阵形式</a></span></li><li><span><a href=\"#非线性回归\" data-toc-modified-id=\"非线性回归-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>非线性回归</a></span></li><li><span><a href=\"#相关性,因果性和预测\" data-toc-modified-id=\"相关性,因果性和预测-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>相关性,因果性和预测</a></span></li><li><span><a href=\"#深入阅读\" data-toc-modified-id=\"深入阅读-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>深入阅读</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归模型\n",
    "\n",
    "<font color=blue>**基本假设**</font>:\n",
    "\n",
    "此处最基本的假设就是:我们假设我们感兴趣的变量$y$是和我们的时间序列呈线性关系的. <br /> <br />\n",
    " \n",
    " \n",
    "\n",
    "<font color=blue>**例子**</font>: \n",
    "- 根据广告支出来预测每个月的销量\n",
    "- 根据温度$x_1$,和周中的某一天$x_2$来预测每月的用电量$y$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单的线性回归 \n",
    "\n",
    "<font color=blue>**基本假设**</font>:\n",
    "\n",
    "$y_t = \\beta_0 + \\beta_1 x_t + \\epsilon_t$\n",
    "\n",
    "![](./pic/linear_reg_exmp.png) <br />\n",
    "\n",
    "我们认为每一个观测变量$y_t$都是由$\\beta_0+\\beta_1x_t$加上一个随机误差$\\epsilon_t$所组成,** 误差项不是一个错误,而是在潜在线性模型周围的一个扰动.它表示除了$x_t$的所有影响$y_t$的因素.**   <br />  <br />\n",
    "\n",
    "\n",
    "\n",
    "<font color=blue>**多元线性回归**</font>:\n",
    "\n",
    "当存在两个或者多个变量时,我们称该模型为多元回归模型,它的数学形式为: $y_t = \\beta_0 + \\beta_1x_{1,t} + \\beta_2x_{2,t}+ ... + \\beta_kx_{k,t} + \\epsilon_t$ <br /><br />\n",
    "\n",
    "\n",
    "<font color=blue>**线性模型假设**</font>:\n",
    "\n",
    "\n",
    "<font color=red>**注意下面的假设和很多书籍中存在一定的不同,大家以自己的理解为主即可.**</font>: \n",
    "\n",
    "- (1).模型对于现实是一种合理的近似,也就是说我们预测的变量(label)和我们的特征之间满足线性的关系.\n",
    "- (2).我们对于误差($\\epsilon$)做如下假设:\n",
    "\n",
    "> 它们之间的均值是0,否则预测的结果将会被系统性的biased.<br />\n",
    "> 它们之间不能自相关,否则,预测将会是无效的,因为数据中存在更多的信息可以被探索(噪音之间的相关性).<br />\n",
    "> 噪音和我们的特征是无关的,否则,应当将更多的信息被引入我们的模型.<br />\n",
    "\n",
    "- (3).每个特征变量不是随机的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最小平方估计\n",
    "\n",
    "我们拥有较多的观测到的特征,但是每个特征对应的系数却是未知的,而最小平方估计可以很好的帮助我们获得我们需要的系数$\\beta_0,\\beta_1,...,\\beta_k$,而求解的方法就是通过去求解下面的式子:<br />\n",
    "\n",
    "$min \\Sigma_{t=1}^T \\epsilon_t^2 = \\Sigma_{t=1}^T(y_t - \\beta_0 - \\beta_1x_{1,t}- \\beta_2x_{2,t} - ... - - \\beta_kx_{k,t})^2$\n",
    "\n",
    "\n",
    "<font color=red>This is called “least squares” estimation because it gives the least value for the sum of squared errors. </font>.\n",
    "\n",
    "在可视化此类数据的时候,R语言和python的包中都有lmplot,大家可以自我尝试一番.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一些有用的预测因子\n",
    "\n",
    "在使用回归模型处理时序问题的时候存在很多有用的预测因子.\n",
    "\n",
    "\n",
    "\n",
    "<font color=blue>**趋势**</font>:\n",
    "\n",
    "时间序列里面最常见的就是趋势,线性模型可以简单的使用$x_{1,t} = t$作为一个特征,此时**$y_t = \\beta_0 + \\beta_1 t + \\epsilon_t$,$t=1,2,3...,T$** <br /><br />\n",
    "\n",
    "\n",
    "\n",
    "<font color=blue>**哑变量**</font>:\n",
    "\n",
    "通常我们会假设我们的特征都是数值型的.(下面是类别为2的情况)\n",
    "\n",
    "- **传统用途**:最常见的例如销量预测中是否为节假日.此时我们可以用0,1来替换该信息;\n",
    "\n",
    "- **特殊用途**:哑变量还可以用来处理奇异值,例如销量预测中总会出现一些奇异的事情,例如演唱会,运动会等等都会使得某类产品的销量大增,这个时候我们可以用哑变量来表示该类奇异值的情况.而不是直接删除数据.\n",
    "\n",
    "如果类别多于2个,采用类似的编码操作即可. <br /><br />\n",
    "\n",
    "\n",
    "<font color=blue>**季节性(周期性)哑变量**</font>:\n",
    "\n",
    "例如我们希望预测每天的用电需求,因为每天的用电需求和周几的关系很大,而且呈现周期性的特性,此时我们经常会选择用哑变量来表示该类特性,例如一周七天我们会用下面的方式表示.\n",
    " \n",
    "![](./pic/seasonal_dummy_variable.png)  <br /> \n",
    "\n",
    "\n",
    "<font color=red>**注意七个类别变量我们只需要6个哑变量,因为第七个变量当其他六个已经确定的时候基本已经定了,剩余一个哑变量带来的影响会被intercept所捕获.**</font> <br /><br />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<font color=blue>**傅里叶序列**</font>:\n",
    "\n",
    "季节性哑变量的一个替代方法就是傅里叶序列,尤其是关于**长期的周期性**,如果$m$是季节性周期,则第一个少量的傅里叶项将会由下述形式所给出:\n",
    "\n",
    "$x_{1,t} = sin(\\frac{2 \\pi t}{m}),x_{2,t} = cos(\\frac{2 \\pi t}{m}),x_{3,t} = sin(\\frac{4 \\pi t}{m}),x_{3,t} = cos(\\frac{4 \\pi t}{m}),x_{5,t} = cos(\\frac{5 \\pi t}{m}),x_{6,t} = cos(\\frac{6 \\pi t}{m}) .......$\n",
    "\n",
    "如果呈现月的周期性,我们使用11个特征变量,然后我们将会获得和11个哑变量一样的特征.\n",
    "\n",
    "\n",
    "<font color=red>** 使用傅里叶项的优势在于我们经常可以使用更少的特征变量,尤其是当$m$特别大的时候,例如当$m=52$的时候,对于较短的周期性,例如季节性的数据,使用傅里叶变换相比于哑变量的优势并不会十分明显.**</font> <br /><br /> 傅里叶变换可以直接使用工具包获得. 含有傅里叶项的回归经常被称作是**谐波回归**,因为连续的傅里叶项表示前两个傅立叶项的谐波. <br /><br />\n",
    "\n",
    "<font color=blue>**干预变量**</font>:\n",
    "\n",
    "对于可能对预测结果带来影响的干预进行建模,例如,广告支出,工业行动等等.\n",
    "\n",
    "当影响只回持续某一个周期的时候,我们会使用一个穗变量来表示.**穗变量**在干预的周期内是1,在其他地方是0,穗变量在处理奇异值的时候等价于哑变量.\n",
    "\n",
    "其他的干预如果有一个立即的持续的影响的话,我们用一个步变量来表示,**步变量**在干预之前都是0,在干预之后都是1.\n",
    "\n",
    "另外一种持久的影响是斜面坡度的影响,次数干预将会使用分段的线性趋势来表示.(具体的会在后面继续讨论)<br /><br />\n",
    "\n",
    "\n",
    "<font color=blue>**交易日**</font>:\n",
    "\n",
    "一个月内的交易日可能会对当月的交易带来非常大的影响,所以为了表示带来的影响,可以将每个月内交易日的个数包含为一个特征.例子如下:\n",
    "\n",
    "![](./pic/Trading_Days.png)\n",
    "\n",
    "<br /><br />\n",
    "\n",
    "<font color=blue>**分布滞后(Distributed lags)**</font>:\n",
    "\n",
    "将广告的支出作为一个特征经常是非常好的一个想法,但是广告的影响会持续较长时间,所以我们需要将广告活动的滞后值包含进来,例如下面的特征:\n",
    "\n",
    "![](./pic/Distributed_Lags.png)\n",
    "\n",
    "**PS:It is common to require the coefficients to decrease as the lag increases.**具体的策略大家可以自行谷歌. <br /><br />\n",
    "\n",
    "<font color=blue>**复活节**</font>:\n",
    "\n",
    "复活节不同于其他的节日,它不会在同一天进行庆祝,而是会在某一天庆祝,而且复活节的影响时间持续一周,此处我们往往需要对复活节带来影响的那几天或者那一个月采用哑变量的形式进行处理. **R语言里面有Easter函数可以直接使用.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归模型的评估\n",
    " \n",
    "\n",
    "<font color=blue>**残差**</font>:\n",
    "\n",
    "观测变量$y$与对应的预测变量$\\bar{y}$的差,我们称之为残差:$e_t = y_t - \\bar{y}_t$,每个残差是相关的观测变量的不可预测的部分.\n",
    "\n",
    "- 残差的一些有用的属性:\n",
    "> $\\Sigma_{t=1}^T e_t = 0$ & $\\Sigma_{t=1}^T x_{k,t}e_t = 0$, for all $k$. <br /><br /> \n",
    " \n",
    "\n",
    "<font color=blue>**残差直方图**</font>:\n",
    "\n",
    "检测残差是否是正态分布的总是一个比较好的想法.例如下图中2004年的预测我们发现有比较大的偏差,此时可以就需要进一步研究究竟在那一年发生了什么不寻常的事情来帮助我们进行后续的建模.\n",
    "\n",
    "![](./pic/Residual_Check.png) <br /><br /> \n",
    "\n",
    "\n",
    "<font color=blue>**残差预测图**</font>:\n",
    "我们希望残差的分布是随机的,没有任何的模式,而这样最简单的方式就是直接采用scatterplot的方式画出散点图.如果绘制的散点图存在某种模式,那么说明存在一些非线性的关系需要进一步进行挖掘. <br /><br /> \n",
    "\n",
    " \n",
    "<font color=blue>**奇异值和影响点**</font>:\n",
    " \n",
    " 对回归模型的预测结果有较大影响的被称作影响点(influential observations),和大多数数据极端不一样的被称作是离群点(outliers).\n",
    "\n",
    " \n",
    " - 对于可能是奇异值的处理:①如果有十足的把握判断是奇异值,建议直接删除;②研究奇异值的产生原因并且好好利用它;③如果无法研究原因,则分别记录带有该奇异值的结果和不带有该奇异值的结果,然后进行比较. \n",
    " \n",
    " - label为离群点或者影响点给线性回归带来的影响可以参考下图.红色的是去除影响点的,而黑色是没有去除影响点的.\n",
    " \n",
    "![](./pic/linear_reg_with_outlier.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**拟合良度(Goodness-of-fit)**</font>:\n",
    "\n",
    "我们常常会使用$R^2$来判断模型的拟合良度.\n",
    "\n",
    "![](./pic/R^2.png)\n",
    "\n",
    "**如果模型的预测结果和真实结果很接近,那么$R^2$就比较接近于1,另一方面,如果预测结果和真是结果是无关的,则$R^2$往往会趋于0.在所有的例子中,$R^2$介于0和1之间. **\n",
    "\n",
    "\n",
    "还有一种关于R^2数学形式如下.\n",
    "\n",
    "![](./pic/R^2_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>**回归的标准误差**</font>:\n",
    "\n",
    "除了$R^2$可以较好的对模型的拟合效果进行评估之外,另外一种评估模型拟合好坏的方法就是残差的标准差.经常也被称作\"residual standard error\",可以通过下面的式子进行计算得到:\n",
    "\n",
    "![](./pic/std_of_reg.png)\n",
    "\n",
    "此处除以$T-k-1$是为了解释计算残差时的估计参数的个数.($k+1$指的是$k+1$ 参数 (the intercept and a coefficient for each predictor variable) ))\n",
    "<br /> <br />\n",
    "\n",
    "<font color=blue>**伪回归(Spurious regression)**</font>\n",
    "\n",
    "时间序列数据经常是不稳定的,也就是说不会再某一个常数值附近上下波动,具体的处理该类数据的方法会在后续讨论.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量选择\n",
    "<br />\n",
    "\n",
    "<font color=blue>**Naive的方法**</font>\n",
    "\n",
    "- 绘制每一个特征与label之间的图,通过观察二者之间有无明确的关系来判定.(很难直接通过散点图等形式直接观测变量之间的潜在关系)\n",
    "\n",
    "- 对所有的特征变量做多元线性回归,然后将$p$值大于0.05的全部丢弃.(统计重要性并不意味着预测值的准确性,当两个或者多个预测变量存在的时候$p$值可能会带来误导) <br /> <br />\n",
    "\n",
    "\n",
    "<font color=blue>**特征选择的指标**</font>\n",
    " \n",
    "上述的方法比较适合特征维度较低,同时特征与label之间的关系较为明显的时候,却不太适用于特征维度较高的时候,在实践中,我们也很少会选择这么做.此处我们介绍一些现在大家用的较多的指标和方法. \n",
    " \n",
    "- **调整的$R^2$**,$\\bar{R}^2 = 1 - (1-R^2) \\frac{T-1}{T-k-1}$,其中$T$是样本的个数,$k$是特征的维度的大小,这样即使$R^2$有增长,但是$\\bar{R}^2$却不一定会增长.\n",
    "> 只要增加特征的维度,增加系数,一般$R^2$都会有一定的增长,但是$\\bar{R}^2$却不一定会增长,究其原因主要在于:$\\bar{R}^2$对特征变量的个数进行了惩罚,当特征变量个数增加时,$ \\frac{T-1}{T-k-1}$也会变大,而这样就可以**较好的控制我们模型的复杂度**. <br /> <br />\n",
    "\n",
    "- **交叉验证**:交叉验证的思路很简单,就不再叙述,最直观的理解可以参考下图.因为交叉验证可以近似我们的预测指标,所以在很多比赛中,我们往往通过线下的交叉验证的结果来近似线上的成绩.因此交叉验证也成为了很多比赛选手最常用的指标之一.\n",
    " \n",
    "![](./pic/CV.png) <br /> <br />\n",
    "\n",
    "- **Akaike信息准则(Akaike’s Information Criterion)**: 也就是我们常说的AIC指标,$AIC = Tlog(\\frac{SSE}{T}) + 2(k+2)$, 同样的,$T$是观测样本的个数,$k$是特征维度的个数. $k+2$是因为一共有$k$个系数加上一个intercept,$b$,再加上一个噪声项. 我们防线当样本$T$足够大的时候,**最小化$AIC$基本等价于最小化$CV$值.** <br /> <br />\n",
    "\n",
    "- **纠正的AIC**:$AIC_c = AIC + \\frac{2(k+2)(k+3)}{T-k-3}$,上面的$AIC$在当样本个数较少的时候,往往会为了最小化前一项而使得我们的特征维度偏高(参数个数偏多),为了对此进行纠正,我们决定加大对于参数个数的惩罚,所以就有了我们的$AIC_c$<br /> <br />\n",
    "\n",
    "- **施瓦茨的贝叶斯信息准则(Schwarz’s Bayesian Information Criterion)**: $BIC = Tlog(\\frac{SSE}{T}) + (k+2)log(T)$,和$AIC$一样,最小化$BIC$也是为了给出最好的模型,$BIC$选出来的模型和$AIC$很像,但是我们也看到了,$BIC$对于模型的系数的个数惩罚相对较大,所以最终$BIC$所选出的模型的系数的个数往往也较少.\n",
    "\n",
    "<font color=green>Many statisticians like to use the BIC because it has the feature that if there is a true underlying model, the BIC will select that model given enough data. However, in reality, there is rarely if ever a true underlying model, and even if there was a true underlying model, selecting that model will not necessarily give the best forecasts (because the parameter estimates may not be accurate). Consequently, we prefer to use the AICc, AIC, or CV statistics, which have forecasting as their objective (and which give equivalent models for large T). </font> <br /> <br />\n",
    " \n",
    " \n",
    "<font color=red> **既然我们有这么多的指标可以用来对我们的模型进行选择,那么究竟哪一个才是最好的呢?** </font> </br>下面我们通过一个例子进行阐述.\n",
    "  <br /> <br />\n",
    "\n",
    "<font color=blue>**例子**</font>\n",
    "\n",
    "![](./pic/Eval_Cmp.png)\n",
    "\n",
    "<font color=red>**从上面我们发现,其实这几个指标是类似的,当我们的CV最小的时候,我们的$AIC,AIC_c,BIC$基本都是最小的**</font>,而我们的$AdjR^2$则是最大的.所以平时我们做特征选择的时候只需要一种指标即可.很多时候,因为$CV$的结果能给我们最为直观的感受,所以我个人还是喜欢以$CV$指标来做特征选择.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "上面的指标使我们可以较好的评估我们的模型,但是如何来生成可以被评估的模型呢?下面我们来探讨如何设计被评估的模型的方法. <br /> <br />\n",
    "\n",
    "<font color=blue>**特征选择的模型构建方法**</font>\n",
    "\n",
    "- **最佳子集回归**:在资源许可的情况下,所有潜在的模型都应当被拟合回归,并且最好的模型应该基于上述讨论的指标而选出,我们知道,只要当我们的观测样本个数$T$足够大,最终我们选出的模型很有可能是同一个.   <br />\n",
    "\n",
    "- **逐步回归**: 当我们的特征维度足够大的时候,是不太可能拟合所有的模型再做评估的,此时我们往往会采用两种方法,**前向逐步回归**和**后向逐步回归**\n",
    "> ** 后向逐步回归: **\n",
    "![](./pic/backwards_stepwise_regression.png) \n",
    "> ** 前向逐步回归: **  <br />\n",
    "> > ①.Starts with a model that includes only the intercept. <br />\n",
    "> > ②.Predictors are added one at a time, and the one that most improves the measure of predictive accuracy is retained in the model <br />\n",
    "> > ③.The procedure is repeated until no further improvement can be achieved. <br />\n",
    "\n",
    ">**混合过程** :模型的初步就包含有一些潜在的预测特征变量,在之后的每一步,我们可以加入一个特征(forward)也可以删去一个特征(backward).\n",
    "\n",
    "<font color=red> **注意:逐步的方法并不一定可以使我们得到最好的模型,但是却可以使我获得一个不错的模型. ** </font> <br />\n",
    " \n",
    "<font color=red> **将特征选择的模型构建方法和我们的特征选择的指标相结合,就可以得到非常不错的特征选择的方法了. ** </font>\n",
    "\n",
    "一个非常经典的例子可以参见Kaggle比赛https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/ PB榜第三名的方案的分享. <br /> <br />\n",
    " \n",
    "\n",
    "<font color=blue>**注意在特征选择完后进行推理(inference)**</font> \n",
    "\n",
    "因为我平时很少做这一步,所以仅仅把作者的语录复制过来,有兴趣的读者可以自己Google.\n",
    "\n",
    "We do not discuss statistical inference of the predictors in this book (e.g., looking at p -values associated with each predictor). If you do wish to look at the statistical significance of the predictors, beware that any procedure involving selecting predictors first will invalidate the assumptions behind the p -values. The procedures we recommend for selecting predictors are helpful when the model is used for forecasting; they are not helpful if you wish to study the effect of any predictor on the forecast variable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归预测\n",
    "<br />\n",
    "\n",
    "<font color=blue>**事前事后预测**</font>\n",
    "\n",
    "- **事前预测**: 事前预测指的是仅仅只使用预测变量之前的信息,例如我们要预测2017.12月份的销量,那么我们就只能使用2017.11月份及之前的数据;\n",
    "\n",
    "- **事后预测**: 事后预测只哪些会使用到未来信息的预测,例如我要预测12月份的销量,我在预测的时候就使用到了12月份以及之后的天气,超市产品的进货信息等这些未来信息,<font color=red>注意事后预测一般指我们可以获得未来的特征变量,而非未来的label</font>.\n",
    "\n",
    "- **事情预测和事后预测可以帮助我们分离预测不确定性的来源.这可以帮助我们理解我们的预测差的原因是来源于模型还是来源于我们的特征.**\n",
    "\n",
    "<font color=blue>**基于场景的预测**</font>\n",
    "\n",
    "在该设定中,预测员假设所有感兴趣的特征可以预见,例如,美国政府想要预测未来的消费量,在预测的时候我们已经知道人们的收入和就业的指数近期呈现常数增长(该增长长期不变),<font color=red>这样我们在模型预测的时候就已经知道了我们的未来特征without uncertainty.</font> <br /> <br />\n",
    "\n",
    "\n",
    "<font color=blue>**构建预测回归模型**</font>\n",
    "\n",
    "<center>**回归模型1**:$\\bar{y}_t = \\beta_0 + \\beta_1x_{1,t} + \\beta_2x_{2,t} + ... + \\beta_kx_{k,t}$ </center>\n",
    "\n",
    "回归模型的最大优势在于它可以捕获特征变量和我们的label之间的关系.而最大的挑战则在于为了进行事前预测,模型需要未来特征,从我们的模型中我们就可以发现,预测$t$时刻的$y$我们需要知道$t$时刻的$x$,如果是基于场景的预测,那么该问题还相对简单很多,所以我们需要对该公式进行变形.\n",
    "\n",
    "\n",
    "<center>**回归模型2**:$\\bar{y}_{t+h} = \\beta_0 + \\beta_1x_{1,t} + \\beta_2x_{2,t} + ... + \\beta_kx_{k,t} + \\epsilon_{t+h},h=1,2,...$ </center>\n",
    "\n",
    "经过变形之后,我们在预测$t+h$时的label时,我们仅仅只需要知道在$t$时刻的特征即可. 增加的误差变量不仅仅使得我们的模型更加易于操作,同时还使得我们的模型更加易于理解. <br /> <br />\n",
    "\n",
    "\n",
    "<font color=blue>**预测区间**</font>\n",
    "\n",
    "预测区间的计算会在下一小节进行讨论,此处我们仅仅给出预测区间的简单公式以及它的直观的解释,假设我们使用简单的回归$\\bar{y} = \\beta_0 + \\beta_1 x$模型进行预测,并且回归模型的误差是正太分布的,那么我们的95%的预测区间可以通过如下公式计算得到:\n",
    "\n",
    "![](./pic/pre_interval.png)\n",
    "\n",
    "其中$T$是我们的观测样本个数,$\\bar{x}$是我们的观测样本特征的均值,$s_x$是$x$的标准差.\n",
    "\n",
    "我们发现:<font color=red>当我们的特征值$x$与我们的均值$\\bar{x}$相距越远的时候,我们的预测区间也会越大. </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 矩阵形式\n",
    "\n",
    "这一节涉及到使用最小平方算法求解线性模型的一些数学知识,因为每一本介绍机器学习的书籍在线性模型模块都会有所介绍,所以此处我们省略具体的数学推导,仅仅给出重要的结论. <br /> <br /> \n",
    "\n",
    "<font color=blue>**多元线性回归的向量形式**</font>\n",
    "\n",
    "假设,我们的多元线性模型可以被写成:$y_t = \\beta_0 + \\beta_1 x_{1,t}+ \\beta_2 x_{2,t} + ... + + \\beta_k x_{k,t} + \\epsilon_t$,其中$\\epsilon_t$的均值为0,方差为$\\sigma^2$.假设有:\n",
    "\n",
    "![](./pic/X.png)\n",
    "\n",
    "![](./pic/beta_y.png)\n",
    "则我们有$y = X \\beta + \\epsilon$, 其中$ \\epsilon$均值为0,方差为$\\sigma^2 I$.  <br /> <br /> \n",
    "\n",
    "\n",
    "<font color=blue>**最小平方估计**</font>\n",
    "\n",
    "通过求解,我们可以获得我们$\\beta$的最优闭式解, \n",
    "![](./pic/bishijie.png)\n",
    "\n",
    "我们可以通过使用下面的式子来计算残差的方差(均值为0,因为$\\epsilon$的均值为0):\n",
    "\n",
    "![](./pic/residual_var.png) <br /> <br /> \n",
    "\n",
    "\n",
    "<font color=blue>**拟合值与交叉验证**</font>\n",
    "\n",
    "- 拟合值:![](./pic/fitted_value.png) <br />\n",
    "\n",
    "- CV 统计量: 是每一个样本的预测误差的平方与(1-该类的特征的平方和与占所有特征的平方的商)的均值\n",
    "\n",
    "![](./pic/CV_statics.png)\n",
    "\n",
    "直观来看就是我们希望特征占的比例越大,我们的预测误差越小,如果特征占的比例较小,我们的预测误差大一点也无所谓. <br /> <br /> \n",
    "\n",
    "\n",
    "<font color=blue>**拟合值与交叉验证**</font>\n",
    "- 拟合值的计算(假设$x^*$是一个特征向量)\n",
    "![](./pic/predict_val.png)\n",
    "\n",
    "- 拟合值的方差\n",
    "![](./pic/var.png)\n",
    "\n",
    "- 95%置信区间(和正太分布的类似):\n",
    "![](./pic/predict_interval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 非线性回归\n",
    "\n",
    "因为很多时候变量之间的关系往往是非线性的,所以我们需要进行非线性建模,本节在之前线性模型的基础之上介绍几种简单的将线性模型转化为非线性模型的技巧.<br />\n",
    "\n",
    "<font color=blue>**几种线性回归转非线性回归的技巧**</font>\n",
    "\n",
    "- log-log函数形式:将x和y都进行log处理,举例如:$logy= \\beta_0 + \\beta_1 logx + \\epsilon$ \n",
    "- linear-log形式:将x进行log处理.\n",
    "- 分段线性转化:例如$x=(x-c)_+$,$x < c$的全部转为0,大于$c$的转化为$x-c$. <br /><br />\n",
    "\n",
    "<font color=blue>**非线性趋势预测**</font>\n",
    " \n",
    "- 最简单的转换: $x_{1,t} = t,x_{2,t} = t^2,x_{3,t} = t^3....$,这种转换很简单,但是在预测的时候却并不十分靠谱.\n",
    "- 分段线性转换:如果趋势在时间$\\tau$处出现了拐弯,我们可以在分段线性函数中令$x=t$,$c = \\tau$,这样就等价于我们认为在$\\tau$之前$x$没有影响,而在此之后才开始有影响.\n",
    "![](./pic/piece_linear.png)\n",
    "- label的log转换,$y \\rightarrow logy$ <br />\n",
    "\n",
    "下图是三种不同转换的对比.\n",
    "\n",
    "![](./pic/3nonlinear_trans.png)\n",
    "\n",
    "我们发现label的log转换和我们的简单线性拟合类似,但是分段线性转换的效果却好了很多.\n",
    "\n",
    "Although the exponential trend does not seem to fit the data much better than the linear trend, it gives a more sensible projection in that the winning times will decrease in the future but at a decaying rate rather than a linear non-changing rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相关性,因果性和预测\n",
    "\n",
    "这一节主要介绍几个易错的概念,相关性,因果性以及预测.<br />\n",
    "\n",
    "<font color=blue>**相关性与因果性**</font>\n",
    "\n",
    "- **相关性**:如果一个特征变量$x$能对我们预测label带来非常大的帮助,那么我们往往称该特征变量与我们的label有着较强的相关性.\n",
    "\n",
    "- **因果性**:如果有B发生了,那么100%是$A$造成的,此时我们称之为因果性.\n",
    "\n",
    "相关性,因果性与预测的关系可以用下面的语句来阐述:**相关性能为我们的预测带来非常大的帮助,即使这二者不存在因果关系; 当然,如果能确定某些因果关系,那么我们就会有更大的希望构建一个好的模型.** <br /> <br />\n",
    "\n",
    "<font color=blue>**混淆特征变量**</font>\n",
    "\n",
    "- **混淆特征变量**:如果两个特征变量给label带来的影响是无法分开的时候,我们称之为混淆变量. 平时我们一般将两个线性相关系数较高的变量称之为一对混淆变量. \n",
    "> 混淆特征变量在我们做模型预测的时候带来的影响往往不是很大,很多时候我们仅仅只需要将两个全部放入模型即可.但是如果我们需要进行场景预测的时候,我们就需要分析特征变量之间的关系,另外,我们希望统计混淆变量在历史数据上对label带来的影响时,这也会成为一大问题.<br /> <br />\n",
    "\n",
    "\n",
    "\n",
    "<font color=blue>**共线性和预测**</font>\n",
    "\n",
    "- 共线性：如果两个特征变量之间的相关性近似为1或者-1,我们认为这两个变量具有共线性.较为典型的例子就是dummy variable trap,例如我们有季节性的哑变量$d_1,d_2,d_3,d_4$并且$d_1+d_2+d_3+d_4 = 1$,那么我们为1的那一列就和我们这四个变量的和是共线的.\n",
    "\n",
    "<font color=red>共线性的变量对于线性模型的预测会带来非常大的损害,尤其是回归系数的计算将会变得非常困难.</font>具体的细节大家可以搜索线性回归方面的论文.<br /> <br />\n",
    "\n",
    "\n",
    "<font color=blue>**注意事项**</font>  <br />  \n",
    "\n",
    "<font color=red>**如果我们的特征变量在我们的特征之外的话,那么模型的预测将会变得比较unreliable.例如某个特征变量$x$在训练集中的$<100$,但是在测试的时候却出现了$x=200$的情况,这个时候我们就会认为我们的模型是不可靠的.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深入阅读\n",
    "\n",
    "1. https://en.wikipedia.org/wiki/Coefficient_of_determination\n",
    "2. 交叉验证:https://baike.baidu.com/item/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/8543100?fr=aladdin\n",
    "3. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. 2014. An Introduction to Statistical Learning: With Applications in R. New York: Springer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "900px",
    "left": "0px",
    "right": "1598px",
    "top": "67px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
