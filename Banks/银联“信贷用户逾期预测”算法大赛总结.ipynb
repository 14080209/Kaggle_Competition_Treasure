{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#“信贷用户逾期预测”算法大赛\" data-toc-modified-id=\"“信贷用户逾期预测”算法大赛-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>“信贷用户逾期预测”算法大赛</a></span><ul class=\"toc-item\"><li><span><a href=\"#背景介绍\" data-toc-modified-id=\"背景介绍-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>背景介绍</a></span></li><li><span><a href=\"#工具包&amp;数据的导入&amp;数据EDA\" data-toc-modified-id=\"工具包&amp;数据的导入&amp;数据EDA-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>工具包&amp;数据的导入&amp;数据EDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#工具包的导入\" data-toc-modified-id=\"工具包的导入-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>工具包的导入</a></span></li><li><span><a href=\"#数据的导入\" data-toc-modified-id=\"数据的导入-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>数据的导入</a></span></li><li><span><a href=\"#数据EDA\" data-toc-modified-id=\"数据EDA-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>数据EDA</a></span></li></ul></li></ul></li><li><span><a href=\"#特征的构建\" data-toc-modified-id=\"特征的构建-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>特征的构建</a></span><ul class=\"toc-item\"><li><span><a href=\"#构建特征集合1\" data-toc-modified-id=\"构建特征集合1-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>构建特征集合1</a></span><ul class=\"toc-item\"><li><span><a href=\"#基于模型的表达能力的特征\" data-toc-modified-id=\"基于模型的表达能力的特征-2.1.1\"><span class=\"toc-item-num\">2.1.1&nbsp;&nbsp;</span>基于模型的表达能力的特征</a></span></li><li><span><a href=\"#比例特征构建\" data-toc-modified-id=\"比例特征构建-2.1.2\"><span class=\"toc-item-num\">2.1.2&nbsp;&nbsp;</span>比例特征构建</a></span></li><li><span><a href=\"#标准差还原特征（反映信息的波动）\" data-toc-modified-id=\"标准差还原特征（反映信息的波动）-2.1.3\"><span class=\"toc-item-num\">2.1.3&nbsp;&nbsp;</span>标准差还原特征（反映信息的波动）</a></span></li><li><span><a href=\"#均值特征\" data-toc-modified-id=\"均值特征-2.1.4\"><span class=\"toc-item-num\">2.1.4&nbsp;&nbsp;</span>均值特征</a></span></li><li><span><a href=\"#趋势特征\" data-toc-modified-id=\"趋势特征-2.1.5\"><span class=\"toc-item-num\">2.1.5&nbsp;&nbsp;</span>趋势特征</a></span></li></ul></li><li><span><a href=\"#构建特征集合2\" data-toc-modified-id=\"构建特征集合2-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>构建特征集合2</a></span></li></ul></li><li><span><a href=\"#模型训练\" data-toc-modified-id=\"模型训练-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>模型训练</a></span><ul class=\"toc-item\"><li><span><a href=\"#指标的构建\" data-toc-modified-id=\"指标的构建-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>指标的构建</a></span><ul class=\"toc-item\"><li><span><a href=\"#获取topN重要的特征\" data-toc-modified-id=\"获取topN重要的特征-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>获取topN重要的特征</a></span></li></ul></li><li><span><a href=\"#模型训练与验证\" data-toc-modified-id=\"模型训练与验证-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>模型训练与验证</a></span><ul class=\"toc-item\"><li><span><a href=\"#获取不同子集数据集\" data-toc-modified-id=\"获取不同子集数据集-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>获取不同子集数据集</a></span></li><li><span><a href=\"#多个模型训练与测试\" data-toc-modified-id=\"多个模型训练与测试-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>多个模型训练与测试</a></span></li><li><span><a href=\"#实验小结\" data-toc-modified-id=\"实验小结-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;</span>实验小结</a></span></li></ul></li></ul></li><li><span><a href=\"#总结与展望\" data-toc-modified-id=\"总结与展望-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>总结与展望</a></span><ul class=\"toc-item\"><li><span><a href=\"#总结\" data-toc-modified-id=\"总结-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>总结</a></span><ul class=\"toc-item\"><li><span><a href=\"#本次比赛难点以及解决方案\" data-toc-modified-id=\"本次比赛难点以及解决方案-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>本次比赛难点以及解决方案</a></span></li><li><span><a href=\"#方案总结\" data-toc-modified-id=\"方案总结-4.1.2\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>方案总结</a></span></li></ul></li><li><span><a href=\"#展望\" data-toc-modified-id=\"展望-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>展望</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE2F184AC8E64CFB8507359CFF9FB704",
    "mdEditEnable": false
   },
   "source": [
    "# “信贷用户逾期预测”算法大赛\n",
    "\n",
    "## 背景介绍\n",
    "\n",
    "个人信用是整个社会信用的基础，市场交易中所有的经济活动，与个人信用息息相关。一旦个人行为失之约束，就会发生个人失信行为，进而出现集体失信。因此，个人信用体系建设具有极其重要的意义，然而随着经济的发展，越来越重要的信用记录与信用记录的缺失之间的矛盾日益激化，建立完善的信用体系迫在眉睫。随着近年来面向个人的小额贷款业务的不断发展，防范个人信贷欺诈，降低不良率是开展相关业务的首要目标。本届大赛旨在利用上述大数据和人工智能、机器学习相关技术，调动社会全员的大数据建模创新积极性，帮助金融机构准确评估个人信用情况，进一步提高信贷风险防范能力。\n",
    "\n",
    "本届大赛的主题为“开放融合，共建信用”，赛题为“信贷用户逾期预测”，由参赛选手完成大数据算法模型的开发设计，实现对小额信贷业务申请个人欺诈和逾期风险的精准识别，进一步提升金融机构防范欺诈和降低不良率的能力。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95B74C6AA3314EE08B7D9F3002E8D758"
   },
   "source": [
    "\n",
    "\n",
    "## 工具包&数据的导入&数据EDA\n",
    "### 工具包的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "79EFFE7C3F5046518B2F7BEBA465CC06"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBDB5B8028A746C38CEF228BD6010789"
   },
   "source": [
    "### 数据的导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8DE7D321E7374B91A2E0C566F3AE3A46"
   },
   "outputs": [],
   "source": [
    "model_sample = pd.read_csv('./Data/model_sample.csv')\n",
    "model_sample.set_index('user_id',inplace=True)\n",
    "label = model_sample[['y']]\n",
    "model_sample = model_sample.drop('y',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4D741C7BAC247EEB3D9A1A9A1F370FC"
   },
   "source": [
    "### 数据EDA\n",
    "1. 数据中存在较多的缺失值，需要进行特殊处理\n",
    "2. 数据的绝大部分维度特征相对合理，年龄什么的都符合要求，未出现离谱的特征；虽然像申请贷款笔数,成功申请贷款笔数(x_198,x_199)的存在一些相对比较夸张的数值,181,132等,但因为缺少先验信息,所以默认也算合理\n",
    "3. 数据中只存在int和float类型的变量，数据格式相对简单，所以无需进行额外处理\n",
    "4. 数据的个数只有11017个，属于低样本的情况，所以此处选择模型时需要注意过拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "A36D589E57A143CD83A49A925026DFAB"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_001</th>\n",
       "      <th>x_002</th>\n",
       "      <th>x_003</th>\n",
       "      <th>x_004</th>\n",
       "      <th>x_005</th>\n",
       "      <th>x_006</th>\n",
       "      <th>x_007</th>\n",
       "      <th>x_008</th>\n",
       "      <th>x_009</th>\n",
       "      <th>x_010</th>\n",
       "      <th>...</th>\n",
       "      <th>x_190</th>\n",
       "      <th>x_191</th>\n",
       "      <th>x_192</th>\n",
       "      <th>x_193</th>\n",
       "      <th>x_194</th>\n",
       "      <th>x_195</th>\n",
       "      <th>x_196</th>\n",
       "      <th>x_197</th>\n",
       "      <th>x_198</th>\n",
       "      <th>x_199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A00002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00005</th>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00006</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00008</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A00009</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         x_001  x_002  x_003  x_004  x_005  x_006  x_007  x_008  x_009  x_010  \\\n",
       "user_id                                                                         \n",
       "A00002     0.0   32.0      0      0      0      0      0      0      1      1   \n",
       "A00005     0.0   29.0      0      0      0      0      0      0      0      0   \n",
       "A00006     0.0   31.0      0      0      0      0      0      0      0      0   \n",
       "A00008     0.0   22.0      0      0      0      0      0      0      0      0   \n",
       "A00009     0.0   31.0      0      0      0      0      0      0      0      0   \n",
       "\n",
       "         ...    x_190  x_191  x_192  x_193  x_194  x_195  x_196  x_197  x_198  \\\n",
       "user_id  ...                                                                    \n",
       "A00002   ...      1.0    1.0    2.0    2.0    2.0    2.0    2.0    2.0    2.0   \n",
       "A00005   ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "A00006   ...      2.0    2.0    4.0    4.0    6.0    6.0    6.0    6.0   10.0   \n",
       "A00008   ...      3.0    3.0    3.0    3.0    5.0    5.0    3.0    3.0    7.0   \n",
       "A00009   ...      NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "         x_199  \n",
       "user_id         \n",
       "A00002     2.0  \n",
       "A00005     NaN  \n",
       "A00006    10.0  \n",
       "A00008     7.0  \n",
       "A00009     NaN  \n",
       "\n",
       "[5 rows x 199 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "6FD36F047046499E87B5013689F835D9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x_001</th>\n",
       "      <th>x_002</th>\n",
       "      <th>x_003</th>\n",
       "      <th>x_004</th>\n",
       "      <th>x_005</th>\n",
       "      <th>x_006</th>\n",
       "      <th>x_007</th>\n",
       "      <th>x_008</th>\n",
       "      <th>x_009</th>\n",
       "      <th>x_010</th>\n",
       "      <th>...</th>\n",
       "      <th>x_190</th>\n",
       "      <th>x_191</th>\n",
       "      <th>x_192</th>\n",
       "      <th>x_193</th>\n",
       "      <th>x_194</th>\n",
       "      <th>x_195</th>\n",
       "      <th>x_196</th>\n",
       "      <th>x_197</th>\n",
       "      <th>x_198</th>\n",
       "      <th>x_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10986.000000</td>\n",
       "      <td>10986.000000</td>\n",
       "      <td>11017.000000</td>\n",
       "      <td>11017.000000</td>\n",
       "      <td>11017.000000</td>\n",
       "      <td>11017.000000</td>\n",
       "      <td>11017.000000</td>\n",
       "      <td>11017.000000</td>\n",
       "      <td>11017.000000</td>\n",
       "      <td>11017.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>9206.000000</td>\n",
       "      <td>9206.000000</td>\n",
       "      <td>9206.000000</td>\n",
       "      <td>9206.000000</td>\n",
       "      <td>9206.000000</td>\n",
       "      <td>9206.000000</td>\n",
       "      <td>9206.000000</td>\n",
       "      <td>9206.000000</td>\n",
       "      <td>9206.000000</td>\n",
       "      <td>9206.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.155653</td>\n",
       "      <td>31.816221</td>\n",
       "      <td>0.147590</td>\n",
       "      <td>0.075157</td>\n",
       "      <td>0.016066</td>\n",
       "      <td>0.023328</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.010983</td>\n",
       "      <td>...</td>\n",
       "      <td>1.693352</td>\n",
       "      <td>1.573539</td>\n",
       "      <td>3.107756</td>\n",
       "      <td>2.995221</td>\n",
       "      <td>4.628286</td>\n",
       "      <td>4.337497</td>\n",
       "      <td>5.228221</td>\n",
       "      <td>5.032913</td>\n",
       "      <td>8.377037</td>\n",
       "      <td>7.844449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.362542</td>\n",
       "      <td>6.234252</td>\n",
       "      <td>0.354709</td>\n",
       "      <td>0.263656</td>\n",
       "      <td>0.125735</td>\n",
       "      <td>0.150949</td>\n",
       "      <td>0.040390</td>\n",
       "      <td>0.021300</td>\n",
       "      <td>0.142369</td>\n",
       "      <td>0.104228</td>\n",
       "      <td>...</td>\n",
       "      <td>3.365888</td>\n",
       "      <td>2.701433</td>\n",
       "      <td>3.181306</td>\n",
       "      <td>3.076779</td>\n",
       "      <td>6.178321</td>\n",
       "      <td>5.543315</td>\n",
       "      <td>4.284777</td>\n",
       "      <td>4.135471</td>\n",
       "      <td>9.146275</td>\n",
       "      <td>8.431693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>132.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 199 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              x_001         x_002         x_003         x_004         x_005  \\\n",
       "count  10986.000000  10986.000000  11017.000000  11017.000000  11017.000000   \n",
       "mean       0.155653     31.816221      0.147590      0.075157      0.016066   \n",
       "std        0.362542      6.234252      0.354709      0.263656      0.125735   \n",
       "min        0.000000     19.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000     27.250000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000     31.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000     35.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000     59.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              x_006         x_007         x_008         x_009         x_010  \\\n",
       "count  11017.000000  11017.000000  11017.000000  11017.000000  11017.000000   \n",
       "mean       0.023328      0.001634      0.000454      0.020695      0.010983   \n",
       "std        0.150949      0.040390      0.021300      0.142369      0.104228   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "          ...             x_190        x_191        x_192        x_193  \\\n",
       "count     ...       9206.000000  9206.000000  9206.000000  9206.000000   \n",
       "mean      ...          1.693352     1.573539     3.107756     2.995221   \n",
       "std       ...          3.365888     2.701433     3.181306     3.076779   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     1.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     2.000000     2.000000   \n",
       "75%       ...          2.000000     2.000000     5.000000     5.000000   \n",
       "max       ...        164.000000    37.000000    26.000000    26.000000   \n",
       "\n",
       "             x_194        x_195        x_196        x_197        x_198  \\\n",
       "count  9206.000000  9206.000000  9206.000000  9206.000000  9206.000000   \n",
       "mean      4.628286     4.337497     5.228221     5.032913     8.377037   \n",
       "std       6.178321     5.543315     4.284777     4.135471     9.146275   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       1.000000     0.000000     2.000000     2.000000     2.000000   \n",
       "50%       3.000000     2.000000     4.000000     4.000000     6.000000   \n",
       "75%       7.000000     6.000000     8.000000     8.000000    12.000000   \n",
       "max     173.000000    65.000000    39.000000    35.000000   181.000000   \n",
       "\n",
       "             x_199  \n",
       "count  9206.000000  \n",
       "mean      7.844449  \n",
       "std       8.431693  \n",
       "min       0.000000  \n",
       "25%       2.000000  \n",
       "50%       6.000000  \n",
       "75%      11.000000  \n",
       "max     132.000000  \n",
       "\n",
       "[8 rows x 199 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sample.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3BDA6B182EDC4A699B826E244E8B7C78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11017 entries, A00002 to A21941\n",
      "Columns: 199 entries, x_001 to x_199\n",
      "dtypes: float64(161), int64(38)\n",
      "memory usage: 16.8+ MB\n"
     ]
    }
   ],
   "source": [
    "model_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E082ECE3BCB6493F84B7DA92A6F557AD",
    "mdEditEnable": false
   },
   "source": [
    "此处字段都有含义，细节可以参考<font color=red>\"字段解释.xlsx\"</font>\n",
    "\n",
    "![Image Name](https://cdn.kesci.com/user_upload/image/1525246302367_76539.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45066C4991934552A2F22183B247DF3E"
   },
   "source": [
    "- 数据的个数只有11017个，属于低样本的情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "92C72AD7DCDC41E193DBB3CE60B3CD83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11017, 199)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CDFD5C3AE7E471D885CF006AB32044A"
   },
   "source": [
    "# 特征的构建\n",
    "\n",
    "我们此处对特征进行构建，构建的思路是两个：\n",
    "\n",
    "1. 基于模型的表达能力，以及需要什么样的特征，将原先的特征重新进行编码（特征转换）\n",
    "2. 对特征进行组合构建，构建更多具有表示能力的特征\n",
    "\n",
    "## 构建特征集合1\n",
    "###  基于模型的表达能力的特征\n",
    "1. 将身份信息以及财产信息进行编码，0-1进行组合编码（提高模型的表示能力）\n",
    "\n",
    "### 比例特征构建\n",
    "1. 借记卡的比例特征(各种借记卡所占的比例)\n",
    "2. 贷记卡的比例特征(各种贷记卡所占的比例)\n",
    "3. 银行卡的比例特征(各种银行卡所占的比例)\n",
    "4. 失败还款笔数占比\n",
    "5. 失败申请贷款的占比\n",
    "\n",
    "### 标准差还原特征（反映信息的波动）\n",
    "1. 将数据中的标准差进行还原\n",
    "\n",
    "\n",
    "### 均值特征\n",
    "1. 每张卡（例如信用）交易金额等；\n",
    "2. 每笔（例如异地每笔）交易金额等；\n",
    "3. 每笔还款金额等\n",
    "4. 每笔商旅，保险，家装，金融等的均值特征\n",
    "5. 每个月的平均交易笔数\n",
    "6. 每个月的交易金额\n",
    "7. 每笔放款金额，每个机构的放款笔数，每个机构的放款金额\n",
    "8. 每个机构的平均还款金额，每个机构的\n",
    "9. 每个机构的贷款金额\n",
    "10. 其他均值特征\n",
    "\n",
    "### 趋势特征\n",
    "1. 90天与30天的申请贷款机构的趋势，180天与90天的申请贷款机构的趋势，180天与30天的申请贷款机构的趋势\n",
    "2. 90天与30天的成功申请贷款机构的趋势，180天与90天的成功申请贷款机构的趋势，180天与30天的成功申请贷款机构的趋势\n",
    "3. 90天与30天的申请贷款笔数的趋势，180天与90天的申请贷款笔数的趋势，180天与30天的申请贷款笔数的趋势\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "02AA9B1684C645B88F6DF018A54A4239"
   },
   "outputs": [],
   "source": [
    "def get_features_middle(data):\n",
    "    model_sample_strong_feature = data.copy()\n",
    "    # 将身份信息以及财产信息进行编码\n",
    "    first_strong_features = ['x_003','x_004','x_005','x_006','x_007','x_008','x_009','x_010','x_011','x_012','x_013','x_014','x_015','x_016','x_017','x_018','x_019']\n",
    "    res = 0\n",
    "    for i in range(len(first_strong_features)):\n",
    "        res += 2 ** i * data[first_strong_features[i]]\n",
    "\n",
    "    model_sample_strong_feature['x_1_strong'] = res \n",
    "    # 借记卡的比例特征\n",
    "    model_sample_strong_feature['x_022/x_020'] = data['x_022'] / (data['x_020'] + 1e-10)\n",
    "    model_sample_strong_feature['x_023/x_020'] = data['x_023'] / (data['x_020'] + 1e-10)\n",
    "    model_sample_strong_feature['x_024/x_020'] = data['x_024'] /  (data['x_020'] + 1e-10)\n",
    "    model_sample_strong_feature['x_025/x_020'] = data['x_025'] /  (data['x_020']+ 1e-10)\n",
    "    model_sample_strong_feature['x_026/x_020'] = data['x_026'] /  (data['x_020'] + 1e-10)\n",
    "    \n",
    "    # 贷记卡的比例特征\n",
    "    model_sample_strong_feature['x_028/x_021'] = data['x_028'] / (data['x_021'] + 1e-10)\n",
    "    model_sample_strong_feature['x_029/x_021'] = data['x_029'] / (data['x_021'] + 1e-10)\n",
    "    model_sample_strong_feature['x_030/x_021'] = data['x_030'] /  (data['x_021'] + 1e-10)\n",
    "    model_sample_strong_feature['x_031/x_021'] = data['x_031'] /  (data['x_021'] + 1e-10)\n",
    "    model_sample_strong_feature['x_032/x_021'] = data['x_032'] /  (data['x_021'] + 1e-10)\n",
    "    \n",
    "    # 银行卡的比例特征\n",
    "    model_sample_strong_feature['all_cards'] = (data['x_034'] +  data['x_035'] + data['x_036'] + data['x_037'] + data['x_038'] + data['x_039'] + data['x_040']  ).values\n",
    "\n",
    "    model_sample_strong_feature['x_034/all_cards'] = data['x_034'] / (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_035/all_cards'] = data['x_035'] / (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_036/all_cards'] = data['x_036'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_037/all_cards'] = data['x_037'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_038/all_cards'] = data['x_038'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_039/all_cards'] = data['x_039'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_040/all_cards'] = data['x_040'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "   \n",
    "    # 标准差还原\n",
    "    model_sample_strong_feature['x_043/x_044'] = data['x_043'] / (data['x_044'] + 1e-10)\n",
    "    model_sample_strong_feature['x_046/x_047'] = data['x_046'] / (data['x_047'] + 1e-10)\n",
    "    model_sample_strong_feature['x_050/x_051'] = data['x_050'] / (data['x_051'] + 1e-10)\n",
    "    model_sample_strong_feature['x_053/x_054'] = data['x_053'] / (data['x_054'] + 1e-10)\n",
    "    model_sample_strong_feature['x_057/x_058'] = data['x_057'] / (data['x_058'] + 1e-10)\n",
    "    model_sample_strong_feature['x_060/x_061'] = data['x_060'] / (data['x_061'] + 1e-10)\n",
    "    model_sample_strong_feature['x_076/x_077'] = data['x_076'] / (data['x_077'] + 1e-10)\n",
    "    model_sample_strong_feature['x_079/x_080'] = data['x_079'] / (data['x_080'] + 1e-10)\n",
    "    model_sample_strong_feature['x_083/x_084'] = data['x_083'] / (data['x_084'] + 1e-10)\n",
    "    model_sample_strong_feature['x_086/x_087'] = data['x_086'] / (data['x_087'] + 1e-10)\n",
    "    model_sample_strong_feature['x_090/x_091'] = data['x_090'] / (data['x_091'] + 1e-10)\n",
    "    model_sample_strong_feature['x_094/x_095'] = data['x_094'] / (data['x_095'] + 1e-10)\n",
    "    model_sample_strong_feature['x_098/x_099'] = data['x_098'] / (data['x_099'] + 1e-10)\n",
    "    model_sample_strong_feature['x_123/x_124'] = data['x_123'] / (data['x_124'] + 1e-10)\n",
    "    model_sample_strong_feature['x_126/x_127'] = data['x_126'] / (data['x_127'] + 1e-10)\n",
    "\n",
    "    \n",
    "    # 每张卡（信用or其他）交易金额等；每笔（异地每笔）交易金额等；每笔还款金额等；每笔商旅，保险，家装，金融等的均值特征；每个月的平均交易笔数；其他有意义的均值特征\n",
    "    \n",
    "    model_sample_strong_feature['x_045/x_41'] = data['x_045'] / (data['x_041'] + 1e-10)\n",
    "    model_sample_strong_feature['x_052/x_48'] = data['x_052'] / (data['x_048'] + 1e-10)\n",
    "    model_sample_strong_feature['x_059/x_55'] = data['x_059'] / (data['x_055'] + 1e-10) \n",
    "    model_sample_strong_feature['x_064/x_062'] = data['x_064'] / (data['x_062'] + 1e-10)\n",
    "    model_sample_strong_feature['x_067/x_065'] = data['x_067'] / (data['x_065'] + 1e-10)  \n",
    "    model_sample_strong_feature['x_070/x_068'] = data['x_070'] / (data['x_068'] + 1e-10)\n",
    "    model_sample_strong_feature['x_073/x_071'] = data['x_073'] / (data['x_071'] + 1e-10) \n",
    "    model_sample_strong_feature['x_078/x_074'] = data['x_078'] / (data['x_074'] + 1e-10)  \n",
    "    model_sample_strong_feature['x_085/x_081'] = data['x_085'] / (data['x_081'] + 1e-10) \n",
    "    model_sample_strong_feature['x_100/x_101'] = data['x_100'] / (data['x_101'] + 1e-10)\n",
    "    model_sample_strong_feature['x_102/x_103'] = data['x_102'] / (data['x_103'] + 1e-10) \n",
    "    model_sample_strong_feature['x_108/x_105'] = data['x_108'] / (data['x_105'] + 1e-10)\n",
    "    model_sample_strong_feature['x_104/x_102'] = data['x_104'] / (data['x_102'] + 1e-10) \n",
    "    model_sample_strong_feature['x_109/x_110'] = data['x_109'] / (data['x_110'] + 1e-10)\n",
    "    model_sample_strong_feature['x_111/x_109'] = data['x_111'] / (data['x_109'] + 1e-10) \n",
    "    model_sample_strong_feature['x_112/x_113'] = data['x_112'] / (data['x_113'] + 1e-10)\n",
    "    model_sample_strong_feature['x_114/x_112'] = data['x_114'] / (data['x_112'] + 1e-10) \n",
    "    model_sample_strong_feature['x_115/x_116'] = data['x_115'] / (data['x_116'] + 1e-10)\n",
    "    model_sample_strong_feature['x_117/x_115'] = data['x_117'] / (data['x_115'] + 1e-10)  \n",
    "    model_sample_strong_feature['x_118/x_119'] = data['x_118'] / (data['x_119'] + 1e-10)\n",
    "    model_sample_strong_feature['x_120/x_118'] = data['x_120'] / (data['x_118'] + 1e-10) \n",
    "    model_sample_strong_feature['x_125/x_121'] = data['x_125'] / (data['x_121'] + 1e-10) \n",
    "    model_sample_strong_feature['x_128/x_129'] = data['x_128'] / (data['x_129'] + 1e-10)\n",
    "    model_sample_strong_feature['x_130/x_128'] = data['x_130'] / (data['x_128'] + 1e-10)\n",
    "\n",
    "    # 每笔放款金额，每个机构的放款笔数，每个机构的放款金额\n",
    "    model_sample_strong_feature['x_133/x_134'] = data['x_133'] / (data['x_134'] + 1e-10)\n",
    "    model_sample_strong_feature['x_133/x_132'] = data['x_133'] / (data['x_132'] + 1e-10)\n",
    "    model_sample_strong_feature['x_134/x_132'] = data['x_134'] / (data['x_132'] + 1e-10) \n",
    "    model_sample_strong_feature['x_138/x_139'] = data['x_138'] / (data['x_139'] + 1e-10)\n",
    "    model_sample_strong_feature['x_138/x_137'] = data['x_138'] / (data['x_137'] + 1e-10)\n",
    "    model_sample_strong_feature['x_139/x_137'] = data['x_139'] / (data['x_137'] + 1e-10) \n",
    "    model_sample_strong_feature['x_143/x_142'] = data['x_143'] / (data['x_142'] + 1e-10)\n",
    "    model_sample_strong_feature['x_143/x_144'] = data['x_143'] / (data['x_144'] + 1e-10)\n",
    "    model_sample_strong_feature['x_144/x_142'] = data['x_144'] / (data['x_142'] + 1e-10)\n",
    "\n",
    "    # 每个机构的放款均值,失败还款笔数占比\n",
    "    model_sample_strong_feature['x_151/x_149'] = data['x_151'] / (data['x_149'] + 1e-10)\n",
    "    model_sample_strong_feature['x_152/x_149'] = data['x_152'] / (data['x_149'] + 1e-10)\n",
    "    model_sample_strong_feature['x_152/x_151'] = data['x_152'] / (data['x_151'] + 1e-10)\n",
    "    model_sample_strong_feature['x_154/x_153'] = data['x_154'] / (data['x_153'] + 1e-10)\n",
    "    model_sample_strong_feature['x_156/x_153'] = data['x_156'] / (data['x_153'] + 1e-10)\n",
    "    model_sample_strong_feature['x_157/x_153'] = data['x_157'] / (data['x_153'] + 1e-10)\n",
    "    model_sample_strong_feature['x_158/x_153'] = data['x_158'] / (data['x_153'] + 1e-10)\n",
    "    model_sample_strong_feature['x_159/x_153'] = data['x_159'] / (data['x_153'] + 1e-10)  \n",
    "    model_sample_strong_feature['x_154/x_155'] = data['x_154'] / (data['x_155'] + 1e-10)  \n",
    "\n",
    "    model_sample_strong_feature['x_164/x_162'] = data['x_164'] / (data['x_162'] + 1e-10)\n",
    "    model_sample_strong_feature['x_165/x_162'] = data['x_165'] / (data['x_162'] + 1e-10)\n",
    "    model_sample_strong_feature['x_165/x_164'] = data['x_165'] / (data['x_164'] + 1e-10)\n",
    "    model_sample_strong_feature['x_167/x_166'] = data['x_167'] / (data['x_166'] + 1e-10)\n",
    "    model_sample_strong_feature['x_169/x_166'] = data['x_169'] / (data['x_166'] + 1e-10)\n",
    "    model_sample_strong_feature['x_170/x_166'] = data['x_170'] / (data['x_166'] + 1e-10)\n",
    "    model_sample_strong_feature['x_171/x_166'] = data['x_171'] / (data['x_166'] + 1e-10)\n",
    "    model_sample_strong_feature['x_180/x_181'] = data['x_180'] / (data['x_181'] + 1e-10) \n",
    "    model_sample_strong_feature['x_167/x_168'] = data['x_167'] / (data['x_168'] + 1e-10) \n",
    "    model_sample_strong_feature['x_172/x_167'] = data['x_172'] / (data['x_167'] + 1e-10)  \n",
    "\n",
    "    model_sample_strong_feature['x_177/x_175'] = data['x_177'] / (data['x_175'] + 1e-10)\n",
    "    model_sample_strong_feature['x_178/x_175'] = data['x_178'] / (data['x_175'] + 1e-10)\n",
    "    model_sample_strong_feature['x_178/x_177'] = data['x_178'] / (data['x_177'] + 1e-10)\n",
    "    model_sample_strong_feature['x_180/x_179'] = data['x_180'] / (data['x_179'] + 1e-10)\n",
    "    model_sample_strong_feature['x_182/x_179'] = data['x_182'] / (data['x_179'] + 1e-10)\n",
    "    model_sample_strong_feature['x_183/x_179'] = data['x_183'] / (data['x_179'] + 1e-10)\n",
    "    model_sample_strong_feature['x_184/x_179'] = data['x_184'] / (data['x_179'] + 1e-10)\n",
    "    model_sample_strong_feature['x_180/x_181'] = data['x_180'] / (data['x_181'] + 1e-10) \n",
    "    model_sample_strong_feature['x_185/x_180'] = data['x_185'] / (data['x_180'] + 1e-10)\n",
    " \n",
    "    # 90天与30天的申请贷款机构的趋势，180天与90天的申请贷款机构的趋势，180天与30天的申请贷款机构的趋势；90天与30天的成功申请贷款机构的趋势，180天与90天的成功申请贷款机构的趋势，180天；\n",
    "    # 30天的成功申请贷款机构的趋势；90天与30天的申请贷款笔数的趋势，180天与90天的申请贷款笔数的趋势，180天与30天的申请贷款笔数的趋势90天的申请贷款笔数的趋势\n",
    "    model_sample_strong_feature['x_189/x_188'] = data['x_189'] / (data['x_188'] + 1e-10)\n",
    "    model_sample_strong_feature['x_191/x_190'] = data['x_191'] / (data['x_190'] + 1e-10)\n",
    "    model_sample_strong_feature['x_193/x_192'] = data['x_193'] / (data['x_192'] + 1e-10)\n",
    "    model_sample_strong_feature['x_195/x_194'] = data['x_195'] / (data['x_194'] + 1e-10)\n",
    "    model_sample_strong_feature['x_197/x_196'] = data['x_197'] / (data['x_196'] + 1e-10)\n",
    "    model_sample_strong_feature['x_199/x_198'] = data['x_199'] / (data['x_198'] + 1e-10)\n",
    "    model_sample_strong_feature['x_196/x_188'] = data['x_196'] / (data['x_188'] + 1e-10)\n",
    "    model_sample_strong_feature['x_192/x_188'] = data['x_192'] / (data['x_188'] + 1e-10)\n",
    "                                                                   \n",
    "    model_sample_strong_feature = model_sample_strong_feature.fillna(-999)\n",
    "    return model_sample_strong_feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59D70C52B01C4832B9A13A34AD33C0C1"
   },
   "source": [
    "## 构建特征集合2\n",
    "- 该集合是上面集合的一个子集，用作融合使用（该集合在5折的时候）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FBB1AAA9CD25473C8EA872A54AB42F11"
   },
   "outputs": [],
   "source": [
    "def get_features_final(data):\n",
    "    model_sample_strong_feature = data.copy()\n",
    "    \n",
    "    first_strong_features = ['x_003','x_004','x_005','x_006','x_007','x_008','x_009','x_010','x_011','x_012','x_013','x_014','x_015','x_016','x_017','x_018','x_019']\n",
    "\n",
    "    res = 0\n",
    "    for i in range(len(first_strong_features)):\n",
    "        res += 2 ** i * data[first_strong_features[i]] \n",
    "    model_sample_strong_feature['x_1_strong'] = res\n",
    "\n",
    "    model_sample_strong_feature['x_022/x_020'] = data['x_022'] / (data['x_020'] + 1e-10)\n",
    "    model_sample_strong_feature['x_023/x_020'] = data['x_023'] / (data['x_020'] + 1e-10)\n",
    "    model_sample_strong_feature['x_024/x_020'] = data['x_024'] /  (data['x_020'] + 1e-10)\n",
    "    model_sample_strong_feature['x_025/x_020'] = data['x_025'] /  (data['x_020'] + 1e-10)\n",
    "    model_sample_strong_feature['x_026/x_020'] = data['x_026'] /  (data['x_020'] + 1e-10)\n",
    "    \n",
    "    \n",
    "    model_sample_strong_feature['x_028/x_021'] = data['x_028'] / (data['x_021'] + 1e-10)\n",
    "    model_sample_strong_feature['x_029/x_021'] = data['x_029'] / (data['x_021'] + 1e-10)\n",
    "    model_sample_strong_feature['x_030/x_021'] = data['x_030'] /  (data['x_021'] + 1e-10)\n",
    "    model_sample_strong_feature['x_031/x_021'] = data['x_031'] /  (data['x_021'] + 1e-10)\n",
    "    model_sample_strong_feature['x_032/x_021'] = data['x_032'] /  (data['x_021'] + 1e-10)\n",
    "    \n",
    "    \n",
    "    model_sample_strong_feature['all_cards'] = (data['x_034']  + data['x_035'] + data['x_036'] + data['x_037'] + data['x_038'] + data['x_039'] + data['x_040']).values\n",
    "\n",
    "    model_sample_strong_feature['x_034/all_cards'] = data['x_034'] / (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_035/all_cards'] = data['x_035'] / (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_036/all_cards'] = data['x_036'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_037/all_cards'] = data['x_037'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_038/all_cards'] = data['x_038'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_039/all_cards'] = data['x_039'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "    model_sample_strong_feature['x_040/all_cards'] = data['x_040'] /  (model_sample_strong_feature['all_cards'] + 1e-10)\n",
    "\n",
    "    model_sample_strong_feature['x_027/x_033'] = data['x_027'] -  (data['x_033'] + 1e-10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model_sample_strong_feature['x_043/x_044'] = data['x_043'] / (data['x_044'] + 1e-10)\n",
    "    model_sample_strong_feature['x_046/x_047'] = data['x_046'] / (data['x_047'] + 1e-10)\n",
    "    model_sample_strong_feature['x_050/x_051'] = data['x_050'] / (data['x_051'] + 1e-10)\n",
    "    model_sample_strong_feature['x_053/x_054'] = data['x_053'] / (data['x_054'] + 1e-10)\n",
    "    model_sample_strong_feature['x_057/x_058'] = data['x_057'] / (data['x_058'] + 1e-10)\n",
    "    model_sample_strong_feature['x_060/x_061'] = data['x_060'] / (data['x_061'] + 1e-10)\n",
    "    model_sample_strong_feature['x_076/x_077'] = data['x_076'] / (data['x_077'] + 1e-10)\n",
    "    model_sample_strong_feature['x_079/x_080'] = data['x_079'] / (data['x_080'] + 1e-10)\n",
    "    model_sample_strong_feature['x_083/x_084'] = data['x_083'] / (data['x_084'] + 1e-10) \n",
    "    model_sample_strong_feature['x_086/x_087'] = data['x_086'] / (data['x_087'] + 1e-10)\n",
    "    model_sample_strong_feature['x_090/x_091'] = data['x_090'] / (data['x_091'] + 1e-10)\n",
    "    model_sample_strong_feature['x_094/x_095'] = data['x_094'] / (data['x_095'] + 1e-10)\n",
    "    model_sample_strong_feature['x_098/x_099'] = data['x_098'] / (data['x_099'] + 1e-10) \n",
    "    model_sample_strong_feature['x_123/x_124'] = data['x_123'] / (data['x_124'] + 1e-10)\n",
    "    model_sample_strong_feature['x_126/x_127'] = data['x_126'] / (data['x_127'] + 1e-10) \n",
    "    \n",
    "    \n",
    "    model_sample_strong_feature['x_064/x_063'] = data['x_064'] / (data['x_063'] + 1e-10)\n",
    "    model_sample_strong_feature['x_067/x_066'] = data['x_067'] / (data['x_066'] + 1e-10)\n",
    "    model_sample_strong_feature['x_070/x_069'] = data['x_070'] / (data['x_069'] + 1e-10)\n",
    "    model_sample_strong_feature['x_073/x_072'] = data['x_073'] / (data['x_072'] + 1e-10)\n",
    "    \n",
    "    model_sample_strong_feature['x_059/x_55'] = data['x_059'] / (data['x_055'] + 1e-10)\n",
    "    model_sample_strong_feature['x_067/x_065'] = data['x_067'] / (data['x_065'] + 1e-10)\n",
    "    model_sample_strong_feature['x_070/x_068'] = data['x_070'] / (data['x_068'] + 1e-10)\n",
    "    model_sample_strong_feature['x_073/x_071'] = data['x_073'] / (data['x_071'] + 1e-10)\n",
    "    model_sample_strong_feature['x_078/x_074'] = data['x_078'] / (data['x_074'] + 1e-10)\n",
    "    model_sample_strong_feature['x_085/x_081'] = data['x_085'] / (data['x_081'] + 1e-10)\n",
    "    model_sample_strong_feature['x_100/x_101'] = data['x_100'] / (data['x_101'] + 1e-10)\n",
    "    model_sample_strong_feature['x_102/x_103'] = data['x_102'] / (data['x_103'] + 1e-10)\n",
    "    model_sample_strong_feature['x_108/x_105'] = data['x_108'] / (data['x_105'] + 1e-10)\n",
    "    model_sample_strong_feature['x_104/x_102'] = data['x_104'] / (data['x_102'] + 1e-10)\n",
    "    model_sample_strong_feature['x_109/x_110'] = data['x_109'] / (data['x_110'] + 1e-10)\n",
    "    model_sample_strong_feature['x_111/x_109'] = data['x_111'] / (data['x_109'] + 1e-10)\n",
    "    model_sample_strong_feature['x_112/x_113'] = data['x_112'] / (data['x_113'] + 1e-10)\n",
    "    model_sample_strong_feature['x_114/x_112'] = data['x_114'] / (data['x_112'] + 1e-10)\n",
    "    model_sample_strong_feature['x_115/x_116'] = data['x_115'] / (data['x_116'] + 1e-10)\n",
    "    model_sample_strong_feature['x_117/x_115'] = data['x_117'] / (data['x_115'] + 1e-10) \n",
    "    model_sample_strong_feature['x_118/x_119'] = data['x_118'] / (data['x_119'] + 1e-10)\n",
    "    model_sample_strong_feature['x_120/x_118'] = data['x_120'] / (data['x_118'] + 1e-10) \n",
    "    model_sample_strong_feature['x_125/x_121'] = data['x_125'] / (data['x_121'] + 1e-10) \n",
    "    model_sample_strong_feature['x_128/x_129'] = data['x_128'] / (data['x_129'] + 1e-10)\n",
    "    model_sample_strong_feature['x_130/x_128'] = data['x_130'] / (data['x_128'] + 1e-10) \n",
    "    \n",
    "    \n",
    "    model_sample_strong_feature['x_133/x_134'] = data['x_133'] / (data['x_134'] + 1e-10)\n",
    "    model_sample_strong_feature['x_133/x_132'] = data['x_133'] / (data['x_132'] + 1e-10)\n",
    "    model_sample_strong_feature['x_134/x_132'] = data['x_134'] / (data['x_132'] + 1e-10)\n",
    "\n",
    "    model_sample_strong_feature['x_138/x_139'] = data['x_138'] / (data['x_139'] + 1e-10)\n",
    "    model_sample_strong_feature['x_138/x_137'] = data['x_138'] / (data['x_137'] + 1e-10)\n",
    "    model_sample_strong_feature['x_139/x_137'] = data['x_139'] / (data['x_137'] + 1e-10)\n",
    "\n",
    "    model_sample_strong_feature['x_143/x_142'] = data['x_143'] / (data['x_142'] + 1e-10)\n",
    "    model_sample_strong_feature['x_143/x_144'] = data['x_143'] / (data['x_144'] + 1e-10)\n",
    "    model_sample_strong_feature['x_144/x_142'] = data['x_144'] / (data['x_142'] + 1e-10)\n",
    "\n",
    "    model_sample_strong_feature['x_151/x_149'] = data['x_151'] / (data['x_149'] + 1e-10)\n",
    "    model_sample_strong_feature['x_152/x_149'] = data['x_152'] / (data['x_149'] + 1e-10)\n",
    "    model_sample_strong_feature['x_152/x_151'] = data['x_152'] / (data['x_151'] + 1e-10)\n",
    "    model_sample_strong_feature['x_154/x_153'] = data['x_154'] / (data['x_153'] + 1e-10)\n",
    "    model_sample_strong_feature['x_156/x_153'] = data['x_156'] / (data['x_153'] + 1e-10)\n",
    "    model_sample_strong_feature['x_157/x_153'] = data['x_157'] / (data['x_153'] + 1e-10)\n",
    "    model_sample_strong_feature['x_158/x_153'] = data['x_158'] / (data['x_153'] + 1e-10)\n",
    "    model_sample_strong_feature['x_159/x_153'] = data['x_159'] / (data['x_153'] + 1e-10)  \n",
    "    model_sample_strong_feature['x_154/x_155'] = data['x_154'] / (data['x_155'] + 1e-10)  \n",
    "\n",
    "\n",
    "    model_sample_strong_feature['x_164/x_162'] = data['x_164'] / (data['x_162'] + 1e-10)\n",
    "    model_sample_strong_feature['x_165/x_162'] = data['x_165'] / (data['x_162'] + 1e-10)\n",
    "    model_sample_strong_feature['x_165/x_164'] = data['x_165'] / (data['x_164'] + 1e-10)\n",
    "    model_sample_strong_feature['x_167/x_166'] = data['x_167'] / (data['x_166'] + 1e-10)\n",
    "    model_sample_strong_feature['x_169/x_166'] = data['x_169'] / (data['x_166'] + 1e-10)\n",
    "    model_sample_strong_feature['x_170/x_166'] = data['x_170'] / (data['x_166'] + 1e-10)\n",
    "    model_sample_strong_feature['x_171/x_166'] = data['x_171'] / (data['x_166'] + 1e-10)\n",
    "    model_sample_strong_feature['x_180/x_181'] = data['x_180'] / (data['x_181'] + 1e-10) \n",
    "    model_sample_strong_feature['x_167/x_168'] = data['x_167'] / (data['x_168'] + 1e-10) \n",
    "    model_sample_strong_feature['x_172/x_167'] = data['x_172'] / (data['x_167'] + 1e-10)  \n",
    "    \n",
    "    model_sample_strong_feature['x_177/x_175'] = data['x_177'] / (data['x_175'] + 1e-10)\n",
    "    model_sample_strong_feature['x_178/x_175'] = data['x_178'] / (data['x_175'] + 1e-10)\n",
    "    model_sample_strong_feature['x_178/x_177'] = data['x_178'] / (data['x_177'] + 1e-10)\n",
    "    model_sample_strong_feature['x_180/x_179'] = data['x_180'] / (data['x_179'] + 1e-10)\n",
    "    model_sample_strong_feature['x_182/x_179'] = data['x_182'] / (data['x_179'] + 1e-10)\n",
    "    model_sample_strong_feature['x_183/x_179'] = data['x_183'] / (data['x_179'] + 1e-10)\n",
    "    model_sample_strong_feature['x_184/x_179'] = data['x_184'] / (data['x_179'] + 1e-10)\n",
    "    model_sample_strong_feature['x_180/x_181'] = data['x_180'] / (data['x_181'] + 1e-10) \n",
    "    model_sample_strong_feature['x_185/x_180'] = data['x_185'] / (data['x_180'] + 1e-10)\n",
    " \n",
    "    model_sample_strong_feature['x_189/x_188'] = data['x_189'] / (data['x_188'] + 1e-10)\n",
    "    model_sample_strong_feature['x_191/x_190'] = data['x_191'] / (data['x_190'] + 1e-10)\n",
    "    model_sample_strong_feature['x_193/x_192'] = data['x_193'] / (data['x_192'] + 1e-10)\n",
    "    model_sample_strong_feature['x_195/x_194'] = data['x_195'] / (data['x_194'] + 1e-10)\n",
    "    model_sample_strong_feature['x_197/x_196'] = data['x_197'] / (data['x_196'] + 1e-10)\n",
    "    model_sample_strong_feature['x_199/x_198'] = data['x_199'] / (data['x_198'] + 1e-10)\n",
    "    model_sample_strong_feature['x_196/x_188'] = data['x_196'] / (data['x_188'] + 1e-10)\n",
    "    model_sample_strong_feature['x_192/x_188'] = data['x_192'] / (data['x_188'] + 1e-10)\n",
    "\n",
    "    \n",
    "    model_sample_strong_feature = model_sample_strong_feature.fillna(-999)\n",
    "    return model_sample_strong_feature "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6D88C05FFCA47A39FC364F561F67D7A"
   },
   "source": [
    "# 模型训练\n",
    "## 指标的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "874E84C2B5D54EF68BF710A3319412FE"
   },
   "outputs": [],
   "source": [
    "def get_score(y_pred, y_true):\n",
    "    acc_ = accuracy_score(y_true=y_true,y_pred=y_pred)\n",
    "    TP = np.sum(((y_pred == 1) & (y_true == 1))) \n",
    "    precision = TP / np.sum(y_pred)\n",
    "    recall = TP / np.sum(y_true)\n",
    "    print('TP: ',TP,'/', np.sum(y_true), 'all ',np.sum(y_pred), ' accuracy: ',acc_, ' precision: ',precision, ' recall: ',recall, ' F_score: ', 2 * precision * recall / (precision + recall),fbeta_score(y_true=y_true,y_pred=y_pred,beta=1) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "404755A1237B4B4F80571D64760EAE9F"
   },
   "source": [
    "### 获取topN重要的特征\n",
    "防止模型过拟合，同时以我们的经验判断，用于融合也可以带来较好的提升。\n",
    "- 因为训练时间的问题，我们最终的模型没有加入该类融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4EA31BDE17F74BA0A381B5C34EB5EF00"
   },
   "outputs": [],
   "source": [
    "def get_top_features(feature,model,topN):\n",
    "    feature_importance = pd.DataFrame({'feature':feature,'importance':model.feature_importance()})\n",
    "    feature_importance = feature_importance.sort_values('importance',ascending=False)\n",
    "    feature_importance = feature_importance.loc[feature_importance['importance'] > 0]\n",
    "    if feature_importance.shape[0] >= topN: \n",
    "        return feature_importance['feature'][:topN]\n",
    "    else:\n",
    "        return feature_importance['feature']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8D9688354DA54D628B8094D922ABFA1C"
   },
   "source": [
    "## 模型训练与验证\n",
    "因为原始的数据只有11000多个，数据相对较少，为了防止模型过拟合，我们选择采用两种融合的方法\n",
    "1. 多个不同数据子集进行模型的训练\n",
    "2. 多个模型融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "75C44B07022543CC8C2DA95D817BE8E7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def N_Fold_Predict( train_fea , train_y,  test_fea, cv_ = 5):\n",
    "\n",
    "    ###########################################################\n",
    "    train_fea = train_fea.fillna(-1)\n",
    "    test_fea = test_fea.fillna(-1)\n",
    "\n",
    "    features_col = [c for c in train_fea.columns if c not in ['user_id','y']]\n",
    "    X = train_fea[features_col] \n",
    "    X_pred = test_fea[features_col]\n",
    "    \n",
    "    pred_out_lgb = 0\n",
    "    pred_out_gbdt = 0\n",
    "    pred_out_rf = 0\n",
    "    \n",
    "    for cv in range(cv_):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, train_y, test_size=0.25, random_state=np.random.randint(1000)) \n",
    "        # create dataset for lightgbm\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "        # specify your configurations as a dict\n",
    "        params = {\n",
    "            'task':'train',\n",
    "            'boosting_type':'gbdt',\n",
    "            'num_leaves': 31,\n",
    "            'objective': 'binary', \n",
    "            'learning_rate': 0.05, \n",
    "            'bagging_freq': 2, \n",
    "            'max_bin':256,\n",
    "            'num_threads': 32\n",
    "        } \n",
    "\n",
    "        # train\n",
    "        gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    verbose_eval= 0,\n",
    "                    num_boost_round=10000,\n",
    "                    valid_sets=lgb_eval,\n",
    "                    early_stopping_rounds=100)\n",
    "\n",
    "        lgb_pred = gbm.predict(X_pred, num_iteration=gbm.best_iteration)\n",
    "        \n",
    "        \n",
    "        gbdt = GradientBoostingClassifier(n_estimators=250,learning_rate=0.01,max_depth=6,min_samples_leaf=5,min_samples_split=5)\n",
    "        gbdt.fit(X_train, y_train)\n",
    "        gbdt_pred = gbdt.predict_proba(X_pred)[:,1] \n",
    "        \n",
    "        \n",
    "        rf = RandomForestClassifier(n_estimators=500,max_depth=6,min_samples_leaf=5,min_samples_split=5)\n",
    "        rf.fit(X_train, y_train)\n",
    "        rf_pred = rf.predict_proba(X_pred)[:,1]  \n",
    "        \n",
    "        if cv == 0:\n",
    "            pred_out_lgb = lgb_pred\n",
    "            pred_out_gbdt = gbdt_pred\n",
    "            pred_out_rf = rf_pred\n",
    "        else:\n",
    "            pred_out_lgb += lgb_pred\n",
    "            pred_out_gbdt += gbdt_pred\n",
    "            pred_out_rf += rf_pred\n",
    "            \n",
    "    pred_out_lgb = pred_out_lgb * 1.0 / cv_\n",
    "    pred_out_gbdt = pred_out_gbdt * 1.0 / cv_\n",
    "    pred_out_rf = pred_out_rf * 1.0 / cv_\n",
    "    return pred_out_lgb, pred_out_gbdt, pred_out_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4C5B6660B9B6437187FF4ADC8954DAF3"
   },
   "source": [
    " ### 获取不同子集数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EB2B62CD33DD4A78A055287058A01425"
   },
   "outputs": [],
   "source": [
    "model_sample_strong_feature_middle = get_features_middle(model_sample)\n",
    "model_sample_strong_feature_final = get_features_final(model_sample)\n",
    "model_sample_strong_feature_middle = model_sample_strong_feature_middle.fillna(-999)\n",
    "model_sample_strong_feature_final = model_sample_strong_feature_final.fillna(-999)\n",
    "model_sample_ = model_sample.fillna(-999)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "672D548AFF5942AC900841560B861C95"
   },
   "source": [
    "### 多个模型训练与测试\n",
    "1. 解决F指标问题，我们选择采用调整阈值的方式，此处我们选择0.215作为我们最后的阈值（还可以调整权重等）\n",
    "2. 为了避免模型的过拟合，我们选择多个模型融合的方式(此处采用简单的加权融合的方式进行，因为stacking比较耗时，可能会超过30分钟）\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "314209913CA240FE99F17AC0BE772CCA",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed is:  1\n",
      "5 fold no feature engineering\n",
      "TP:  335 / 442 all  896  accuracy:  0.6969147005444646  precision:  0.37388392857142855  recall:  0.7579185520361991  F_score:  0.5007473841554558 0.5007473841554558\n",
      "TP:  303 / 442 all  792  accuracy:  0.7150635208711433  precision:  0.38257575757575757  recall:  0.6855203619909502  F_score:  0.49108589951377635 0.49108589951377635\n",
      "TP:  301 / 442 all  754  accuracy:  0.7304900181488203  precision:  0.39920424403183025  recall:  0.6809954751131222  F_score:  0.5033444816053512 0.5033444816053512\n",
      "TP:  298 / 442 all  766  accuracy:  0.7223230490018149  precision:  0.38903394255874674  recall:  0.6742081447963801  F_score:  0.4933774834437086 0.4933774834437086\n",
      "TP:  299 / 442 all  770  accuracy:  0.721415607985481  precision:  0.38831168831168833  recall:  0.6764705882352942  F_score:  0.4933993399339934 0.4933993399339934\n",
      "TP:  306 / 442 all  783  accuracy:  0.7218693284936479  precision:  0.39080459770114945  recall:  0.6923076923076923  F_score:  0.49959183673469393 0.49959183673469393\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  335 / 442 all  896  accuracy:  0.6969147005444646  precision:  0.37388392857142855  recall:  0.7579185520361991  F_score:  0.5007473841554558 0.5007473841554558\n",
      "TP:  304 / 442 all  782  accuracy:  0.720508166969147  precision:  0.3887468030690537  recall:  0.6877828054298643  F_score:  0.49673202614379075 0.49673202614379075\n",
      "TP:  299 / 442 all  761  accuracy:  0.7254990925589837  precision:  0.392904073587385  recall:  0.6764705882352942  F_score:  0.4970906068162925 0.4970906068162925\n",
      "TP:  302 / 442 all  767  accuracy:  0.7254990925589837  precision:  0.39374185136897  recall:  0.6832579185520362  F_score:  0.499586435070306 0.499586435070306\n",
      "TP:  303 / 442 all  769  accuracy:  0.7254990925589837  precision:  0.3940182054616385  recall:  0.6855203619909502  F_score:  0.500412881915772 0.500412881915772\n",
      "TP:  306 / 442 all  779  accuracy:  0.7236842105263158  precision:  0.392811296534018  recall:  0.6923076923076923  F_score:  0.5012285012285012 0.5012285012285012\n",
      "TP:  307 / 442 all  778  accuracy:  0.7250453720508166  precision:  0.39460154241645246  recall:  0.6945701357466063  F_score:  0.5032786885245902 0.5032786885245902\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  345 / 442 all  917  accuracy:  0.6964609800362976  precision:  0.376226826608506  recall:  0.7805429864253394  F_score:  0.5077262693156733 0.5077262693156733\n",
      "TP:  316 / 442 all  792  accuracy:  0.7268602540834845  precision:  0.398989898989899  recall:  0.7149321266968326  F_score:  0.5121555915721232 0.5121555915721232\n",
      "TP:  306 / 442 all  769  accuracy:  0.7282214156079855  precision:  0.39791937581274384  recall:  0.6923076923076923  F_score:  0.5053674649050373 0.5053674649050373\n",
      "TP:  309 / 442 all  791  accuracy:  0.7209618874773139  precision:  0.3906447534766119  recall:  0.6990950226244343  F_score:  0.5012165450121654 0.5012165450121654\n",
      "TP:  309 / 442 all  793  accuracy:  0.72005444646098  precision:  0.3896595208070618  recall:  0.6990950226244343  F_score:  0.5004048582995952 0.5004048582995952\n",
      "TP:  311 / 442 all  798  accuracy:  0.7196007259528131  precision:  0.38972431077694236  recall:  0.7036199095022625  F_score:  0.5016129032258065 0.5016129032258065\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  305 / 442 all  767  accuracy:  0.7282214156079855  precision:  0.39765319426336376  recall:  0.6900452488687783  F_score:  0.5045492142266336 0.5045492142266336\n",
      "TP:  305 / 442 all  770  accuracy:  0.7268602540834845  precision:  0.3961038961038961  recall:  0.6900452488687783  F_score:  0.5033003300330033 0.5033003300330033\n",
      "final and  original \n",
      "TP:  307 / 442 all  781  accuracy:  0.7236842105263158  precision:  0.39308578745198464  recall:  0.6945701357466063  F_score:  0.5020441537203597 0.5020441537203597\n",
      "TP:  308 / 442 all  783  accuracy:  0.7236842105263158  precision:  0.3933588761174968  recall:  0.6968325791855203  F_score:  0.5028571428571428 0.5028571428571428\n",
      "final and  middle and original \n",
      "TP:  307 / 442 all  778  accuracy:  0.7250453720508166  precision:  0.39460154241645246  recall:  0.6945701357466063  F_score:  0.5032786885245902 0.5032786885245902\n",
      "TP:  306 / 442 all  777  accuracy:  0.7245916515426497  precision:  0.3938223938223938  recall:  0.6923076923076923  F_score:  0.5020508613617719 0.5020508613617719\n",
      "TP:  313 / 442 all  791  accuracy:  0.7245916515426497  precision:  0.39570164348925413  recall:  0.7081447963800905  F_score:  0.5077047850770479 0.5077047850770479\n",
      "TP:  314 / 442 all  795  accuracy:  0.7236842105263158  precision:  0.3949685534591195  recall:  0.7104072398190046  F_score:  0.50767987065481 0.50767987065481\n",
      "Random Seed is:  10\n",
      "5 fold no feature engineering\n",
      "TP:  337 / 438 all  932  accuracy:  0.6842105263157895  precision:  0.361587982832618  recall:  0.769406392694064  F_score:  0.491970802919708 0.491970802919708\n",
      "TP:  313 / 438 all  815  accuracy:  0.7155172413793104  precision:  0.38404907975460123  recall:  0.7146118721461188  F_score:  0.4996009577015164 0.4996009577015164\n",
      "TP:  334 / 438 all  849  accuracy:  0.719147005444646  precision:  0.3934040047114252  recall:  0.7625570776255708  F_score:  0.5190365190365192 0.5190365190365192\n",
      "TP:  325 / 438 all  827  accuracy:  0.7209618874773139  precision:  0.39298669891172916  recall:  0.7420091324200914  F_score:  0.5138339920948617 0.5138339920948617\n",
      "TP:  323 / 438 all  826  accuracy:  0.7196007259528131  precision:  0.3910411622276029  recall:  0.7374429223744292  F_score:  0.5110759493670886 0.5110759493670886\n",
      "TP:  326 / 438 all  837  accuracy:  0.7173321234119783  precision:  0.38948626045400236  recall:  0.7442922374429224  F_score:  0.5113725490196078 0.5113725490196078\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  348 / 438 all  946  accuracy:  0.6878402903811253  precision:  0.3678646934460888  recall:  0.7945205479452054  F_score:  0.5028901734104045 0.5028901734104045\n",
      "TP:  329 / 438 all  811  accuracy:  0.7318511796733213  precision:  0.40567200986436497  recall:  0.7511415525114156  F_score:  0.5268214571657326 0.5268214571657326\n",
      "TP:  343 / 438 all  882  accuracy:  0.7123411978221416  precision:  0.3888888888888889  recall:  0.7831050228310502  F_score:  0.5196969696969698 0.5196969696969698\n",
      "TP:  336 / 438 all  846  accuracy:  0.7223230490018149  precision:  0.3971631205673759  recall:  0.7671232876712328  F_score:  0.5233644859813084 0.5233644859813084\n",
      "TP:  336 / 438 all  843  accuracy:  0.7236842105263158  precision:  0.398576512455516  recall:  0.7671232876712328  F_score:  0.5245901639344263 0.5245901639344263\n",
      "TP:  337 / 438 all  852  accuracy:  0.720508166969147  precision:  0.3955399061032864  recall:  0.769406392694064  F_score:  0.5224806201550387 0.5224806201550387\n",
      "TP:  337 / 438 all  853  accuracy:  0.72005444646098  precision:  0.3950762016412661  recall:  0.769406392694064  F_score:  0.5220759101471728 0.5220759101471728\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  343 / 438 all  929  accuracy:  0.691016333938294  precision:  0.36921420882669537  recall:  0.7831050228310502  F_score:  0.5018288222384785 0.5018288222384785\n",
      "TP:  316 / 438 all  805  accuracy:  0.7227767695099818  precision:  0.39254658385093166  recall:  0.7214611872146118  F_score:  0.5084473049074819 0.5084473049074819\n",
      "TP:  321 / 438 all  797  accuracy:  0.7309437386569873  precision:  0.4027603513174404  recall:  0.7328767123287672  F_score:  0.5198380566801619 0.5198380566801619\n",
      "TP:  317 / 438 all  800  accuracy:  0.7259528130671506  precision:  0.39625  recall:  0.723744292237443  F_score:  0.5121163166397416 0.5121163166397416\n",
      "TP:  320 / 438 all  804  accuracy:  0.7268602540834845  precision:  0.39800995024875624  recall:  0.730593607305936  F_score:  0.5152979066022545 0.5152979066022545\n",
      "TP:  323 / 438 all  814  accuracy:  0.7250453720508166  precision:  0.3968058968058968  recall:  0.7374429223744292  F_score:  0.5159744408945687 0.5159744408945687\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  335 / 438 all  843  accuracy:  0.7227767695099818  precision:  0.3973902728351127  recall:  0.7648401826484018  F_score:  0.5230288836846214 0.5230288836846214\n",
      "TP:  335 / 438 all  843  accuracy:  0.7227767695099818  precision:  0.3973902728351127  recall:  0.7648401826484018  F_score:  0.5230288836846214 0.5230288836846214\n",
      "final and  original \n",
      "TP:  327 / 438 all  823  accuracy:  0.7245916515426497  precision:  0.3973268529769137  recall:  0.7465753424657534  F_score:  0.5186360031720856 0.5186360031720856\n",
      "TP:  326 / 438 all  821  accuracy:  0.7245916515426497  precision:  0.39707673568818513  recall:  0.7442922374429224  F_score:  0.517871326449563 0.517871326449563\n",
      "final and  middle and original \n",
      "TP:  334 / 438 all  830  accuracy:  0.7277676950998185  precision:  0.40240963855421685  recall:  0.7625570776255708  F_score:  0.5268138801261829 0.5268138801261829\n",
      "TP:  333 / 438 all  828  accuracy:  0.7277676950998185  precision:  0.40217391304347827  recall:  0.7602739726027398  F_score:  0.5260663507109006 0.5260663507109006\n",
      "TP:  335 / 438 all  835  accuracy:  0.7264065335753176  precision:  0.40119760479041916  recall:  0.7648401826484018  F_score:  0.5263157894736842 0.5263157894736842\n",
      "TP:  336 / 438 all  839  accuracy:  0.7254990925589837  precision:  0.40047675804529204  recall:  0.7671232876712328  F_score:  0.5262333594361785 0.5262333594361785\n",
      "Random Seed is:  100\n",
      "5 fold no feature engineering\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  327 / 430 all  922  accuracy:  0.6833030852994555  precision:  0.3546637744034707  recall:  0.7604651162790698  F_score:  0.4837278106508875 0.4837278106508875\n",
      "TP:  309 / 430 all  817  accuracy:  0.7146098003629764  precision:  0.37821297429620565  recall:  0.7186046511627907  F_score:  0.49558941459502814 0.49558941459502814\n",
      "TP:  314 / 430 all  837  accuracy:  0.7100725952813067  precision:  0.3751493428912784  recall:  0.7302325581395349  F_score:  0.49565903709550124 0.49565903709550124\n",
      "TP:  314 / 430 all  832  accuracy:  0.7123411978221416  precision:  0.37740384615384615  recall:  0.7302325581395349  F_score:  0.49762282091917587 0.49762282091917587\n",
      "TP:  313 / 430 all  830  accuracy:  0.7123411978221416  precision:  0.37710843373493974  recall:  0.727906976744186  F_score:  0.49682539682539684 0.49682539682539684\n",
      "TP:  314 / 430 all  835  accuracy:  0.7109800362976406  precision:  0.37604790419161677  recall:  0.7302325581395349  F_score:  0.49644268774703554 0.49644268774703554\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  338 / 430 all  931  accuracy:  0.6892014519056261  precision:  0.36305048335123524  recall:  0.786046511627907  F_score:  0.4966936076414401 0.4966936076414401\n",
      "TP:  315 / 430 all  822  accuracy:  0.7177858439201452  precision:  0.38321167883211676  recall:  0.7325581395348837  F_score:  0.5031948881789138 0.5031948881789138\n",
      "TP:  322 / 430 all  862  accuracy:  0.705989110707804  precision:  0.37354988399071926  recall:  0.7488372093023256  F_score:  0.49845201238390086 0.49845201238390086\n",
      "TP:  325 / 430 all  863  accuracy:  0.7082577132486388  precision:  0.3765932792584009  recall:  0.7558139534883721  F_score:  0.502706883217324 0.502706883217324\n",
      "TP:  323 / 430 all  856  accuracy:  0.7096188747731398  precision:  0.3773364485981308  recall:  0.7511627906976744  F_score:  0.5023328149300155 0.5023328149300155\n",
      "TP:  324 / 430 all  858  accuracy:  0.7096188747731398  precision:  0.3776223776223776  recall:  0.7534883720930232  F_score:  0.5031055900621118 0.5031055900621118\n",
      "TP:  323 / 430 all  857  accuracy:  0.7091651542649727  precision:  0.37689614935822635  recall:  0.7511627906976744  F_score:  0.5019425019425019 0.5019425019425019\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  334 / 430 all  932  accuracy:  0.6851179673321234  precision:  0.3583690987124464  recall:  0.7767441860465116  F_score:  0.49045521292217326 0.49045521292217326\n",
      "TP:  321 / 430 all  844  accuracy:  0.7132486388384754  precision:  0.3803317535545024  recall:  0.7465116279069768  F_score:  0.5039246467817897 0.5039246467817897\n",
      "TP:  316 / 430 all  833  accuracy:  0.7137023593466425  precision:  0.3793517406962785  recall:  0.7348837209302326  F_score:  0.5003958828186856 0.5003958828186856\n",
      "TP:  316 / 430 all  842  accuracy:  0.7096188747731398  precision:  0.3752969121140142  recall:  0.7348837209302326  F_score:  0.4968553459119497 0.4968553459119497\n",
      "TP:  316 / 430 all  840  accuracy:  0.7105263157894737  precision:  0.3761904761904762  recall:  0.7348837209302326  F_score:  0.49763779527559054 0.49763779527559054\n",
      "TP:  319 / 430 all  851  accuracy:  0.7082577132486388  precision:  0.37485311398354876  recall:  0.7418604651162791  F_score:  0.498048399687744 0.498048399687744\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  319 / 430 all  848  accuracy:  0.7096188747731398  precision:  0.3761792452830189  recall:  0.7418604651162791  F_score:  0.49921752738654146 0.49921752738654146\n",
      "TP:  319 / 430 all  846  accuracy:  0.7105263157894737  precision:  0.37706855791962174  recall:  0.7418604651162791  F_score:  0.5 0.5\n",
      "final and  original \n",
      "TP:  318 / 430 all  845  accuracy:  0.7100725952813067  precision:  0.3763313609467456  recall:  0.7395348837209302  F_score:  0.4988235294117647 0.4988235294117647\n",
      "TP:  318 / 430 all  845  accuracy:  0.7100725952813067  precision:  0.3763313609467456  recall:  0.7395348837209302  F_score:  0.4988235294117647 0.4988235294117647\n",
      "final and  middle and original \n",
      "TP:  320 / 430 all  846  accuracy:  0.7114337568058077  precision:  0.37825059101654845  recall:  0.7441860465116279  F_score:  0.5015673981191222 0.5015673981191222\n",
      "TP:  320 / 430 all  847  accuracy:  0.7109800362976406  precision:  0.3778040141676505  recall:  0.7441860465116279  F_score:  0.5011746280344558 0.5011746280344558\n",
      "TP:  323 / 430 all  857  accuracy:  0.7091651542649727  precision:  0.37689614935822635  recall:  0.7511627906976744  F_score:  0.5019425019425019 0.5019425019425019\n",
      "TP:  324 / 430 all  862  accuracy:  0.7078039927404719  precision:  0.37587006960556846  recall:  0.7534883720930232  F_score:  0.501547987616099 0.501547987616099\n",
      "Random Seed is:  1000\n",
      "5 fold no feature engineering\n",
      "TP:  297 / 404 all  871  accuracy:  0.691016333938294  precision:  0.3409873708381171  recall:  0.7351485148514851  F_score:  0.46588235294117647 0.46588235294117647\n",
      "TP:  285 / 404 all  765  accuracy:  0.7282214156079855  precision:  0.37254901960784315  recall:  0.7054455445544554  F_score:  0.4875962360992301 0.4875962360992301\n",
      "TP:  287 / 404 all  766  accuracy:  0.7295825771324864  precision:  0.37467362924281983  recall:  0.7103960396039604  F_score:  0.4905982905982905 0.4905982905982905\n",
      "TP:  293 / 404 all  773  accuracy:  0.7318511796733213  precision:  0.37904269081500647  recall:  0.7252475247524752  F_score:  0.49787595581988103 0.49787595581988103\n",
      "TP:  293 / 404 all  774  accuracy:  0.7313974591651543  precision:  0.3785529715762274  recall:  0.7252475247524752  F_score:  0.49745331069609505 0.49745331069609505\n",
      "TP:  294 / 404 all  780  accuracy:  0.7295825771324864  precision:  0.3769230769230769  recall:  0.7277227722772277  F_score:  0.49662162162162166 0.49662162162162166\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  317 / 404 all  927  accuracy:  0.6837568058076225  precision:  0.3419633225458468  recall:  0.7846534653465347  F_score:  0.47633358377160034 0.47633358377160034\n",
      "TP:  300 / 404 all  824  accuracy:  0.7150635208711433  precision:  0.3640776699029126  recall:  0.7425742574257426  F_score:  0.48859934853420195 0.48859934853420195\n",
      "TP:  298 / 404 all  824  accuracy:  0.7132486388384754  precision:  0.3616504854368932  recall:  0.7376237623762376  F_score:  0.485342019543974 0.485342019543974\n",
      "TP:  304 / 404 all  839  accuracy:  0.7118874773139746  precision:  0.36233611442193087  recall:  0.7524752475247525  F_score:  0.4891391794046661 0.4891391794046661\n",
      "TP:  304 / 404 all  842  accuracy:  0.7105263157894737  precision:  0.36104513064133015  recall:  0.7524752475247525  F_score:  0.48796147672552165 0.48796147672552165\n",
      "TP:  306 / 404 all  842  accuracy:  0.7123411978221416  precision:  0.36342042755344417  recall:  0.7574257425742574  F_score:  0.4911717495987159 0.4911717495987159\n",
      "TP:  305 / 404 all  841  accuracy:  0.7118874773139746  precision:  0.3626634958382878  recall:  0.754950495049505  F_score:  0.48995983935742976 0.48995983935742976\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  316 / 404 all  913  accuracy:  0.6892014519056261  precision:  0.34611171960569553  recall:  0.7821782178217822  F_score:  0.4798785117691724 0.4798785117691724\n",
      "TP:  299 / 404 all  830  accuracy:  0.7114337568058077  precision:  0.3602409638554217  recall:  0.7400990099009901  F_score:  0.48460291734197725 0.48460291734197725\n",
      "TP:  291 / 404 all  795  accuracy:  0.72005444646098  precision:  0.3660377358490566  recall:  0.7202970297029703  F_score:  0.48540450375312766 0.48540450375312766\n",
      "TP:  294 / 404 all  808  accuracy:  0.7168784029038112  precision:  0.36386138613861385  recall:  0.7277227722772277  F_score:  0.4851485148514852 0.4851485148514852\n",
      "TP:  296 / 404 all  809  accuracy:  0.7182395644283122  precision:  0.3658838071693449  recall:  0.7326732673267327  F_score:  0.48804616652926625 0.48804616652926625\n",
      "TP:  300 / 404 all  820  accuracy:  0.7168784029038112  precision:  0.36585365853658536  recall:  0.7425742574257426  F_score:  0.49019607843137253 0.49019607843137253\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  299 / 404 all  806  accuracy:  0.7223230490018149  precision:  0.3709677419354839  recall:  0.7400990099009901  F_score:  0.4942148760330578 0.4942148760330578\n",
      "TP:  299 / 404 all  809  accuracy:  0.7209618874773139  precision:  0.3695920889987639  recall:  0.7400990099009901  F_score:  0.49299258037922505 0.49299258037922505\n",
      "final and  original \n",
      "TP:  298 / 404 all  810  accuracy:  0.7196007259528131  precision:  0.36790123456790125  recall:  0.7376237623762376  F_score:  0.49093904448105447 0.49093904448105447\n",
      "TP:  298 / 404 all  810  accuracy:  0.7196007259528131  precision:  0.36790123456790125  recall:  0.7376237623762376  F_score:  0.49093904448105447 0.49093904448105447\n",
      "final and  middle and original \n",
      "TP:  301 / 404 all  819  accuracy:  0.7182395644283122  precision:  0.36752136752136755  recall:  0.745049504950495  F_score:  0.4922322158626328 0.4922322158626328\n",
      "TP:  301 / 404 all  822  accuracy:  0.7168784029038112  precision:  0.3661800486618005  recall:  0.745049504950495  F_score:  0.49102773246329523 0.49102773246329523\n",
      "TP:  302 / 404 all  828  accuracy:  0.7150635208711433  precision:  0.3647342995169082  recall:  0.7475247524752475  F_score:  0.4902597402597403 0.4902597402597403\n",
      "TP:  303 / 404 all  838  accuracy:  0.7114337568058077  precision:  0.3615751789976134  recall:  0.75  F_score:  0.48792270531400966 0.48792270531400966\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for rnd in [1,10,100,1000]:\n",
    "    print('Random Seed is: ',rnd)  \n",
    "    train_X,test_X, train_y, test_y = train_test_split(model_sample_strong_feature_final,label,test_size=0.2,random_state=rnd) \n",
    "    \n",
    "    train_X_orig = model_sample_.loc[train_X.index]\n",
    "    test_X_orig = model_sample_.loc[test_X.index]\n",
    "    train_X_middle = model_sample_strong_feature_middle.loc[train_X.index]\n",
    "    test_X_middle = model_sample_strong_feature_middle.loc[test_X.index]\n",
    "    \n",
    "    \n",
    "    print('5 fold no feature engineering')\n",
    "    pred_out_lgb, pred_out_gbdt, pred_out_rf = N_Fold_Predict(train_X_orig, train_y['y'].values, test_X_orig, cv_ = 3)\n",
    "    pred =pred_out_rf >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_gbdt >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb * 0.55 + 0.45 * pred_out_gbdt>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb * 0.5 + 0.5 * pred_out_gbdt>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =(pred_out_lgb * 0.5 + 0.5 * pred_out_gbdt) * 0.9 + 0.1 * pred_out_rf>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    print('*' * 50)\n",
    "    print('5 fold feature engineering middle')\n",
    "    pred_out_lgb_middle, pred_out_gbdt_middle, pred_out_rf_middle = N_Fold_Predict(train_X_middle,train_y['y'].values, test_X_middle, cv_ = 3)\n",
    "    pred = pred_out_rf_middle >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_gbdt_middle >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_middle >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    pred =pred_out_lgb_middle * 0.55 + 0.45 * pred_out_gbdt_middle>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_middle * 0.5 + 0.5 * pred_out_gbdt_middle>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =(pred_out_lgb_middle * 0.5 + 0.5 * pred_out_gbdt_middle) * 0.9 + 0.1 * pred_out_rf_middle >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "     \n",
    "#     pred =(pred_out_lgb_middle * 0.5 + 0.5 * pred_out_gbdt_middle) * 0.9 + 0.05 * (pred_out_rf_middle + pred_out_rf)>= 0.215\n",
    "#     get_score(pred, test_y['y'].values)\n",
    "     \n",
    "    \n",
    "    print('*' * 50)\n",
    "    print('5 fold feature engineering final')\n",
    "    pred_out_lgb_final, pred_out_gbdt_final, pred_out_rf_final = N_Fold_Predict(train_X,train_y['y'].values, test_X, cv_ = 3)\n",
    "    pred =pred_out_rf_final >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_gbdt_final >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_final >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    pred =pred_out_lgb_final * 0.55 + 0.45 * pred_out_gbdt_final>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_final * 0.5 + 0.5 * pred_out_gbdt_final>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =(pred_out_lgb_final * 0.5 + 0.5 * pred_out_gbdt_final) * 0.9 + 0.1 * pred_out_rf_final>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    print('*' * 50)\n",
    "     \n",
    "    print('Fire!')\n",
    "    print('middle  and  original ')\n",
    "    \n",
    "    pred =((pred_out_lgb_middle * 0.5 + pred_out_lgb * 0.5 )*0.55  + 0.45 * (pred_out_gbdt_middle * 0.5 + 0.5 * pred_out_gbdt))>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =((pred_out_lgb_middle * 0.5 + pred_out_lgb* 0.5 )*0.5  + 0.5 * (pred_out_gbdt_middle *0.5 + 0.5 * pred_out_gbdt ))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "     \n",
    "    print('final and  original ')\n",
    "    pred =((pred_out_lgb_final * 0.5 + pred_out_lgb * 0.5 )*0.55  + 0.45 * (pred_out_gbdt_final * 0.5 + 0.5 * pred_out_gbdt))>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =((pred_out_lgb_final * 0.5 + pred_out_lgb* 0.5 )*0.5  + 0.5 * (pred_out_gbdt_final *0.5 + 0.5 * pred_out_gbdt ))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "     \n",
    "    \n",
    "    print('final and  middle and original ')\n",
    "    pred =((pred_out_lgb_final * 0.3 + pred_out_lgb * 0.3 + 0.4 * pred_out_lgb_middle)*0.55  + 0.45 * (pred_out_gbdt_final * 0.3 + 0.3 * pred_out_gbdt + 0.4 * pred_out_gbdt_middle))>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    \n",
    "    pred = 0.1 * (pred_out_rf_middle + pred_out_rf + pred_out_rf_final ) /3.0 + 0.9 * ((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    pred = 0.15 * (pred_out_rf_middle + pred_out_rf + pred_out_rf_final ) /3.0 + 0.85 * ((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4CDF9CAE73D64F49A81CFACAB7CFB643",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed is:  1\n",
      "5 fold no feature engineering\n",
      "TP:  315 / 442 all  833  accuracy:  0.707350272232305  precision:  0.37815126050420167  recall:  0.7126696832579186  F_score:  0.4941176470588235 0.4941176470588235\n",
      "TP:  286 / 442 all  721  accuracy:  0.7318511796733213  precision:  0.39667128987517336  recall:  0.6470588235294118  F_score:  0.4918314703353397 0.4918314703353397\n",
      "TP:  290 / 442 all  728  accuracy:  0.7323049001814882  precision:  0.3983516483516483  recall:  0.6561085972850679  F_score:  0.4957264957264957 0.4957264957264957\n",
      "TP:  290 / 442 all  722  accuracy:  0.73502722323049  precision:  0.40166204986149584  recall:  0.6561085972850679  F_score:  0.49828178694158065 0.49828178694158065\n",
      "TP:  291 / 442 all  725  accuracy:  0.734573502722323  precision:  0.4013793103448276  recall:  0.6583710407239819  F_score:  0.4987146529562982 0.4987146529562982\n",
      "TP:  292 / 442 all  731  accuracy:  0.7327586206896551  precision:  0.399452804377565  recall:  0.6606334841628959  F_score:  0.4978687127024723 0.4978687127024723\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  316 / 442 all  811  accuracy:  0.7182395644283122  precision:  0.38964241676942046  recall:  0.7149321266968326  F_score:  0.5043894652833201 0.5043894652833201\n",
      "TP:  282 / 442 all  708  accuracy:  0.7341197822141561  precision:  0.3983050847457627  recall:  0.6380090497737556  F_score:  0.4904347826086956 0.4904347826086956\n",
      "TP:  279 / 442 all  688  accuracy:  0.7404718693284936  precision:  0.4055232558139535  recall:  0.6312217194570136  F_score:  0.49380530973451336 0.49380530973451336\n",
      "TP:  286 / 442 all  703  accuracy:  0.7400181488203267  precision:  0.406827880512091  recall:  0.6470588235294118  F_score:  0.4995633187772926 0.4995633187772926\n",
      "TP:  281 / 442 all  701  accuracy:  0.7363883847549909  precision:  0.4008559201141227  recall:  0.6357466063348416  F_score:  0.4916885389326334 0.4916885389326334\n",
      "TP:  285 / 442 all  710  accuracy:  0.735934664246824  precision:  0.4014084507042254  recall:  0.6447963800904978  F_score:  0.4947916666666667 0.4947916666666667\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  322 / 442 all  846  accuracy:  0.7078039927404719  precision:  0.3806146572104019  recall:  0.7285067873303167  F_score:  0.5 0.5\n",
      "TP:  287 / 442 all  726  accuracy:  0.7304900181488203  precision:  0.3953168044077135  recall:  0.6493212669683258  F_score:  0.4914383561643836 0.4914383561643836\n",
      "TP:  287 / 442 all  711  accuracy:  0.7372958257713249  precision:  0.40365682137834036  recall:  0.6493212669683258  F_score:  0.4978317432784042 0.4978317432784042\n",
      "TP:  290 / 442 all  725  accuracy:  0.7336660617059891  precision:  0.4  recall:  0.6561085972850679  F_score:  0.49700085689802914 0.49700085689802914\n",
      "TP:  291 / 442 all  726  accuracy:  0.7341197822141561  precision:  0.40082644628099173  recall:  0.6583710407239819  F_score:  0.49828767123287665 0.49828767123287665\n",
      "TP:  294 / 442 all  738  accuracy:  0.7313974591651543  precision:  0.3983739837398374  recall:  0.665158371040724  F_score:  0.49830508474576274 0.49830508474576274\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  288 / 442 all  711  accuracy:  0.7382032667876588  precision:  0.4050632911392405  recall:  0.6515837104072398  F_score:  0.4995663486556808 0.4995663486556808\n",
      "TP:  290 / 442 all  713  accuracy:  0.7391107078039928  precision:  0.4067321178120617  recall:  0.6561085972850679  F_score:  0.5021645021645021 0.5021645021645021\n",
      "final and  original \n",
      "TP:  291 / 442 all  723  accuracy:  0.735480943738657  precision:  0.4024896265560166  recall:  0.6583710407239819  F_score:  0.4995708154506438 0.4995708154506438\n",
      "TP:  291 / 442 all  723  accuracy:  0.735480943738657  precision:  0.4024896265560166  recall:  0.6583710407239819  F_score:  0.4995708154506438 0.4995708154506438\n",
      "final and  middle and original \n",
      "TP:  290 / 442 all  727  accuracy:  0.7327586206896551  precision:  0.3988995873452545  recall:  0.6561085972850679  F_score:  0.49615055603079555 0.49615055603079555\n",
      "TP:  291 / 442 all  727  accuracy:  0.7336660617059891  precision:  0.40027510316368636  recall:  0.6583710407239819  F_score:  0.49786142001710865 0.49786142001710865\n",
      "TP:  293 / 442 all  734  accuracy:  0.7323049001814882  precision:  0.3991825613079019  recall:  0.6628959276018099  F_score:  0.49829931972789115 0.49829931972789115\n",
      "TP:  294 / 442 all  737  accuracy:  0.7318511796733213  precision:  0.3989145183175034  recall:  0.665158371040724  F_score:  0.49872773536895676 0.49872773536895676\n",
      "Random Seed is:  10\n",
      "5 fold no feature engineering\n",
      "TP:  323 / 438 all  859  accuracy:  0.7046279491833031  precision:  0.3760186263096624  recall:  0.7374429223744292  F_score:  0.49807247494217427 0.49807247494217427\n",
      "TP:  296 / 438 all  735  accuracy:  0.7363883847549909  precision:  0.40272108843537413  recall:  0.6757990867579908  F_score:  0.5046888320545609 0.5046888320545609\n",
      "TP:  308 / 438 all  782  accuracy:  0.7259528130671506  precision:  0.3938618925831202  recall:  0.7031963470319634  F_score:  0.5049180327868853 0.5049180327868853\n",
      "TP:  304 / 438 all  763  accuracy:  0.7309437386569873  precision:  0.3984272608125819  recall:  0.6940639269406392  F_score:  0.5062447960033305 0.5062447960033305\n",
      "TP:  303 / 438 all  759  accuracy:  0.7318511796733213  precision:  0.39920948616600793  recall:  0.6917808219178082  F_score:  0.506265664160401 0.506265664160401\n",
      "TP:  305 / 438 all  769  accuracy:  0.7291288566243194  precision:  0.3966189856957087  recall:  0.6963470319634704  F_score:  0.5053852526926264 0.5053852526926264\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  344 / 438 all  916  accuracy:  0.6978221415607986  precision:  0.37554585152838427  recall:  0.7853881278538812  F_score:  0.5081240768094535 0.5081240768094535\n",
      "TP:  310 / 438 all  783  accuracy:  0.7273139745916516  precision:  0.3959131545338442  recall:  0.7077625570776256  F_score:  0.5077805077805078 0.5077805077805078\n",
      "TP:  325 / 438 all  837  accuracy:  0.7164246823956443  precision:  0.38829151732377537  recall:  0.7420091324200914  F_score:  0.5098039215686274 0.5098039215686274\n",
      "TP:  320 / 438 all  815  accuracy:  0.7218693284936479  precision:  0.39263803680981596  recall:  0.730593607305936  F_score:  0.5107741420590584 0.5107741420590584\n",
      "TP:  321 / 438 all  814  accuracy:  0.7232304900181489  precision:  0.39434889434889436  recall:  0.7328767123287672  F_score:  0.512779552715655 0.512779552715655\n",
      "TP:  321 / 438 all  819  accuracy:  0.7209618874773139  precision:  0.39194139194139194  recall:  0.7328767123287672  F_score:  0.5107398568019094 0.5107398568019094\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  337 / 438 all  880  accuracy:  0.7078039927404719  precision:  0.38295454545454544  recall:  0.769406392694064  F_score:  0.511380880121396 0.511380880121396\n",
      "TP:  309 / 438 all  772  accuracy:  0.7313974591651543  precision:  0.40025906735751293  recall:  0.7054794520547946  F_score:  0.5107438016528926 0.5107438016528926\n",
      "TP:  328 / 438 all  811  accuracy:  0.7309437386569873  precision:  0.40443896424167697  recall:  0.7488584474885844  F_score:  0.5252201761409128 0.5252201761409128\n",
      "TP:  326 / 438 all  801  accuracy:  0.7336660617059891  precision:  0.4069912609238452  recall:  0.7442922374429224  F_score:  0.526230831315577 0.526230831315577\n",
      "TP:  324 / 438 all  797  accuracy:  0.7336660617059891  precision:  0.4065244667503137  recall:  0.7397260273972602  F_score:  0.5246963562753036 0.5246963562753036\n",
      "TP:  327 / 438 all  806  accuracy:  0.7323049001814882  precision:  0.4057071960297767  recall:  0.7465753424657534  F_score:  0.5257234726688104 0.5257234726688104\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  313 / 438 all  778  accuracy:  0.7323049001814882  precision:  0.4023136246786632  recall:  0.7146118721461188  F_score:  0.5148026315789473 0.5148026315789473\n",
      "TP:  311 / 438 all  775  accuracy:  0.7318511796733213  precision:  0.4012903225806452  recall:  0.7100456621004566  F_score:  0.5127782357790601 0.5127782357790601\n",
      "final and  original \n",
      "TP:  317 / 438 all  780  accuracy:  0.73502722323049  precision:  0.4064102564102564  recall:  0.723744292237443  F_score:  0.5205254515599343 0.5205254515599343\n",
      "TP:  315 / 438 all  778  accuracy:  0.7341197822141561  precision:  0.40488431876606684  recall:  0.7191780821917808  F_score:  0.5180921052631579 0.5180921052631579\n",
      "final and  middle and original \n",
      "TP:  320 / 438 all  791  accuracy:  0.7327586206896551  precision:  0.404551201011378  recall:  0.730593607305936  F_score:  0.5207485760781123 0.5207485760781123\n",
      "TP:  319 / 438 all  789  accuracy:  0.7327586206896551  precision:  0.40430925221799746  recall:  0.728310502283105  F_score:  0.5199674001629991 0.5199674001629991\n",
      "TP:  326 / 438 all  799  accuracy:  0.734573502722323  precision:  0.40801001251564456  recall:  0.7442922374429224  F_score:  0.5270816491511722 0.5270816491511722\n",
      "TP:  326 / 438 all  807  accuracy:  0.7309437386569873  precision:  0.40396530359355637  recall:  0.7442922374429224  F_score:  0.5236947791164658 0.5236947791164658\n",
      "Random Seed is:  100\n",
      "5 fold no feature engineering\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  312 / 430 all  851  accuracy:  0.7019056261343013  precision:  0.36662749706227965  recall:  0.7255813953488373  F_score:  0.48711943793911006 0.48711943793911006\n",
      "TP:  292 / 430 all  742  accuracy:  0.7332123411978222  precision:  0.3935309973045822  recall:  0.6790697674418604  F_score:  0.4982935153583618 0.4982935153583618\n",
      "TP:  300 / 430 all  774  accuracy:  0.7259528130671506  precision:  0.3875968992248062  recall:  0.6976744186046512  F_score:  0.4983388704318937 0.4983388704318937\n",
      "TP:  297 / 430 all  755  accuracy:  0.7318511796733213  precision:  0.3933774834437086  recall:  0.6906976744186046  F_score:  0.5012658227848101 0.5012658227848101\n",
      "TP:  298 / 430 all  755  accuracy:  0.7327586206896551  precision:  0.39470198675496687  recall:  0.6930232558139535  F_score:  0.5029535864978903 0.5029535864978903\n",
      "TP:  301 / 430 all  766  accuracy:  0.7304900181488203  precision:  0.39295039164490864  recall:  0.7  F_score:  0.5033444816053512 0.5033444816053512\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  331 / 430 all  893  accuracy:  0.7000907441016334  precision:  0.3706606942889138  recall:  0.7697674418604651  F_score:  0.5003779289493575 0.5003779289493575\n",
      "TP:  306 / 430 all  786  accuracy:  0.7259528130671506  precision:  0.3893129770992366  recall:  0.7116279069767442  F_score:  0.5032894736842105 0.5032894736842105\n",
      "TP:  302 / 430 all  777  accuracy:  0.7264065335753176  precision:  0.3886743886743887  recall:  0.7023255813953488  F_score:  0.5004142502071252 0.5004142502071252\n",
      "TP:  305 / 430 all  781  accuracy:  0.7273139745916516  precision:  0.3905249679897567  recall:  0.7093023255813954  F_score:  0.5037159372419487 0.5037159372419487\n",
      "TP:  304 / 430 all  781  accuracy:  0.7264065335753176  precision:  0.3892445582586428  recall:  0.7069767441860465  F_score:  0.5020644095788604 0.5020644095788604\n",
      "TP:  304 / 430 all  785  accuracy:  0.7245916515426497  precision:  0.3872611464968153  recall:  0.7069767441860465  F_score:  0.5004115226337449 0.5004115226337449\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  326 / 430 all  876  accuracy:  0.7032667876588021  precision:  0.3721461187214612  recall:  0.7581395348837209  F_score:  0.49923430321592643 0.49923430321592643\n",
      "TP:  296 / 430 all  758  accuracy:  0.7295825771324864  precision:  0.39050131926121373  recall:  0.6883720930232559  F_score:  0.4983164983164983 0.4983164983164983\n",
      "TP:  295 / 430 all  754  accuracy:  0.7304900181488203  precision:  0.3912466843501326  recall:  0.686046511627907  F_score:  0.4983108108108108 0.4983108108108108\n",
      "TP:  298 / 430 all  764  accuracy:  0.7286751361161524  precision:  0.3900523560209424  recall:  0.6930232558139535  F_score:  0.4991624790619766 0.4991624790619766\n",
      "TP:  299 / 430 all  766  accuracy:  0.7286751361161524  precision:  0.39033942558746737  recall:  0.6953488372093023  F_score:  0.5 0.5\n",
      "TP:  304 / 430 all  782  accuracy:  0.7259528130671506  precision:  0.3887468030690537  recall:  0.7069767441860465  F_score:  0.5016501650165016 0.5016501650165016\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  298 / 430 all  772  accuracy:  0.7250453720508166  precision:  0.3860103626943005  recall:  0.6930232558139535  F_score:  0.49584026622296173 0.49584026622296173\n",
      "TP:  297 / 430 all  768  accuracy:  0.7259528130671506  precision:  0.38671875  recall:  0.6906976744186046  F_score:  0.4958263772954925 0.4958263772954925\n",
      "final and  original \n",
      "TP:  301 / 430 all  768  accuracy:  0.7295825771324864  precision:  0.3919270833333333  recall:  0.7  F_score:  0.5025041736227045 0.5025041736227045\n",
      "TP:  302 / 430 all  766  accuracy:  0.7313974591651543  precision:  0.39425587467362927  recall:  0.7023255813953488  F_score:  0.5050167224080268 0.5050167224080268\n",
      "final and  middle and original \n",
      "TP:  299 / 430 all  771  accuracy:  0.7264065335753176  precision:  0.38780804150453957  recall:  0.6953488372093023  F_score:  0.49791840133222315 0.49791840133222315\n",
      "TP:  299 / 430 all  769  accuracy:  0.7273139745916516  precision:  0.38881664499349805  recall:  0.6953488372093023  F_score:  0.4987489574645537 0.4987489574645537\n",
      "TP:  302 / 430 all  778  accuracy:  0.7259528130671506  precision:  0.38817480719794345  recall:  0.7023255813953488  F_score:  0.5 0.5\n",
      "TP:  304 / 430 all  787  accuracy:  0.7236842105263158  precision:  0.386277001270648  recall:  0.7069767441860465  F_score:  0.49958915365653245 0.49958915365653245\n",
      "Random Seed is:  1000\n",
      "5 fold no feature engineering\n",
      "TP:  292 / 404 all  833  accuracy:  0.7037205081669692  precision:  0.3505402160864346  recall:  0.7227722772277227  F_score:  0.4721099434114794 0.4721099434114794\n",
      "TP:  281 / 404 all  745  accuracy:  0.7336660617059891  precision:  0.37718120805369126  recall:  0.6955445544554455  F_score:  0.4891209747606614 0.4891209747606614\n",
      "TP:  278 / 404 all  749  accuracy:  0.7291288566243194  precision:  0.3711615487316422  recall:  0.6881188118811881  F_score:  0.4822202948829142 0.4822202948829142\n",
      "TP:  280 / 404 all  749  accuracy:  0.7309437386569873  precision:  0.37383177570093457  recall:  0.693069306930693  F_score:  0.48568950563746743 0.48568950563746743\n",
      "TP:  282 / 404 all  753  accuracy:  0.7309437386569873  precision:  0.3745019920318725  recall:  0.698019801980198  F_score:  0.48746758859118416 0.48746758859118416\n",
      "TP:  283 / 404 all  755  accuracy:  0.7309437386569873  precision:  0.3748344370860927  recall:  0.7004950495049505  F_score:  0.4883520276100086 0.4883520276100086\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  306 / 404 all  870  accuracy:  0.6996370235934665  precision:  0.35172413793103446  recall:  0.7574257425742574  F_score:  0.4803767660910518 0.4803767660910518\n",
      "TP:  275 / 404 all  770  accuracy:  0.7168784029038112  precision:  0.35714285714285715  recall:  0.6806930693069307  F_score:  0.46848381601362865 0.46848381601362865\n",
      "TP:  276 / 404 all  759  accuracy:  0.7227767695099818  precision:  0.36363636363636365  recall:  0.6831683168316832  F_score:  0.47463456577815993 0.47463456577815993\n",
      "TP:  287 / 404 all  785  accuracy:  0.7209618874773139  precision:  0.36560509554140125  recall:  0.7103960396039604  F_score:  0.48275862068965514 0.48275862068965514\n",
      "TP:  285 / 404 all  782  accuracy:  0.720508166969147  precision:  0.36445012787723785  recall:  0.7054455445544554  F_score:  0.4806070826306914 0.4806070826306914\n",
      "TP:  289 / 404 all  790  accuracy:  0.720508166969147  precision:  0.3658227848101266  recall:  0.7153465346534653  F_score:  0.4840871021775544 0.4840871021775544\n",
      "**************************************************\n",
      "5 fold feature engineering final\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-5d39d8026266>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'5 fold feature engineering final'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[0mpred_out_lgb_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_out_gbdt_final\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_out_rf_final\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN_Fold_Predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mpred_out_rf_final\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.23\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[0mget_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-180237355f8d>\u001b[0m in \u001b[0;36mN_Fold_Predict\u001b[1;34m(train_fea, train_y, test_fea, cv_)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mgbdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mgbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mgbdt_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgbdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1032\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state,\n\u001b[1;32m-> 1034\u001b[1;33m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1035\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_stages\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1087\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[0;32m   1088\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1089\u001b[1;33m                                      X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    786\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m                 tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m--> 788\u001b[1;33m                          check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1125\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_sample_strong_feature_middle = model_sample_strong_feature_middle.fillna(-999)\n",
    "model_sample_strong_feature_final = model_sample_strong_feature_final.fillna(-999)\n",
    "model_sample_ = model_sample.fillna(-999)\n",
    "for rnd in [1,10,100,1000]:\n",
    "    print('Random Seed is: ',rnd)  \n",
    "    train_X,test_X, train_y, test_y = train_test_split(model_sample_strong_feature_final,label,test_size=0.2,random_state=rnd) \n",
    "    \n",
    "    train_X_orig = model_sample_.loc[train_X.index]\n",
    "    test_X_orig = model_sample_.loc[test_X.index]\n",
    "    train_X_middle = model_sample_strong_feature_middle.loc[train_X.index]\n",
    "    test_X_middle = model_sample_strong_feature_middle.loc[test_X.index]\n",
    "    \n",
    "    \n",
    "    print('5 fold no feature engineering')\n",
    "    pred_out_lgb, pred_out_gbdt, pred_out_rf = N_Fold_Predict(train_X_orig, train_y['y'].values, test_X_orig, cv_ = 3)\n",
    "    pred =pred_out_rf >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_gbdt >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb * 0.55 + 0.45 * pred_out_gbdt>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb * 0.5 + 0.5 * pred_out_gbdt>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =(pred_out_lgb * 0.5 + 0.5 * pred_out_gbdt) * 0.9 + 0.1 * pred_out_rf>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    print('*' * 50)\n",
    "    print('5 fold feature engineering middle')\n",
    "    pred_out_lgb_middle, pred_out_gbdt_middle, pred_out_rf_middle = N_Fold_Predict(train_X_middle,train_y['y'].values, test_X_middle, cv_ = 3)\n",
    "    pred = pred_out_rf_middle >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_gbdt_middle >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_middle >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    pred =pred_out_lgb_middle * 0.55 + 0.45 * pred_out_gbdt_middle>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_middle * 0.5 + 0.5 * pred_out_gbdt_middle>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =(pred_out_lgb_middle * 0.5 + 0.5 * pred_out_gbdt_middle) * 0.9 + 0.1 * pred_out_rf_middle >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "     \n",
    "#     pred =(pred_out_lgb_middle * 0.5 + 0.5 * pred_out_gbdt_middle) * 0.9 + 0.05 * (pred_out_rf_middle + pred_out_rf)>= 0.23\n",
    "#     get_score(pred, test_y['y'].values)\n",
    "     \n",
    "    \n",
    "    print('*' * 50)\n",
    "    print('5 fold feature engineering final')\n",
    "    pred_out_lgb_final, pred_out_gbdt_final, pred_out_rf_final = N_Fold_Predict(train_X,train_y['y'].values, test_X, cv_ = 3)\n",
    "    pred =pred_out_rf_final >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_gbdt_final >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_final >= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    pred =pred_out_lgb_final * 0.55 + 0.45 * pred_out_gbdt_final>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_final * 0.5 + 0.5 * pred_out_gbdt_final>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =(pred_out_lgb_final * 0.5 + 0.5 * pred_out_gbdt_final) * 0.9 + 0.1 * pred_out_rf_final>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    print('*' * 50)\n",
    "     \n",
    "    print('Fire!')\n",
    "    print('middle  and  original ')\n",
    "    \n",
    "    pred =((pred_out_lgb_middle * 0.5 + pred_out_lgb * 0.5 )*0.55  + 0.45 * (pred_out_gbdt_middle * 0.5 + 0.5 * pred_out_gbdt))>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =((pred_out_lgb_middle * 0.5 + pred_out_lgb* 0.5 )*0.5  + 0.5 * (pred_out_gbdt_middle *0.5 + 0.5 * pred_out_gbdt ))>= 0.23\n",
    "    get_score(pred, test_y['y'].values) \n",
    "     \n",
    "    print('final and  original ')\n",
    "    pred =((pred_out_lgb_final * 0.5 + pred_out_lgb * 0.5 )*0.55  + 0.45 * (pred_out_gbdt_final * 0.5 + 0.5 * pred_out_gbdt))>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =((pred_out_lgb_final * 0.5 + pred_out_lgb* 0.5 )*0.5  + 0.5 * (pred_out_gbdt_final *0.5 + 0.5 * pred_out_gbdt ))>= 0.23\n",
    "    get_score(pred, test_y['y'].values) \n",
    "     \n",
    "    \n",
    "    print('final and  middle and original ')\n",
    "    pred =((pred_out_lgb_final * 0.3 + pred_out_lgb * 0.3 + 0.4 * pred_out_lgb_middle)*0.55  + 0.45 * (pred_out_gbdt_final * 0.3 + 0.3 * pred_out_gbdt + 0.4 * pred_out_gbdt_middle))>= 0.23\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.23\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    \n",
    "    pred = 0.1 * (pred_out_rf_middle + pred_out_rf + pred_out_rf_final ) /3.0 + 0.9 * ((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.23\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    pred = 0.15 * (pred_out_rf_middle + pred_out_rf + pred_out_rf_final ) /3.0 + 0.85 * ((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.23\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "716707A6345841C48FB3069CB27D8DE9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed is:  1\n",
      "5 fold no feature engineering\n",
      "TP:  340 / 442 all  923  accuracy:  0.6892014519056261  precision:  0.36836403033586135  recall:  0.7692307692307693  F_score:  0.4981684981684982 0.4981684981684982\n",
      "TP:  312 / 442 all  801  accuracy:  0.719147005444646  precision:  0.3895131086142322  recall:  0.7058823529411765  F_score:  0.50201126307321 0.50201126307321\n",
      "TP:  310 / 442 all  796  accuracy:  0.7196007259528131  precision:  0.38944723618090454  recall:  0.7013574660633484  F_score:  0.5008077544426495 0.5008077544426495\n",
      "TP:  315 / 442 all  806  accuracy:  0.7196007259528131  precision:  0.39081885856079407  recall:  0.7126696832579186  F_score:  0.5048076923076924 0.5048076923076924\n",
      "TP:  314 / 442 all  804  accuracy:  0.7196007259528131  precision:  0.39054726368159204  recall:  0.7104072398190046  F_score:  0.5040128410914928 0.5040128410914928\n",
      "TP:  314 / 442 all  811  accuracy:  0.7164246823956443  precision:  0.3871763255240444  recall:  0.7104072398190046  F_score:  0.5011971268954509 0.5011971268954509\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  343 / 442 all  914  accuracy:  0.6960072595281307  precision:  0.37527352297593  recall:  0.7760180995475113  F_score:  0.5058997050147492 0.5058997050147492\n",
      "TP:  318 / 442 all  820  accuracy:  0.7159709618874773  precision:  0.3878048780487805  recall:  0.7194570135746606  F_score:  0.5039619651347068 0.5039619651347068\n",
      "TP:  307 / 442 all  779  accuracy:  0.7245916515426497  precision:  0.3940949935815148  recall:  0.6945701357466063  F_score:  0.5028665028665029 0.5028665028665029\n",
      "TP:  311 / 442 all  800  accuracy:  0.7186932849364791  precision:  0.38875  recall:  0.7036199095022625  F_score:  0.500805152979066 0.500805152979066\n",
      "TP:  312 / 442 all  803  accuracy:  0.7182395644283122  precision:  0.38854296388542964  recall:  0.7058823529411765  F_score:  0.5012048192771086 0.5012048192771086\n",
      "TP:  316 / 442 all  814  accuracy:  0.7168784029038112  precision:  0.3882063882063882  recall:  0.7149321266968326  F_score:  0.5031847133757962 0.5031847133757962\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  341 / 442 all  905  accuracy:  0.6982758620689655  precision:  0.37679558011049724  recall:  0.7714932126696833  F_score:  0.5063103192279139 0.5063103192279139\n",
      "TP:  308 / 442 all  779  accuracy:  0.7254990925589837  precision:  0.39537869062901154  recall:  0.6968325791855203  F_score:  0.5045045045045045 0.5045045045045045\n",
      "TP:  310 / 442 all  789  accuracy:  0.7227767695099818  precision:  0.3929024081115336  recall:  0.7013574660633484  F_score:  0.503655564581641 0.503655564581641\n",
      "TP:  311 / 442 all  791  accuracy:  0.7227767695099818  precision:  0.393173198482933  recall:  0.7036199095022625  F_score:  0.5044606650446067 0.5044606650446067\n",
      "TP:  311 / 442 all  791  accuracy:  0.7227767695099818  precision:  0.393173198482933  recall:  0.7036199095022625  F_score:  0.5044606650446067 0.5044606650446067\n",
      "TP:  310 / 442 all  794  accuracy:  0.720508166969147  precision:  0.3904282115869018  recall:  0.7013574660633484  F_score:  0.5016181229773463 0.5016181229773463\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  315 / 442 all  800  accuracy:  0.7223230490018149  precision:  0.39375  recall:  0.7126696832579186  F_score:  0.5072463768115942 0.5072463768115942\n",
      "TP:  314 / 442 all  800  accuracy:  0.721415607985481  precision:  0.3925  recall:  0.7104072398190046  F_score:  0.5056360708534622 0.5056360708534622\n",
      "final and  original \n",
      "TP:  313 / 442 all  797  accuracy:  0.7218693284936479  precision:  0.39272271016311167  recall:  0.7081447963800905  F_score:  0.5052461662631155 0.5052461662631155\n",
      "TP:  312 / 442 all  797  accuracy:  0.7209618874773139  precision:  0.39146800501882056  recall:  0.7058823529411765  F_score:  0.5036319612590798 0.5036319612590798\n",
      "final and  middle and original \n",
      "TP:  315 / 442 all  807  accuracy:  0.719147005444646  precision:  0.3903345724907063  recall:  0.7126696832579186  F_score:  0.5044035228182546 0.5044035228182546\n",
      "TP:  314 / 442 all  804  accuracy:  0.7196007259528131  precision:  0.39054726368159204  recall:  0.7104072398190046  F_score:  0.5040128410914928 0.5040128410914928\n",
      "TP:  318 / 442 all  811  accuracy:  0.72005444646098  precision:  0.3921085080147966  recall:  0.7194570135746606  F_score:  0.5075818036711892 0.5075818036711892\n",
      "TP:  317 / 442 all  815  accuracy:  0.7173321234119783  precision:  0.3889570552147239  recall:  0.7171945701357466  F_score:  0.5043754972155926 0.5043754972155926\n",
      "Random Seed is:  10\n",
      "5 fold no feature engineering\n",
      "TP:  332 / 438 all  914  accuracy:  0.6878402903811253  precision:  0.36323851203501095  recall:  0.7579908675799086  F_score:  0.4911242603550296 0.4911242603550296\n",
      "TP:  319 / 438 all  816  accuracy:  0.720508166969147  precision:  0.3909313725490196  recall:  0.728310502283105  F_score:  0.5087719298245613 0.5087719298245613\n",
      "TP:  328 / 438 all  849  accuracy:  0.7137023593466425  precision:  0.38633686690223795  recall:  0.7488584474885844  F_score:  0.5097125097125097 0.5097125097125097\n",
      "TP:  326 / 438 all  837  accuracy:  0.7173321234119783  precision:  0.38948626045400236  recall:  0.7442922374429224  F_score:  0.5113725490196078 0.5113725490196078\n",
      "TP:  326 / 438 all  838  accuracy:  0.7168784029038112  precision:  0.38902147971360385  recall:  0.7442922374429224  F_score:  0.5109717868338558 0.5109717868338558\n",
      "TP:  326 / 438 all  846  accuracy:  0.7132486388384754  precision:  0.38534278959810875  recall:  0.7442922374429224  F_score:  0.5077881619937694 0.5077881619937694\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  347 / 438 all  931  accuracy:  0.6937386569872959  precision:  0.3727175080558539  recall:  0.7922374429223744  F_score:  0.5069393718042365 0.5069393718042365\n",
      "TP:  324 / 438 all  817  accuracy:  0.7245916515426497  precision:  0.39657282741738065  recall:  0.7397260273972602  F_score:  0.5163346613545816 0.5163346613545816\n",
      "TP:  337 / 438 all  857  accuracy:  0.7182395644283122  precision:  0.39323220536756126  recall:  0.769406392694064  F_score:  0.5204633204633206 0.5204633204633206\n",
      "TP:  331 / 438 all  841  accuracy:  0.72005444646098  precision:  0.3935790725326992  recall:  0.7557077625570776  F_score:  0.5175918686473807 0.5175918686473807\n",
      "TP:  332 / 438 all  841  accuracy:  0.7209618874773139  precision:  0.3947681331747919  recall:  0.7579908675799086  F_score:  0.5191555903049258 0.5191555903049258\n",
      "TP:  338 / 438 all  852  accuracy:  0.721415607985481  precision:  0.3967136150234742  recall:  0.771689497716895  F_score:  0.524031007751938 0.524031007751938\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  348 / 438 all  936  accuracy:  0.6923774954627949  precision:  0.3717948717948718  recall:  0.7945205479452054  F_score:  0.5065502183406113 0.5065502183406113\n",
      "TP:  331 / 438 all  818  accuracy:  0.7304900181488203  precision:  0.40464547677261614  recall:  0.7557077625570776  F_score:  0.5270700636942675 0.5270700636942675\n",
      "TP:  333 / 438 all  833  accuracy:  0.7254990925589837  precision:  0.3997599039615846  recall:  0.7602739726027398  F_score:  0.5239968528717546 0.5239968528717546\n",
      "TP:  334 / 438 all  826  accuracy:  0.7295825771324864  precision:  0.4043583535108959  recall:  0.7625570776255708  F_score:  0.5284810126582279 0.5284810126582279\n",
      "TP:  335 / 438 all  824  accuracy:  0.7313974591651543  precision:  0.4065533980582524  recall:  0.7648401826484018  F_score:  0.5309033280507132 0.5309033280507132\n",
      "TP:  337 / 438 all  838  accuracy:  0.7268602540834845  precision:  0.4021479713603819  recall:  0.769406392694064  F_score:  0.5282131661442007 0.5282131661442007\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  330 / 438 all  830  accuracy:  0.7241379310344828  precision:  0.39759036144578314  recall:  0.7534246575342466  F_score:  0.5205047318611987 0.5205047318611987\n",
      "TP:  329 / 438 all  827  accuracy:  0.7245916515426497  precision:  0.3978234582829504  recall:  0.7511415525114156  F_score:  0.5201581027667985 0.5201581027667985\n",
      "final and  original \n",
      "TP:  333 / 438 all  834  accuracy:  0.7250453720508166  precision:  0.39928057553956836  recall:  0.7602739726027398  F_score:  0.5235849056603774 0.5235849056603774\n",
      "TP:  333 / 438 all  832  accuracy:  0.7259528130671506  precision:  0.40024038461538464  recall:  0.7602739726027398  F_score:  0.5244094488188976 0.5244094488188976\n",
      "final and  middle and original \n",
      "TP:  333 / 438 all  827  accuracy:  0.7282214156079855  precision:  0.4026602176541717  recall:  0.7602739726027398  F_score:  0.5264822134387351 0.5264822134387351\n",
      "TP:  335 / 438 all  825  accuracy:  0.7309437386569873  precision:  0.40606060606060607  recall:  0.7648401826484018  F_score:  0.5304829770387964 0.5304829770387964\n",
      "TP:  336 / 438 all  831  accuracy:  0.7291288566243194  precision:  0.4043321299638989  recall:  0.7671232876712328  F_score:  0.5295508274231678 0.5295508274231678\n",
      "TP:  337 / 438 all  841  accuracy:  0.7254990925589837  precision:  0.40071343638525564  recall:  0.769406392694064  F_score:  0.5269741985926505 0.5269741985926505\n",
      "Random Seed is:  100\n",
      "5 fold no feature engineering\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP:  327 / 430 all  905  accuracy:  0.691016333938294  precision:  0.36132596685082874  recall:  0.7604651162790698  F_score:  0.4898876404494382 0.4898876404494382\n",
      "TP:  307 / 430 all  810  accuracy:  0.7159709618874773  precision:  0.3790123456790123  recall:  0.713953488372093  F_score:  0.4951612903225806 0.4951612903225806\n",
      "TP:  308 / 430 all  801  accuracy:  0.7209618874773139  precision:  0.38451935081148564  recall:  0.7162790697674418  F_score:  0.5004061738424046 0.5004061738424046\n",
      "TP:  309 / 430 all  807  accuracy:  0.719147005444646  precision:  0.3828996282527881  recall:  0.7186046511627907  F_score:  0.4995957962813258 0.4995957962813258\n",
      "TP:  309 / 430 all  807  accuracy:  0.719147005444646  precision:  0.3828996282527881  recall:  0.7186046511627907  F_score:  0.4995957962813258 0.4995957962813258\n",
      "TP:  311 / 430 all  815  accuracy:  0.7173321234119783  precision:  0.3815950920245399  recall:  0.7232558139534884  F_score:  0.4995983935742972 0.4995983935742972\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  333 / 430 all  923  accuracy:  0.6882940108892922  precision:  0.3607800650054171  recall:  0.7744186046511627  F_score:  0.49223946784922396 0.49223946784922396\n",
      "TP:  308 / 430 all  825  accuracy:  0.7100725952813067  precision:  0.37333333333333335  recall:  0.7162790697674418  F_score:  0.4908366533864541 0.4908366533864541\n",
      "TP:  297 / 430 all  772  accuracy:  0.7241379310344828  precision:  0.38471502590673573  recall:  0.6906976744186046  F_score:  0.4941763727121463 0.4941763727121463\n",
      "TP:  307 / 430 all  793  accuracy:  0.7236842105263158  precision:  0.3871374527112232  recall:  0.713953488372093  F_score:  0.5020441537203597 0.5020441537203597\n",
      "TP:  307 / 430 all  794  accuracy:  0.7232304900181489  precision:  0.3866498740554156  recall:  0.713953488372093  F_score:  0.5016339869281046 0.5016339869281046\n",
      "TP:  308 / 430 all  806  accuracy:  0.7186932849364791  precision:  0.38213399503722084  recall:  0.7162790697674418  F_score:  0.49838187702265374 0.49838187702265374\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  335 / 430 all  930  accuracy:  0.6869328493647913  precision:  0.3602150537634409  recall:  0.7790697674418605  F_score:  0.4926470588235293 0.4926470588235293\n",
      "TP:  315 / 430 all  842  accuracy:  0.7087114337568058  precision:  0.37410926365795727  recall:  0.7325581395348837  F_score:  0.4952830188679246 0.4952830188679246\n",
      "TP:  315 / 430 all  842  accuracy:  0.7087114337568058  precision:  0.37410926365795727  recall:  0.7325581395348837  F_score:  0.4952830188679246 0.4952830188679246\n",
      "TP:  315 / 430 all  845  accuracy:  0.707350272232305  precision:  0.3727810650887574  recall:  0.7325581395348837  F_score:  0.4941176470588236 0.4941176470588236\n",
      "TP:  315 / 430 all  845  accuracy:  0.707350272232305  precision:  0.3727810650887574  recall:  0.7325581395348837  F_score:  0.4941176470588236 0.4941176470588236\n",
      "TP:  318 / 430 all  856  accuracy:  0.70508166969147  precision:  0.37149532710280375  recall:  0.7395348837209302  F_score:  0.4945567651632971 0.4945567651632971\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  305 / 430 all  797  accuracy:  0.72005444646098  precision:  0.38268506900878296  recall:  0.7093023255813954  F_score:  0.49714751426242876 0.49714751426242876\n",
      "TP:  307 / 430 all  801  accuracy:  0.72005444646098  precision:  0.383270911360799  recall:  0.713953488372093  F_score:  0.4987814784727863 0.4987814784727863\n",
      "final and  original \n",
      "TP:  311 / 430 all  825  accuracy:  0.7127949183303085  precision:  0.37696969696969695  recall:  0.7232558139534884  F_score:  0.4956175298804781 0.4956175298804781\n",
      "TP:  312 / 430 all  825  accuracy:  0.7137023593466425  precision:  0.3781818181818182  recall:  0.7255813953488373  F_score:  0.49721115537848604 0.49721115537848604\n",
      "final and  middle and original \n",
      "TP:  307 / 430 all  815  accuracy:  0.7137023593466425  precision:  0.37668711656441717  recall:  0.713953488372093  F_score:  0.4931726907630522 0.4931726907630522\n",
      "TP:  307 / 430 all  814  accuracy:  0.7141560798548094  precision:  0.37714987714987713  recall:  0.713953488372093  F_score:  0.4935691318327974 0.4935691318327974\n",
      "TP:  311 / 430 all  824  accuracy:  0.7132486388384754  precision:  0.3774271844660194  recall:  0.7232558139534884  F_score:  0.49601275917065385 0.49601275917065385\n",
      "TP:  313 / 430 all  833  accuracy:  0.7109800362976406  precision:  0.375750300120048  recall:  0.727906976744186  F_score:  0.49564528899445764 0.49564528899445764\n",
      "Random Seed is:  1000\n",
      "5 fold no feature engineering\n",
      "TP:  304 / 404 all  901  accuracy:  0.6837568058076225  precision:  0.3374028856825749  recall:  0.7524752475247525  F_score:  0.4659003831417624 0.4659003831417624\n",
      "TP:  294 / 404 all  801  accuracy:  0.72005444646098  precision:  0.36704119850187267  recall:  0.7277227722772277  F_score:  0.4879668049792531 0.4879668049792531\n",
      "TP:  290 / 404 all  769  accuracy:  0.7309437386569873  precision:  0.37711313394018203  recall:  0.7178217821782178  F_score:  0.49445865302642794 0.49445865302642794\n",
      "TP:  290 / 404 all  786  accuracy:  0.7232304900181489  precision:  0.36895674300254455  recall:  0.7178217821782178  F_score:  0.4873949579831934 0.4873949579831934\n",
      "TP:  289 / 404 all  788  accuracy:  0.721415607985481  precision:  0.366751269035533  recall:  0.7153465346534653  F_score:  0.4848993288590604 0.4848993288590604\n",
      "TP:  290 / 404 all  796  accuracy:  0.7186932849364791  precision:  0.36432160804020103  recall:  0.7178217821782178  F_score:  0.48333333333333345 0.48333333333333345\n",
      "**************************************************\n",
      "5 fold feature engineering middle\n",
      "TP:  315 / 404 all  924  accuracy:  0.6833030852994555  precision:  0.3409090909090909  recall:  0.7797029702970297  F_score:  0.4743975903614458 0.4743975903614458\n",
      "TP:  298 / 404 all  831  accuracy:  0.7100725952813067  precision:  0.358604091456077  recall:  0.7376237623762376  F_score:  0.48259109311740883 0.48259109311740883\n",
      "TP:  291 / 404 all  795  accuracy:  0.72005444646098  precision:  0.3660377358490566  recall:  0.7202970297029703  F_score:  0.48540450375312766 0.48540450375312766\n",
      "TP:  295 / 404 all  820  accuracy:  0.7123411978221416  precision:  0.3597560975609756  recall:  0.7301980198019802  F_score:  0.4820261437908497 0.4820261437908497\n",
      "TP:  297 / 404 all  822  accuracy:  0.7132486388384754  precision:  0.3613138686131387  recall:  0.7351485148514851  F_score:  0.4845024469820554 0.4845024469820554\n",
      "TP:  298 / 404 all  833  accuracy:  0.7091651542649727  precision:  0.3577430972388956  recall:  0.7376237623762376  F_score:  0.48181083265966046 0.48181083265966046\n",
      "**************************************************\n",
      "5 fold feature engineering final\n",
      "TP:  316 / 404 all  919  accuracy:  0.6864791288566243  precision:  0.3438520130576714  recall:  0.7821782178217822  F_score:  0.47770219198790626 0.47770219198790626\n",
      "TP:  296 / 404 all  825  accuracy:  0.7109800362976406  precision:  0.35878787878787877  recall:  0.7326732673267327  F_score:  0.4816924328722539 0.4816924328722539\n",
      "TP:  301 / 404 all  813  accuracy:  0.7209618874773139  precision:  0.37023370233702335  recall:  0.745049504950495  F_score:  0.49465899753492193 0.49465899753492193\n",
      "TP:  302 / 404 all  825  accuracy:  0.7164246823956443  precision:  0.3660606060606061  recall:  0.7475247524752475  F_score:  0.49145646867371856 0.49145646867371856\n",
      "TP:  300 / 404 all  824  accuracy:  0.7150635208711433  precision:  0.3640776699029126  recall:  0.7425742574257426  F_score:  0.48859934853420195 0.48859934853420195\n",
      "TP:  301 / 404 all  831  accuracy:  0.7127949183303085  precision:  0.3622141997593261  recall:  0.745049504950495  F_score:  0.4874493927125506 0.4874493927125506\n",
      "**************************************************\n",
      "Fire!\n",
      "middle  and  original \n",
      "TP:  294 / 404 all  809  accuracy:  0.7164246823956443  precision:  0.36341161928306553  recall:  0.7277227722772277  F_score:  0.4847485572959604 0.4847485572959604\n",
      "TP:  294 / 404 all  810  accuracy:  0.7159709618874773  precision:  0.362962962962963  recall:  0.7277227722772277  F_score:  0.48434925864909395 0.48434925864909395\n",
      "final and  original \n",
      "TP:  298 / 404 all  809  accuracy:  0.72005444646098  precision:  0.3683559950556242  recall:  0.7376237623762376  F_score:  0.49134377576257215 0.49134377576257215\n",
      "TP:  298 / 404 all  811  accuracy:  0.719147005444646  precision:  0.36744759556103573  recall:  0.7376237623762376  F_score:  0.4905349794238683 0.4905349794238683\n",
      "final and  middle and original \n",
      "TP:  300 / 404 all  816  accuracy:  0.7186932849364791  precision:  0.36764705882352944  recall:  0.7425742574257426  F_score:  0.4918032786885246 0.4918032786885246\n",
      "TP:  302 / 404 all  821  accuracy:  0.7182395644283122  precision:  0.36784409257003653  recall:  0.7475247524752475  F_score:  0.49306122448979584 0.49306122448979584\n",
      "TP:  303 / 404 all  830  accuracy:  0.7150635208711433  precision:  0.3650602409638554  recall:  0.75  F_score:  0.4910858995137763 0.4910858995137763\n",
      "TP:  304 / 404 all  837  accuracy:  0.7127949183303085  precision:  0.3632019115890084  recall:  0.7524752475247525  F_score:  0.4899274778404513 0.4899274778404513\n"
     ]
    }
   ],
   "source": [
    "model_sample_strong_feature_middle = model_sample_strong_feature_middle.fillna(-999)\n",
    "model_sample_strong_feature_final = model_sample_strong_feature_final.fillna(-999)\n",
    "model_sample_ = model_sample.fillna(-999)\n",
    "for rnd in [1,10,100,1000]:\n",
    "    print('Random Seed is: ',rnd)  \n",
    "    train_X,test_X, train_y, test_y = train_test_split(model_sample_strong_feature_final,label,test_size=0.2,random_state=rnd) \n",
    "    \n",
    "    train_X_orig = model_sample_.loc[train_X.index]\n",
    "    test_X_orig = model_sample_.loc[test_X.index]\n",
    "    train_X_middle = model_sample_strong_feature_middle.loc[train_X.index]\n",
    "    test_X_middle = model_sample_strong_feature_middle.loc[test_X.index]\n",
    "    \n",
    "    \n",
    "    print('5 fold no feature engineering')\n",
    "    pred_out_lgb, pred_out_gbdt, pred_out_rf = N_Fold_Predict(train_X_orig, train_y['y'].values, test_X_orig, cv_ = 3)\n",
    "    pred =pred_out_rf >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_gbdt >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb * 0.55 + 0.45 * pred_out_gbdt>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb * 0.5 + 0.5 * pred_out_gbdt>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =(pred_out_lgb * 0.5 + 0.5 * pred_out_gbdt) * 0.9 + 0.1 * pred_out_rf>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    print('*' * 50)\n",
    "    print('5 fold feature engineering middle')\n",
    "    pred_out_lgb_middle, pred_out_gbdt_middle, pred_out_rf_middle = N_Fold_Predict(train_X_middle,train_y['y'].values, test_X_middle, cv_ = 3)\n",
    "    pred = pred_out_rf_middle >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_gbdt_middle >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_middle >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    pred =pred_out_lgb_middle * 0.55 + 0.45 * pred_out_gbdt_middle>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_middle * 0.5 + 0.5 * pred_out_gbdt_middle>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =(pred_out_lgb_middle * 0.5 + 0.5 * pred_out_gbdt_middle) * 0.9 + 0.1 * pred_out_rf_middle >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "     \n",
    "#     pred =(pred_out_lgb_middle * 0.5 + 0.5 * pred_out_gbdt_middle) * 0.9 + 0.05 * (pred_out_rf_middle + pred_out_rf)>= 0.215\n",
    "#     get_score(pred, test_y['y'].values)\n",
    "     \n",
    "    \n",
    "    print('*' * 50)\n",
    "    print('5 fold feature engineering final')\n",
    "    pred_out_lgb_final, pred_out_gbdt_final, pred_out_rf_final = N_Fold_Predict(train_X,train_y['y'].values, test_X, cv_ = 5)\n",
    "    pred =pred_out_rf_final >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_gbdt_final >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_final >= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    pred =pred_out_lgb_final * 0.55 + 0.45 * pred_out_gbdt_final>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =pred_out_lgb_final * 0.5 + 0.5 * pred_out_gbdt_final>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =(pred_out_lgb_final * 0.5 + 0.5 * pred_out_gbdt_final) * 0.9 + 0.1 * pred_out_rf_final>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    \n",
    "    print('*' * 50)\n",
    "     \n",
    "    print('Fire!')\n",
    "    print('middle  and  original ')\n",
    "    \n",
    "    pred =((pred_out_lgb_middle * 0.5 + pred_out_lgb * 0.5 )*0.55  + 0.45 * (pred_out_gbdt_middle * 0.5 + 0.5 * pred_out_gbdt))>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =((pred_out_lgb_middle * 0.5 + pred_out_lgb* 0.5 )*0.5  + 0.5 * (pred_out_gbdt_middle *0.5 + 0.5 * pred_out_gbdt ))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "     \n",
    "    print('final and  original ')\n",
    "    pred =((pred_out_lgb_final * 0.5 + pred_out_lgb * 0.5 )*0.55  + 0.45 * (pred_out_gbdt_final * 0.5 + 0.5 * pred_out_gbdt))>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =((pred_out_lgb_final * 0.5 + pred_out_lgb* 0.5 )*0.5  + 0.5 * (pred_out_gbdt_final *0.5 + 0.5 * pred_out_gbdt ))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "     \n",
    "    \n",
    "    print('final and  middle and original ')\n",
    "    pred =((pred_out_lgb_final * 0.3 + pred_out_lgb * 0.3 + 0.4 * pred_out_lgb_middle)*0.55  + 0.45 * (pred_out_gbdt_final * 0.3 + 0.3 * pred_out_gbdt + 0.4 * pred_out_gbdt_middle))>= 0.215\n",
    "    get_score(pred, test_y['y'].values)\n",
    "    pred =((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    \n",
    "    pred = 0.1 * (pred_out_rf_middle + pred_out_rf + pred_out_rf_final ) /3.0 + 0.9 * ((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    pred = 0.15 * (pred_out_rf_middle + pred_out_rf + pred_out_rf_final ) /3.0 + 0.85 * ((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.215\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    \n",
    "    pred = 0.1 * (pred_out_rf_middle + pred_out_rf + pred_out_rf_final ) /3.0 + 0.9 * ((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.23\n",
    "    get_score(pred, test_y['y'].values) \n",
    "    pred = 0.15 * (pred_out_rf_middle + pred_out_rf + pred_out_rf_final ) /3.0 + 0.85 * ((pred_out_lgb_final * 1.0 /3 + pred_out_lgb* 1.0 /3 +  pred_out_lgb_middle* 1.0 /3)*0.5  + 0.5 * (pred_out_gbdt_final * 1.0 /3 +  1.0 /3 * pred_out_gbdt + 1.0 /3 * pred_out_gbdt_middle))>= 0.23\n",
    "    get_score(pred, test_y['y'].values) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B10C949662E84480875BF9C8BC942354"
   },
   "source": [
    "### 实验小结\n",
    "从上面的实验中，我们可以得到如下的结论：\n",
    "1. 采用集成的方式，我们的模型在所有的训练集上都可以获得稳定的提升(线上线下是一致的情况)；\n",
    "2. 模型的结果波动有些大，大概在0.49-0.52之间波动，因为数据少，所以随机性会较大，较为合理；\n",
    "3. 实验证明我们多个子集以及N折交叉的结果相比于单个模型的结果不仅效果好，而且稳定很多(集成大概会提升0.01-0.02个点)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A9EA35758E94DEE8F3011B2405E34CB"
   },
   "source": [
    "# 总结与展望\n",
    "## 总结\n",
    "### 本次比赛难点以及解决方案\n",
    "因为本次赛事的数据特征都相对较好，但是因为数据的个数相对较少，所以我们的重点就落在了下面的三个问题上：\n",
    "\n",
    "1. 如何进一步挖掘更好的特征\n",
    "2. 如何在少量数据的问题上提升模型的性能同时提高模型的鲁棒性\n",
    "3. 如何优化这种不可以直接求导的目标函数\n",
    "\n",
    "而针对上面的三个问题，我们给出了如下的解决方案：\n",
    "1. 我们尽可能提取有意义的特征同时将 ：最后我们给出了新的5类特征：1.提高模型的表达能力的特征¶；2.比例特征；3.标准差还原特征（反映信息的波动）；4.均值特征;5.趋势特征；具体的细节可以参考特征的构建部分。\n",
    "2. 我们采用多个模型集成的方式来提升模型的性能同时提高模型的鲁棒性；这些又由下面两个模块组成：多个不同训练集的模型融合+多个不同模型的融合\n",
    "3. 优化F-score一共有3种常见的方法，包括加权，转化为类别不平衡的问题；优化F-score的下界或者上届近似函数；设置阈值等，此处我们为了方便，选择直接使用设置阈值的方式进行。\n",
    "\n",
    "### 方案总结\n",
    "模型的优点：\n",
    "1. 鲁棒性好，稳定，性能相对不错；\n",
    "2. 模型的架构较为完善，可以获得稳步提升；\n",
    "3. 给出了较好的特征构建的思路；\n",
    "\n",
    "## 展望\n",
    "本次比赛还可以进行进一步的完善，我们将其总结如下：\n",
    "1. 尽可能扩展数据集（本质）\n",
    "2. 调整模型的参数，因为数据集相对较小的情况下，参数的影响会相对较大，所以参数的调整往往也可以带来较大的收获；\n",
    "3. 提取一些高质量特征，例如一些差值特征等，贷款与还款的差值等等，相信这些特征从多个角度对模型带来帮助。\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
