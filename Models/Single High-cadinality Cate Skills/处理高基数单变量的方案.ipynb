{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#问题解答:\" data-toc-modified-id=\"问题解答:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>问题解答:</a></span></li><li><span><a href=\"#简介\" data-toc-modified-id=\"简介-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href=\"#处理单个高基数(High-Cadinality)特征的方法\" data-toc-modified-id=\"处理单个高基数(High-Cadinality)特征的方法-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>处理单个高基数(High-Cadinality)特征的方法</a></span><ul class=\"toc-item\"><li><span><a href=\"#One-Hot编码\" data-toc-modified-id=\"One-Hot编码-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>One-Hot编码</a></span></li><li><span><a href=\"#Label编码\" data-toc-modified-id=\"Label编码-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Label编码</a></span></li><li><span><a href=\"#Count编码\" data-toc-modified-id=\"Count编码-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Count编码</a></span></li><li><span><a href=\"#nan编码\" data-toc-modified-id=\"nan编码-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>nan编码</a></span></li><li><span><a href=\"#expansion编码\" data-toc-modified-id=\"expansion编码-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>expansion编码</a></span></li><li><span><a href=\"#consolidation编码\" data-toc-modified-id=\"consolidation编码-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>consolidation编码</a></span></li><li><span><a href=\"#Mean-target编码\" data-toc-modified-id=\"Mean-target编码-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Mean-target编码</a></span><ul class=\"toc-item\"><li><span><a href=\"#Leave-one-out-mean-target-编码\" data-toc-modified-id=\"Leave-one-out-mean-target-编码-3.7.1\"><span class=\"toc-item-num\">3.7.1&nbsp;&nbsp;</span>Leave-one-out mean-target 编码</a></span></li><li><span><a href=\"#K-fold-mean-target-编码\" data-toc-modified-id=\"K-fold-mean-target-编码-3.7.2\"><span class=\"toc-item-num\">3.7.2&nbsp;&nbsp;</span>K-fold mean-target 编码</a></span></li><li><span><a href=\"#其他\" data-toc-modified-id=\"其他-3.7.3\"><span class=\"toc-item-num\">3.7.3&nbsp;&nbsp;</span>其他</a></span></li></ul></li></ul></li><li><span><a href=\"#小结\" data-toc-modified-id=\"小结-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>小结</a></span></li><li><span><a href=\"#推荐阅读资料\" data-toc-modified-id=\"推荐阅读资料-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>推荐阅读资料</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题解答:\n",
    "1.南京工业赛的数据在哪可以下载？\n",
    "\n",
    "- 目前南京工业赛的数据已经上传至百度网盘:https://pan.baidu.com/s/1hzdP59IbZKOY4MGpozMOSA, 提取码为:9hwc\n",
    "\n",
    "2.KDD的代码在哪里可以下载？\n",
    "\n",
    "- 目前已经上传至我的Github,欢迎有兴趣的前往下载:https://github.com/dayeren/Kaggle_Competition_Treasure/tree/master/KDD19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简介\n",
    "\n",
    "在很多表格类的问题中，高基数的特征类别处理一直是一个困扰着很多人的问题，究竟哪一种操作是最好的，很难说，往往是不同的数据集存在不同的特性，可能某一个操作这A数据集上取得了提升，但是在B数据集上就不行了，但是知道的技巧越多，我们成功的概率往往也会越大。\n",
    "\n",
    "本篇文章，我们会介绍几种常见的处理High-Cadinality类别特征的方法。\n",
    "\n",
    "\n",
    "# 处理单个高基数(High-Cadinality)特征的方法\n",
    "\n",
    "## One-Hot编码\n",
    "\n",
    "One-Hot编码**将一个维度为M的变量转变为M个二元向量**,具体的例子如下:\n",
    "\n",
    "![](./pic/one-hot.png)\n",
    "\n",
    "我们发现One-Hot编码将我们的数据展开之后内存的消耗变得非常大,但是它的好处在于,这样的数据我们的线性模型可以更好的吸收High-Cadinality的类别信息,原先我们的采用线性模型,那么我们类别变量A的对预测带来的贡献为,w\\*A, (A由(A1,A2,...,AN组成)我们发现类别2的贡献就是类别1的一倍,这很明显和我们的直觉不符,但是展开之后,我们类别变量A对预测带来的贡献为:w1 \\* A1 + w2\\*A2 + ... + wN \\* AN. 变量之间的关系变得更加合理了。\n",
    "\n",
    "所以One-Hot编码对于很多线性模型是有必要的。那么对于LightGBM之类的树模型是否有必要呢？也是有的，当我们的类别变量中有一些变量是人为构造的，加入了很多噪音，这个时候将其展开，那么模型可以更加快的找到那些非构建的类别。(参考讯飞18年举办的推荐比赛)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label编码\n",
    "\n",
    "Label编码可以**将类型为object的变量转变为数值形式**,具体的例子如下:\n",
    "\n",
    "![](./pic/label-encode.png)\n",
    "\n",
    "Label编码默认会先将object类型的变量进行排序,然后按照大小顺序进行0-N-1的编码,此处N为该特征中不同变量的个数。几乎所有的赛题中都会这么做，这样我们模型至少可以吸收该特征的信息，虽然可能只吸收10%左右，但是总比浪费该变量的信息好很多。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count编码\n",
    "\n",
    "Count编码也经常被称作是频率编码，它通过计算每个值的出现次数来表示该特征的信息,具体的例子如下:\n",
    "\n",
    "![](./pic/count-encode.png)\n",
    "\n",
    "Count编码几乎在所有的比赛中都会被使用，它可以反映很多信息，例如商品销量预测中某些商品的出现次数特别多，那么往往说明该商品是非常热门的;微软的设备被攻击概率的比赛中，预测设备受攻击的概率，那么设备安装的软件是非常重要的信息，此时安装软件的count编码可以反映该软件的流行度，越流行的产品的受众越多，那么黑客往往会倾向对此类产品进行攻击，这样黑客往往可以获得更多的利益。\n",
    "\n",
    "在很多实践问题中，Count编码往往都可以给模型的性能带来非常大的帮助。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nan编码\n",
    "\n",
    "Nan编码是处理缺失值的一种常见方案，它会额外生成一列来表示某特征列是否缺失的信息，如果缺失则为1，否则为0, 所以不管是不是High-cadinality的类别列都可以尝试一下，具体的例子如下:\n",
    "\n",
    "![](./pic/nan-encode.png)\n",
    "\n",
    "在很多问题中,缺失值有非常多含义的,例如在很多银行贷款的问题中，缺失可能是用户故意不去填充某些信息造成的，而这些用户往往是不良用户；有时候也有可能是因为数据收集时不小心漏掉的，此时，这类数据可以不进行填充，让模型自动填充。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expansion编码\n",
    "\n",
    "expansion编码常常出现在一些复杂的字符串中，例如一些带有版本信息的字符串，很多版本号的信息中涵盖了时间以及编号等信息，我们需要将其拆分开,**形成多个新的特征列**,例如下面的例子:\n",
    "\n",
    "![](./pic/expansion-encode.png)\n",
    "\n",
    "这类编码类似于一种带有业务信息的聚类信息,可以加速树模型的搜索速度,往往是一类非常不错的特征."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## consolidation编码\n",
    "\n",
    "consolidation编码常常出现在一些特殊的字符串中，例如一些带有地址的字符串，字符串会给出详细的信息，某市某区等,这时我们可以将市抽取来作为一个全新的特征,例如下面的例子:\n",
    "\n",
    "![](./pic/consolidation-encode.png)\n",
    "\n",
    "这类编码和上面的expansion编码类似，也是一种带有业务信息的聚类信息,可以加速树模型的搜索速度,也是一类非常不错的特征."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-target编码\n",
    "\n",
    "Mean-target编码是06年就提出的一种结合标签进行编码的技术，该技术在很多比赛都取得了非常好的效果，但是特别需要注意过拟合的问题。在kaggle竞赛中成功的案例有owen zhang的leave-one-out的操作和莫斯科gm的基于K-fold的mean-target编码,此处我们介绍两种Mean-target编码;\n",
    "\n",
    "### Leave-one-out mean-target 编码\n",
    "\n",
    "Leave-one-out mean-target编码的思路相对简单,我们每次编码时,不考虑当前样本的情况,用其他样本对应的标签的均值作为我们的编码，而测试集则用全部训练集样本的均值进行编码,案例如下：\n",
    "\n",
    "<img src=\"./pic/leave-one-out-encode.png\" width=\"200\" hegiht=\"313\" align=center />\n",
    "\n",
    "上面的案例摘自owen-zhang(曾经的kaggle第一名)的分享。\n",
    "\n",
    "### K-fold mean-target 编码\n",
    "\n",
    "K-fold mean-target编码的步骤为:\n",
    "\n",
    "---------\n",
    "1. 将训练集划分为K折;\n",
    "2. 对第A折的样本进行编码时,我们删除K折中A折,并用剩余的数据计算如下公式:\n",
    "![](./pic/mean-encode-eqa.png)\n",
    "3. 然后利用上面计算得到的值对第A折进行编码。\n",
    "\n",
    "---------\n",
    "\n",
    "首先我们先理解一下上面的公式,最原始的Mean-target编码是最容易导致过拟合的,这其中过拟合的最大的原因之一在于对于一些特征列中出现次数很少的值过拟合了，比如某些值只有1个或者2到3个，但是这些样本对应的标签全部是1，怎么办，他们的编码值就应该是1，但是很明显这些值的统计意义不大，大家可以通过伯努利分布去计算概率来理解。而如果我们直接给他们编码了，就会误导模型的学习。那么我们该怎么办呢？\n",
    "\n",
    "**老办法，加正则！**\n",
    "\n",
    "于是我们就有了上面的计算式子,式子$n_c$是值c出现的次数,$p_c$是它对应的概率,$p_{global}$是全局的均值, 那么当$\\alpha$为0同时$n_c$比较小的时候, 就会有大概率出现过拟合的现象,此时我们调大$\\alpha$就可以缓解这一点,所以很多时候都需要不断地去调整$\\alpha$的值。\n",
    "\n",
    "### 其他\n",
    "\n",
    "除了上面这种最常用的方法，还有一种较常见的叫做:Expanding mean regularisation,它的流程如下:\n",
    "1. Fix some random permutation of rows (samples)\n",
    "2. Moving from top to bottom, for each example, estimate the encoding using target mean of all the examples before the estimated one. An estimated example is not used.\n",
    "\n",
    "最后我们把几种不同方法的结果展示如下，希望对大家有一定的参考。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pic/mean-target-encode-example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 小结\n",
    "\n",
    "在本篇文章中，我们介绍了处理高基数单类别变量的7中常用方法以及有效的场景，本篇文章我们并没有给出对应的代码，欢迎有兴趣的自己去实现同时阅读后面我推荐的资料。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推荐阅读资料\n",
    "1. https://blog.socratesk.com/blog/2018/06/17/featuren-engineering-and-extraction\n",
    "2. https://datascience.stackexchange.com/questions/10509/how-to-deal-with-categorical-feature-of-very-high-cardinality\n",
    "3. KaggleDays SF: 2. Amazon - Unsupervised encoding:https://www.kaggle.com/dmitrylarko/kaggledays-sf-2-amazon-unsupervised-encoding\n",
    "4. Mean (likelihood) encodings: a comprehensive study：https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study\n",
    "5. http://blog.kaggle.com/2015/06/22/profiling-top-kagglers-owen-zhang-currently-1-in-the-world/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
