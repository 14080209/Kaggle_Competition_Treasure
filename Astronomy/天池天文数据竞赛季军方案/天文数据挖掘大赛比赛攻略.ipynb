{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#赛题背景\" data-toc-modified-id=\"赛题背景-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>赛题背景</a></span><ul class=\"toc-item\"><li><span><a href=\"#竞赛题目\" data-toc-modified-id=\"竞赛题目-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>竞赛题目</a></span></li><li><span><a href=\"#赛题数据,提交案例与评价指标\" data-toc-modified-id=\"赛题数据,提交案例与评价指标-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>赛题数据,提交案例与评价指标</a></span><ul class=\"toc-item\"><li><span><a href=\"#赛题数据\" data-toc-modified-id=\"赛题数据-1.2.1\"><span class=\"toc-item-num\">1.2.1&nbsp;&nbsp;</span>赛题数据</a></span></li><li><span><a href=\"#提交示例\" data-toc-modified-id=\"提交示例-1.2.2\"><span class=\"toc-item-num\">1.2.2&nbsp;&nbsp;</span>提交示例</a></span></li><li><span><a href=\"#赛题评价指标\" data-toc-modified-id=\"赛题评价指标-1.2.3\"><span class=\"toc-item-num\">1.2.3&nbsp;&nbsp;</span>赛题评价指标</a></span></li><li><span><a href=\"#注意事项\" data-toc-modified-id=\"注意事项-1.2.4\"><span class=\"toc-item-num\">1.2.4&nbsp;&nbsp;</span>注意事项</a></span></li></ul></li></ul></li><li><span><a href=\"#问题分析与理解\" data-toc-modified-id=\"问题分析与理解-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>问题分析与理解</a></span><ul class=\"toc-item\"><li><span><a href=\"#本题难点\" data-toc-modified-id=\"本题难点-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>本题难点</a></span></li><li><span><a href=\"#求解思路初步\" data-toc-modified-id=\"求解思路初步-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>求解思路初步</a></span><ul class=\"toc-item\"><li><span><a href=\"#难点1的解决方案\" data-toc-modified-id=\"难点1的解决方案-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>难点1的解决方案</a></span></li><li><span><a href=\"#难点2的解决方案\" data-toc-modified-id=\"难点2的解决方案-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>难点2的解决方案</a></span></li></ul></li></ul></li><li><span><a href=\"#方案细节以及核心代码\" data-toc-modified-id=\"方案细节以及核心代码-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>方案细节以及核心代码</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型1：基于LGB的模型\" data-toc-modified-id=\"模型1：基于LGB的模型-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>模型1：基于LGB的模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#特征工程\" data-toc-modified-id=\"特征工程-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>特征工程</a></span><ul class=\"toc-item\"><li><span><a href=\"#机器特征\" data-toc-modified-id=\"机器特征-3.1.1.1\"><span class=\"toc-item-num\">3.1.1.1&nbsp;&nbsp;</span>机器特征</a></span></li><li><span><a href=\"#滑窗统计特征\" data-toc-modified-id=\"滑窗统计特征-3.1.1.2\"><span class=\"toc-item-num\">3.1.1.2&nbsp;&nbsp;</span>滑窗统计特征</a></span></li></ul></li><li><span><a href=\"#模型\" data-toc-modified-id=\"模型-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>模型</a></span></li></ul></li><li><span><a href=\"#模型2：基于神经网络的模型\" data-toc-modified-id=\"模型2：基于神经网络的模型-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>模型2：基于神经网络的模型</a></span><ul class=\"toc-item\"><li><span><a href=\"#数据预处理\" data-toc-modified-id=\"数据预处理-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;</span>数据预处理</a></span></li><li><span><a href=\"#神经网络框架的设计\" data-toc-modified-id=\"神经网络框架的设计-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;</span>神经网络框架的设计</a></span></li></ul></li><li><span><a href=\"#进一步提升MicroFscore\" data-toc-modified-id=\"进一步提升MicroFscore-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>进一步提升MicroFscore</a></span><ul class=\"toc-item\"><li><span><a href=\"#转化为求解代价敏感问题\" data-toc-modified-id=\"转化为求解代价敏感问题-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;</span>转化为求解代价敏感问题</a></span><ul class=\"toc-item\"><li><span><a href=\"#问题转化\" data-toc-modified-id=\"问题转化-3.3.1.1\"><span class=\"toc-item-num\">3.3.1.1&nbsp;&nbsp;</span>问题转化</a></span></li><li><span><a href=\"#我们的发现以及新方案\" data-toc-modified-id=\"我们的发现以及新方案-3.3.1.2\"><span class=\"toc-item-num\">3.3.1.2&nbsp;&nbsp;</span>我们的发现以及新方案</a></span></li></ul></li><li><span><a href=\"#模型集成细节\" data-toc-modified-id=\"模型集成细节-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;</span>模型集成细节</a></span><ul class=\"toc-item\"><li><span><a href=\"#模型内部集成\" data-toc-modified-id=\"模型内部集成-3.3.2.1\"><span class=\"toc-item-num\">3.3.2.1&nbsp;&nbsp;</span>模型内部集成</a></span></li><li><span><a href=\"#模型间集成\" data-toc-modified-id=\"模型间集成-3.3.2.2\"><span class=\"toc-item-num\">3.3.2.2&nbsp;&nbsp;</span>模型间集成</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#三轮比赛的结果\" data-toc-modified-id=\"三轮比赛的结果-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>三轮比赛的结果</a></span><ul class=\"toc-item\"><li><span><a href=\"#初赛(基于LGB的方案)\" data-toc-modified-id=\"初赛(基于LGB的方案)-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>初赛(基于LGB的方案)</a></span></li><li><span><a href=\"#复赛(基于LGB+Deep+集成的方案)\" data-toc-modified-id=\"复赛(基于LGB+Deep+集成的方案)-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>复赛(基于LGB+Deep+集成的方案)</a></span></li><li><span><a href=\"#决赛(基于Deep的方案)\" data-toc-modified-id=\"决赛(基于Deep的方案)-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>决赛(基于Deep的方案)</a></span></li></ul></li><li><span><a href=\"#比赛经验总结和感想\" data-toc-modified-id=\"比赛经验总结和感想-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>比赛经验总结和感想</a></span><ul class=\"toc-item\"><li><span><a href=\"#比赛总结\" data-toc-modified-id=\"比赛总结-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>比赛总结</a></span><ul class=\"toc-item\"><li><span><a href=\"#方案优点\" data-toc-modified-id=\"方案优点-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>方案优点</a></span></li></ul></li><li><span><a href=\"#方案改进\" data-toc-modified-id=\"方案改进-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>方案改进</a></span></li></ul></li><li><span><a href=\"#参考文献\" data-toc-modified-id=\"参考文献-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>参考文献</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 赛题背景\n",
    "\n",
    "比赛的链接:https://tianchi.aliyun.com/competition/information.htm?spm=5176.100067.5678.2.1f1817178BW7Od&raceId=231646\n",
    " \n",
    "\n",
    "## 竞赛题目\n",
    "\n",
    "在天文学中，光谱描述了天体的辐射特性，以不同波长处辐射强度的分布来表示。每条观测得到的光谱主要是由黑体辐射产生的连续谱、天体中元素的原子能级跃迁产生的特征谱线（吸收线、发射线）以及噪声组成。通常天文学家依据光谱的特征谱线和物理参数等来判定天体的类型。在目前的LAMOST巡天数据发布中，光谱主要被分为恒星、星系、类星体和未知天体四大类。 LAMOST数据集中的每一条光谱提供了3690-9100埃的波长范围内的一系列辐射强度值。光谱自动分类就是要从上千维的光谱数据中选择和提取对分类识别最有效的特征来构建特征空间，例如选择特定波长或波段上的光谱流量值等作为特征，并运用算法对各种天体进行区分。\n",
    "\n",
    "本次大赛旨在通过机器学习算法对LAMOST DR3光谱进行自动分类（STAR/GALAXY/QSO/UNKNOWN），参赛选手需要设计高效高准确率的算法来解决这个天文学研究中的实际问题。\n",
    " \n",
    "\n",
    "## 赛题数据,提交案例与评价指标\n",
    "\n",
    "### 赛题数据\n",
    "\n",
    "**1**.赛题数据包括索引文件（index.csv）和波段文件（id.txt集合的zip）两部分：\n",
    "\n",
    "> 1）索引文件的第一行是字段名，之后每一行代表一个天体。索引文件的第一个字段为波段文件id号。训练集的索引文件记录了波段文件id号以及分类信息，测试集的索引文件记录了波段文件id号，需要预测分类信息。\n",
    "\n",
    "> 2）波段文件是txt后缀的文本文件，存储的是已经插值采样好的波段数据，以逗号分隔。所有波段文件的波段区间和采样点都相同，采样点个数都是2600个。\n",
    "\n",
    "> 3）带 train 为训练集；带 test 为第一阶段测试集；带 rank 为第二阶段测试集。\n",
    "\n",
    "**2**.Unknown数据补充说明：\n",
    "\n",
    "> 1）LAMOST数据集中的unknown类别是由于光谱质量（信噪比低）等原因，未能够给出确切的分类的天体；\n",
    "\n",
    "> 2）Unknown分类目前由程序给出，其中不排除有恒星、星系和类星体。\n",
    "\n",
    "### 提交示例\n",
    "\n",
    "参赛结果以csv文件上传，用逗号分隔，且不用包含字段名。文件共两列，第一列为波段文件id号，与索引文件中的一致。第二列为分类信息，为galaxy，star，qso和unknown中的一种。样例如下：\n",
    "\n",
    "\n",
    "![](./pictures/submit.png)\n",
    "\n",
    "\n",
    "### 赛题评价指标\n",
    "\n",
    "考虑到imbalance data的特性和易计算性，用marco f1 score作为评分规则。\n",
    "\n",
    "* marco f1 score是每一类算出的 f1 score的算术平均。\n",
    "\n",
    "以四类分类为例：每一类以oneVSall计算各类的f1 score\n",
    "\n",
    "![](./pictures/evaluation_index.png)\n",
    "\n",
    "\n",
    "### 注意事项\n",
    "\n",
    "大赛不允许使用外部数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 问题分析与理解\n",
    "\n",
    "## 本题难点\n",
    "\n",
    "通过对数据的分析，我们将本次比赛的难点总结为如下两点：\n",
    "\n",
    "1. 所有样本的特征在**同一量纲**上面且**具有相同的物理意义**，相邻特征之间存在较强的关系，如何挖掘特征之间潜藏的关系？\n",
    "2. 样本存在类别不平衡的情况，评价指标无法直接优化，如何解决类别不平衡的情况\n",
    "\n",
    "\n",
    "## 求解思路初步\n",
    "\n",
    "### 难点1的解决方案\n",
    "\n",
    "所有样本的特征在同一量纲上面且具有相同的物理意义，相邻特征之间存在较强的关系，为了挖掘特征之间存在的较强关系，我们采用如下两种不同的思路。\n",
    "\n",
    "1. 传统方法：\n",
    "> **特征构建思路**：采用滑窗的方式进行统计特征的计算（包含均值，方差，大于0的个数，偏度等等），将滑窗的大小设置为不同的值，从而提取局部以及全局的特征；将统计特征分为机器特征以及人为统计特征，其中机器特征包括正则化之后的PCA,ICA,NMF等，至于什么时候这些特征能带来较大的帮助，可以参考我写的文章；人为的统计特征，我们将其归纳为零阶特征（原始数据+均值+方差等统计特征），一阶特征，相临的值的差值的均值方差，均值等统计特征，来反映曲线的波动等；二阶特征，差值差值的统计特征；因为一般零阶+一阶+二阶特征已经能教好的反映曲线的关系，所以我们最终就构建了零阶一阶二阶的特征。<br />\n",
    "> **传统方法优点**：简单直接，易理解，可解释性强；<br />\n",
    "> **传统方法缺点**：构建特征的方式受到滑窗大小的选择的影响，此处传统的统计特征我们仅仅只使用了pandas里面自带的，还有很多其他的统计特征，所以不全，会漏掉很多信息，数据样本较多，采用传统方法所得到的结果会带来较大的内存消耗以及计算资源。\n",
    "\n",
    "2. 深度方法：\n",
    "> **特征构建思路**：因为数据量相对较大，而且特征都在同一量纲上面有相同的物理意义，神经网络在该类的问题上面往往可以取得非常好的效果，我们希望能获得局部特征同时希望获得全局特征，所以构建神经网络的时候，我们选择较小的卷积核，但是构建较深的网络(四层)；为了获取更多的信息，我们选用不同的卷积核，整体的结构设计师参考的魏秀参的DAN网络以及NASNET网络的形式。<br />\n",
    "> **深度方法优点**：特征自动生成，省去很多人力去构建人为的特征；<br />\n",
    "> **深度方法缺点**：模型的结构较难调整，不同的模型结构带来的影响非常大，需要有GPU支撑。\n",
    "\n",
    " \n",
    "\n",
    "### 难点2的解决方案\n",
    "\n",
    "本赛题的优化指标是根据数据不平衡设计的，所以我们仅仅讨论**如何对类别不平衡同时优化指标不可导的问题的求解方案**。求解该问题的方法有很多，我仅仅列出我总结的几种方案。\n",
    "\n",
    "1. 通过优化相关性高的指标，例如categorical entropy；对于类别不平衡的数据时往往需要进行重采样或者加权的方式才能取得较好的效果；\n",
    "> 该类方法较为简单且往往可以得到还不错的次优解,可以作为Baseline。但是波动较大，运气成份较高。典型的例子如下：\n",
    "![](./pictures/Acc_AUC.png)\n",
    "\n",
    "2. 调整模型预测结果的概率的阈值(最常见与F1 score问题当中,在Micro Fscore上面做细微调整);\n",
    "> 该方法往往可以取得相对于上面的方法更好的效果，也常常出现在各类比赛中，最经典的参考文献有[4,5]\n",
    "> 三种微调方法：①对类别较少的样本的预测概率进行排序，取topN的元素作为最终结果;②对同一个样本的每个类的概率进行对比，如果两个类别的预测概率类似(自定义阈值)，可以对将预测结果修改为类别个数较少的那个;③对预测结果进行再学习,之前我们团队提出的,现在论文再投.\n",
    "\n",
    "3. 设计评估指标的近似可微函数；\n",
    "> 这是最好的方式，较好的方法有谷歌2017年的论文[1]，通过Bounding Blocks技术，将原先不可导的问题转化为可导的上下界。<br />\n",
    "> <font color=red>**Bounding Box技术**</font>: ![](./pictures/bouding_box.png)\n",
    "\n",
    "4. 问题转化，将原先不可直接求导的评估指标转化为等价的可以求导的问题. (将求解MicroF score的问题转化为代价敏感的分类问题进行求解)[3] \n",
    "> <font color=red>**求解MicroF score的问题转化为如下的代价敏感(有理论保证,而且算法简单,但是对于神经网络来说代价较大)**</font>,下面的截图来源于论文[6] \n",
    "![](./pictures/f1_to_cost_sensitive1.png)\n",
    "![](./pictures/f1_to_cost_sensitive2.png) \n",
    "5. 一种基于Precision+Recall的融合方案（我们团队自己设计,暂时不清楚文献中是否有）\n",
    "\n",
    "最终我们团队采用第四种方法(改进版)以及第五种方法的综合来求解该问题,前两种方案也都进行了尝试,效果会有提升,可以作为Baseline,第三种方案未进行尝试,打算作为未来工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方案细节以及核心代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型1：基于LGB的模型\n",
    "\n",
    "### 特征工程\n",
    "\n",
    "#### 机器特征\n",
    "\n",
    "使用下面的机器特征,是因为在观测lgb特征重要性的时候,我们发现滑窗大小为2600时的skewness的重要性非常高,而PCA提取的特征,我们将其作为另外一种特征（另外一种方差,也是分布的一种显示）\n",
    "\n",
    "- PCA特征\n",
    "- NMF特征\n",
    "- ICA特征\n",
    "- FCA特征\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components= 100)\n",
    "nmf.fit(train_test_feature * (train_test_feature>0))  \n",
    "train_test_nmf = nmf.transform(train_test_feature* (train_test_feature>0)) \n",
    "\n",
    "pca = PCA(n_components=300)\n",
    "pca.fit(train_test_feature) \n",
    "train_test_pca = pca.transform(train_test_feature)\n",
    "\n",
    "\n",
    "fca = FactorAnalysis(n_components=200)\n",
    "ica = FastICA(n_components=200)\n",
    "\n",
    "fca.fit(train_test_feature)  \n",
    "ica.fit(train_test_feature)  \n",
    "\n",
    "train_test_fca = fca.transform(train_test_feature)\n",
    "train_test_ica = ica.transform(train_test_feature) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 滑窗统计特征\n",
    "我们设计特征的思路如下:\n",
    "\n",
    "- 零阶特征:反映曲线的原始分布,包括极值等;\n",
    "- 一阶特征:反映曲线波动的情况\n",
    "- 二阶特征:反映曲线波动的波动\n",
    "- 其他的补充特征,包括大于0的个数等等\n",
    "\n",
    "具体的细节可以参考下面的代码.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_windows_featues(train_test):\n",
    "    train_test_ = pd.DataFrame({'id':train_test.index}) \n",
    "    train_test_diff = train_test.diff(1,axis=1).fillna(0)\n",
    "    train_test_big_0 = train_test > 0\n",
    "    train_test_abs = train_test.apply(np.abs) \n",
    "    train_test_abs_big_0 = train_test_big_0 * train_test_abs\n",
    "    \n",
    "    window_sizes = [20,50, 100, 200, 310,500, 625, 1000, 1300, train_test.shape[1]]   # \n",
    "    for window_size in window_sizes:\n",
    "        print('Window_Size is:', window_size)\n",
    "        len_ = int(train_test.shape[1] / window_size)\n",
    "        mean_cols = []\n",
    "        var_cols = []\n",
    "        min_cols = []\n",
    "        max_cols = []\n",
    "        kurt_cols = []\n",
    "        skew_cols = []\n",
    "        sum_cols = []\n",
    "        median_cols = [] \n",
    "        \n",
    "        diff_mean_cols = []\n",
    "        diff_var_cols = []\n",
    "        diff_max_cols = []\n",
    "        diff_min_cols = []\n",
    "        diff_skew_cols = []\n",
    "        diff_kurt_cols = []\n",
    "        \n",
    "        diff_mean_cols = []\n",
    "        diff_var_cols = []\n",
    "        diff_max_cols = []\n",
    "        diff_min_cols =[]\n",
    "        diff_skew_cols =[]   \n",
    "        diff_kurt_cols =[]   \n",
    "        \n",
    "        \n",
    "        diff_mean10_cols = []\n",
    "        diff_var10_cols = []\n",
    "        diff_max10_cols = []\n",
    "        diff_min10_cols =[]\n",
    "        diff_skew10_cols =[]   \n",
    "        diff_kurt10_cols =[]   \n",
    "        \n",
    "        second_cols = dict()\n",
    "        for i in range(len_): \n",
    "            # Original\n",
    "            tmp = train_test.iloc[:,i*window_size:(i+1)*window_size].copy() \n",
    "            # First Order\n",
    "            tmp_diff = train_test_diff.iloc[:,i*window_size:(i+1)*window_size].copy() \n",
    "            # If big than 0 or not\n",
    "            tmp_big_0 = train_test_big_0.iloc[:,i*window_size:(i+1)*window_size].copy() \n",
    "            # Original apply abs\n",
    "            tmp_abs = train_test_abs.iloc[:,i*window_size:(i+1)*window_size].copy()\n",
    "            # Original only with value greater than 0\n",
    "            tmp_abs_big_0 =  train_test_abs_big_0.iloc[:,i*window_size:(i+1)*window_size].copy()\n",
    "    \n",
    "            train_test_[str(i)+'_'+str(window_size)+'_mean'] = tmp.mean(axis=1).values\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_max'] = tmp.max(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].max(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_min'] = tmp.min(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].min(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_var'] = tmp.var(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            \n",
    "            train_test_[str(i)+'_'+str(window_size)+'_median'] = tmp.median(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].min(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_sum'] = tmp.sum(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_skew'] = tmp.skew(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].min(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_kurt'] = tmp.kurt(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            ## 1.New Add\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_range'] = train_test_[str(i)+'_'+str(window_size)+'_max'] - train_test_[str(i)+'_'+str(window_size)+'_min']\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_argmax'] = tmp.idxmax(axis=1).values #apply(lambda x:int(x.split('_')[-1])).values\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_argmin'] = tmp.idxmin(axis=1).values #.apply(lambda x:int(x.split('_')[-1])).values\n",
    "#             train_test_[str(i)+'_'+str(window_size)+'_abovemean'] = (tmp > train_test_[str(i)+'_'+str(window_size)+'_mean']).sum(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].min(axis=1)\n",
    "            ## \n",
    "            \n",
    "            train_test_[str(i)+'_'+str(window_size)+'_diffmean'] = tmp_diff.mean(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_diffvar'] = tmp_diff.var(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_diffmax'] = tmp_diff.max(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            \n",
    "            train_test_[str(i)+'_'+str(window_size)+'_diffmin'] = tmp_diff.min(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_diffskew'] = tmp_diff.skew(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_diffkurt'] = tmp_diff.kurt(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            ## 2.New Add\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_diffrange'] = train_test_[str(i)+'_'+str(window_size)+'_diffmax'] - train_test_[str(i)+'_'+str(window_size)+'_diffmin']\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_diffargmax'] = tmp_diff.idxmax(axis=1).values #.apply(lambda x:int(x.split('_')[-1])).values\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_diffargmin'] = tmp_diff.idxmin(axis=1).values #.apply(lambda x:int(x.split('_')[-1])).values\n",
    "#             train_test_[str(i)+'_'+str(window_size)+'_diffabovemean'] = (tmp_diff > train_test_[str(i)+'_'+str(window_size)+'_diffmean']).sum(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].min(axis=1)\n",
    "            ## \n",
    "            \n",
    "            ## 3.New Add\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_abovezero'] = tmp_big_0.sum(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].min(axis=1)\n",
    "            ## \n",
    "            \n",
    "            ## 4.New Add\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_absmean'] = tmp_abs.mean(axis=1).values\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_absmax'] = tmp_abs.max(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].max(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_absmin'] = tmp_abs.min(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].min(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_absvar'] = tmp_abs.var(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_absmedian'] = tmp_abs.median(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].min(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_abssum'] = tmp_abs.sum(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_absskew'] = tmp_abs.skew(axis=1).values#train.iloc[:,i*window_size:(i+1)*window_size].min(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_abskurt'] = tmp_abs.kurt(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            ## \n",
    "             \n",
    "            ## 5.New Add\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_abs_big_0_mean'] = tmp_abs_big_0.mean(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_abs_big_0_var'] = tmp_abs_big_0.var(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_abs_big_0_max'] = tmp_abs_big_0.max(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_abs_big_0_min'] = tmp_abs_big_0.min(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_abs_big_0_skew'] = tmp_abs_big_0.skew(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[str(i)+'_'+str(window_size)+'_abs_big_0_kurt'] = tmp_abs_big_0.kurt(axis=1).fillna(0).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            ## \n",
    "                \n",
    "            mean_cols.append(str(i)+'_'+str(window_size)+'_mean')\n",
    "            max_cols.append(str(i)+'_'+str(window_size)+'_max')\n",
    "            min_cols.append(str(i)+'_'+str(window_size)+'_min')\n",
    "            var_cols.append(str(i)+'_'+str(window_size)+'_var')\n",
    "            median_cols.append(str(i)+'_'+str(window_size)+'_median')\n",
    "            sum_cols.append(str(i)+'_'+str(window_size)+'_sum')\n",
    "            skew_cols.append(str(i)+'_'+str(window_size)+'_skew')\n",
    "            kurt_cols.append(str(i)+'_'+str(window_size)+'_kurt') \n",
    "            \n",
    "            diff_mean_cols.append(str(i)+'_'+str(window_size)+'_diffmean')\n",
    "            diff_var_cols.append(str(i)+'_'+str(window_size)+'_diffvar') \n",
    "            diff_max_cols.append(str(i)+'_'+str(window_size)+'_diffmax')\n",
    "            diff_min_cols.append(str(i)+'_'+str(window_size)+'_diffmin') \n",
    "            diff_skew_cols.append(str(i)+'_'+str(window_size)+'_diffskew')  \n",
    "            diff_kurt_cols.append(str(i)+'_'+str(window_size)+'_diffkurt') \n",
    "            \n",
    "            \n",
    "            diff_mean10_cols.append(str(i)+'_'+str(window_size)+'_abs_big_0_mean')\n",
    "            diff_var10_cols.append(str(i)+'_'+str(window_size)+'_abs_big_0_var') \n",
    "            diff_max10_cols.append(str(i)+'_'+str(window_size)+'_abs_big_0_max')\n",
    "            diff_min10_cols.append(str(i)+'_'+str(window_size)+'_abs_big_0_min') \n",
    "            diff_skew10_cols.append(str(i)+'_'+str(window_size)+'_abs_big_0_skew') \n",
    "            diff_kurt10_cols.append(str(i)+'_'+str(window_size)+'_abs_big_0_kurt') \n",
    "             \n",
    "        if window_size >= 1000:\n",
    "            continue\n",
    "            \n",
    "        second_cols[str(window_size)+'_mean'] = mean_cols\n",
    "        second_cols[str(window_size)+'_max'] = max_cols\n",
    "        second_cols[str(window_size)+'_min'] = min_cols\n",
    "        second_cols[str(window_size)+'_var'] = var_cols\n",
    "        second_cols[str(window_size)+'_median'] = median_cols\n",
    "        second_cols[str(window_size)+'_sum'] = sum_cols\n",
    "        second_cols[str(window_size)+'_skew'] = skew_cols \n",
    "        second_cols[str(window_size)+'_kurt'] = kurt_cols \n",
    "        \n",
    "        second_cols[str(window_size)+'_diff_mean'] = diff_mean_cols\n",
    "        second_cols[str(window_size)+'_diff_var'] = diff_var_cols\n",
    "        second_cols[str(window_size)+'_diff_max'] = diff_max_cols \n",
    "        second_cols[str(window_size)+'_diff_min'] = diff_min_cols\n",
    "        second_cols[str(window_size)+'_diff_skew'] = diff_skew_cols \n",
    "        second_cols[str(window_size)+'_diff_kurt'] = diff_kurt_cols\n",
    "        \n",
    "        second_cols[str(window_size)+'_abs_big_0_mean'] = diff_mean10_cols\n",
    "        second_cols[str(window_size)+'_abs_big_0_var'] = diff_var10_cols\n",
    "        second_cols[str(window_size)+'_abs_big_0_max'] = diff_max10_cols \n",
    "        second_cols[str(window_size)+'_abs_big_0_min'] = diff_min10_cols\n",
    "        second_cols[str(window_size)+'_abs_big_0_skew'] = diff_skew10_cols \n",
    "        second_cols[str(window_size)+'_abs_big_0_kurt'] = diff_kurt10_cols\n",
    "        \n",
    "        \n",
    "        for key in second_cols.keys():\n",
    "            cols = second_cols[key] \n",
    "            \n",
    "            train_test_[key+'_mean']  = train_test_[cols].mean(axis=1).values\n",
    "            train_test_[key+'_median']  = train_test_[cols].median(axis=1).values\n",
    "            train_test_[key+'_max']  = train_test_[cols].max(axis=1).values\n",
    "            train_test_[key+'_min']  = train_test_[cols].min(axis=1).values\n",
    "            train_test_[key+'_max_min']  = train_test_[key+'_max'] - train_test_[key+'_min']\n",
    "            train_test_[key+'_var']  = train_test_[cols].var(axis=1).values\n",
    "            train_test_[key+'_skew']  = train_test_[cols].skew(axis=1).values\n",
    "            train_test_[key+'_kurt']  = train_test_[cols].kurt(axis=1).values\n",
    "            train_test_[key+'_sum']  = train_test_[cols].sum(axis=1).values\n",
    "             \n",
    "            train_test_[key+'_diffmean'] = train_test_[cols].diff(1,axis=1).fillna(0).mean(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[key+'_diffvar'] = train_test_[cols].diff(1,axis=1).fillna(0).var(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[key+'_diffmax'] = train_test_[cols].diff(1,axis=1).fillna(0).max(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[key+'_diffmin'] = train_test_[cols].diff(1,axis=1).fillna(0).min(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[key+'_diffskew'] = train_test_[cols].diff(1,axis=1).fillna(0).skew(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "            train_test_[key+'_diffskurt'] = train_test_[cols].diff(1,axis=1).fillna(0).kurt(axis=1).values #train.iloc[:,i*window_size:(i+1)*window_size].var(axis=1)\n",
    "             \n",
    "    return train_test_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型\n",
    "\n",
    "我们采用最流行的LGB模型,XGBoost的训练速度较慢,考虑开销,我们选用XGBoost作为我们最终的方案.下面使我们算法获取最好效果的参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'task':'train',\n",
    "#     'boosting_type':'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class':4,\n",
    "    'min_data_in_leaf': 300,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.95,\n",
    "    'bagging_freq': 2,\n",
    "#     'metric': 'multi_logloss',\n",
    "    'max_bin':128,\n",
    "    'num_threads': 32\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  \n",
    "## 模型2：基于神经网络的模型\n",
    "\n",
    "### 数据预处理\n",
    "- 使用rank,然后使用Robust标准化(防止奇异值带来的影响)\n",
    "- 直接除以均值(感谢天池id为望着蓝天的海的分享以及推荐的文献[8])\n",
    "\n",
    "实验中,我们发现直接除以均值的效果要略好于使用rank的情况,所以最终我们选择使用直接除以均值的方式\n",
    "\n",
    "\n",
    "### 神经网络框架的设计\n",
    "- 参考了天池id为望着蓝天的海的开源代码(已经可以取得非常好的效果)\n",
    "\n",
    "- 参考NASNET论文中给出的最优框架进行设计（第一层加深）,文献[2]\n",
    "\n",
    "![](./pictures/NASNET.png)\n",
    "\n",
    "- 我们网络的模型框架，可以根据自己的计算资源调整第一层的大小\n",
    "\n",
    "![](./pictures/NN_Framework.png)\n",
    "\n",
    "上面是我们模型的设计架构，下面是我们的模型的代码以及验证效果(划出大概100000个样本作为验证集)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Convnet(input_shape=(2600,1,1),classes=4):\n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = Conv2D(32,(3,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input) \n",
    "    X = AveragePooling2D((3,1),strides=(3,1))(X) \n",
    "    X = Conv2D(32,(3,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X) \n",
    "    X = AveragePooling2D((3,1),strides=(3,1))(X)   \n",
    "    X = Conv2D(32,(3,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X) \n",
    "    X = AveragePooling2D((3,1),strides=(3,1))(X) \n",
    "    X = Conv2D(32,(3,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X) \n",
    "    X = AveragePooling2D((3,1),strides=(3,1))(X)\n",
    "    \n",
    "    X3 = Conv2D(32,(5,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input) \n",
    "    X3 = AveragePooling2D((3,1),strides=(3,1))(X3) \n",
    "    X3 = Conv2D(32,(5,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X3) \n",
    "    X3 = AveragePooling2D((3,1),strides=(3,1))(X3)   \n",
    "    X3 = Conv2D(32,(5,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X3) \n",
    "    X3 = AveragePooling2D((3,1),strides=(3,1))(X3) \n",
    "    X3 = Conv2D(32,(5,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X3) \n",
    "    X3 = AveragePooling2D((3,1),strides=(3,1))(X3)\n",
    "        \n",
    "    X1 = Conv2D(32,(7,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input) \n",
    "    X1 = AveragePooling2D((3,1),strides=(3,1))(X1) \n",
    "    X1 = Conv2D(32,(7,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X1) \n",
    "    X1 = AveragePooling2D((3,1),strides=(3,1))(X1)  \n",
    "    X1 = Conv2D(32,(7,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X1) \n",
    "    X1 = AveragePooling2D((3,1),strides=(3,1))(X1) \n",
    "    X1 = Conv2D(32,(7,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X1) \n",
    "    X1 = AveragePooling2D((3,1),strides=(3,1))(X1)   \n",
    "    \n",
    "    X4 = Conv2D(32,(9,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input) \n",
    "    X4 = AveragePooling2D((3,1),strides=(3,1))(X4) \n",
    "    X4 = Conv2D(32,(9,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X4) \n",
    "    X4 = AveragePooling2D((3,1),strides=(3,1))(X4)   \n",
    "    X4 = Conv2D(32,(9,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X4) \n",
    "    X4 = AveragePooling2D((3,1),strides=(3,1))(X4) \n",
    "    X4 = Conv2D(32,(9,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X4) \n",
    "    X4 = AveragePooling2D((3,1),strides=(3,1))(X4)\n",
    "    \n",
    "    X2 = Conv2D(32,(11,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input) \n",
    "    X2 = AveragePooling2D((3,1),strides=(3,1))(X2) \n",
    "    X2 = Conv2D(32,(11,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X2) \n",
    "    X2 = AveragePooling2D((3,1),strides=(3,1))(X2)   \n",
    "    X2 = Conv2D(32,(11,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X2) \n",
    "    X2 = AveragePooling2D((3,1),strides=(3,1))(X2) \n",
    "    X2 = Conv2D(32,(11,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X2) \n",
    "    X2 = AveragePooling2D((3,1),strides=(3,1))(X2) \n",
    "    \n",
    "    \n",
    "    X5 = Conv2D(32,(13,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input) \n",
    "    X5 = AveragePooling2D((3,1),strides=(3,1))(X5) \n",
    "    X5 = Conv2D(32,(13,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X5) \n",
    "    X5 = AveragePooling2D((3,1),strides=(3,1))(X5)   \n",
    "    X5 = Conv2D(32,(13,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X5) \n",
    "    X5 = AveragePooling2D((3,1),strides=(3,1))(X5) \n",
    "    X5 = Conv2D(32,(13,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X5) \n",
    "    X5 = AveragePooling2D((3,1),strides=(3,1))(X5) \n",
    "    \n",
    "    \n",
    "    X6 = Conv2D(32,(15,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input) \n",
    "    X6 = AveragePooling2D((3,1),strides=(3,1))(X6) \n",
    "    X6 = Conv2D(32,(15,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X6) \n",
    "    X6 = AveragePooling2D((3,1),strides=(3,1))(X6)   \n",
    "    X6 = Conv2D(32,(15,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X6) \n",
    "    X6 = AveragePooling2D((3,1),strides=(3,1))(X6) \n",
    "    X6 = Conv2D(32,(15,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X6) \n",
    "    X6 = AveragePooling2D((3,1),strides=(3,1))(X6) \n",
    "    \n",
    "    X7 = Conv2D(32,(17,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X_input) \n",
    "    X7 = AveragePooling2D((3,1),strides=(3,1))(X7) \n",
    "    X7 = Conv2D(32,(17,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X7) \n",
    "    X7 = AveragePooling2D((3,1),strides=(3,1))(X7)   \n",
    "    X7 = Conv2D(32,(17,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X7) \n",
    "    X7 = AveragePooling2D((3,1),strides=(3,1))(X7) \n",
    "    X7 = Conv2D(32,(17,1),strides=(1,1),padding='same',activation='relu',kernel_initializer=glorot_uniform(seed=921212))(X7) \n",
    "    X7 = AveragePooling2D((3,1),strides=(3,1))(X7)\n",
    "    \n",
    "    X = concatenate([X,X1,X2,X3,X4,X5,X6,X7],axis=3)    \n",
    "    \n",
    "    X = Flatten()(X)  \n",
    "    X = Dense(1024, activation='relu', kernel_initializer=glorot_uniform(seed=921212))(X)\n",
    "    X = Dropout(0.6)(X)\n",
    "    X = Dense(512, activation='relu', kernel_initializer=glorot_uniform(seed=921212))(X)\n",
    "    X = Dropout(0.6)(X) \n",
    "    X = Dense(256, activation='tanh', kernel_initializer=glorot_uniform(seed=921212))(X)\n",
    "    X = Dropout(0.6)(X) \n",
    "    X = Dense(classes, activation='softmax', kernel_initializer=glorot_uniform(seed=921212))(X) \n",
    "    model = Model(inputs=X_input,outputs=X,name='Convnet') \n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./pictures/validation_score.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进一步提升MicroFscore\n",
    "\n",
    "神经网络单模型的结果已经非常高了,但是神经网络经常会有波动比较大情况,我们为了获得更好更加鲁棒的结果,采用了如下的几种提升方式.\n",
    "\n",
    "### 转化为求解代价敏感问题 \n",
    "\n",
    "#### 问题转化\n",
    "\n",
    "- 将求解MicroF score的问题转化为代价敏感的分类问题进行求解\n",
    "\n",
    "> 因为论文中的算法要使用for循环来选取效果最好的,这样代价非常大,虽然有理论保证，但是我们无法保证在验证集上取得最好的结果在测试集上也能取得最好的结果,而如果每次用线上来测结果,这样对于最后只有两次提交机会的我们来说,赌博性质太强。\n",
    "\n",
    "于是我们转而选择采用集成的方式来缓解这种方法带来的弊端，我们对不同类别的数据设置不同的权重,然后对所有的这些权重进行模型训练，然后对训练好的模型的预测结果进行均值集成,保证我们的模型的鲁棒性，不会因为某一个较差的结果所影响,而实验也发现，这样的方式会有很大概率(90%)以上的概率获得比所有结果都要好的结果。\n",
    "\n",
    "#### 我们的发现以及新方案\n",
    "此外，<font color=red>我们基于对数据预测结果的观察，提出了一种非常好的处理类别不平衡问题的方法</font>。\n",
    "\n",
    "观察结果:\n",
    "1. 不进行加权的预测结果类别较少的样本可以获得非常好的Precision但是Recall较低；\n",
    "    \n",
    "2. 进行加权的预测结果类别较少的样本可以获得非常好的Recall但是Precision较低；\n",
    "\n",
    "我们提出的方法:\n",
    "\n",
    "- 对加权和不加权的预测结果进行集成用来充分利用这种观测信息,这种情况我们得到的结果都较好的提升了我们模型的结果。\n",
    "\n",
    "### 模型集成细节\n",
    "\n",
    "为了提升模型的鲁棒性同时为了提升模型的性能,我们采用了模型的集成的方式,本次比赛中，我们一共采用如下的两种集成方式.\n",
    "\n",
    "#### 模型内部集成\n",
    "> 因为神经网络的训练代价较大，为了能用最少的时间来得到最好最鲁棒的结果，我们对我们单个神经网络进行内部集成，通过内部集成，我们的单模型的效果都往往接近模型的最好的结果,细节如下:\n",
    "\n",
    "> ![](./pictures/inner_ensemble.png)\n",
    "\n",
    "#### 模型间集成\n",
    "\n",
    "这个是基于我们的发现的，细节可以参考我们的发现以及新方案部分。下图是我们复赛2的示意图（PS:决赛因为机器原因,只训练了4个模型）：\n",
    "\n",
    "> ![](./pictures/outer_ensemble.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三轮比赛的结果\n",
    "\n",
    "## 初赛(基于LGB的方案)\n",
    "\n",
    "![](./pictures/game1.png)\n",
    "\n",
    "## 复赛(基于LGB+Deep+集成的方案)\n",
    "\n",
    "![](./pictures/game2.png)\n",
    "\n",
    "## 决赛(基于Deep的方案)\n",
    "\n",
    "![](./pictures/game3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 比赛经验总结和感想\n",
    "\n",
    "## 比赛总结\n",
    "\n",
    "### 方案优点\n",
    "\n",
    "1. 多方案：我们采用了基于传统方案的LGB以及基于深度的方法,并且都取得了不错的效果\n",
    "2. 模型的鲁棒性强，性能好：我们模型的线上线下得分基本保持一致，线下的结果略好于线上的结果 \n",
    "3. 创新性强\n",
    "> 创新1：修改了论文[4]中的方法，采用集成的方法提供了一种泛化性能更好的方案；<br />\n",
    "> 创新2：通过观测验证集上每个类别的Precision以及Recall的预测结果，我们对不加权和加权的预测结果进行加权集成充分利用这种信息\n",
    " \n",
    "\n",
    "## 方案改进\n",
    "\n",
    "1. 设计MicroFscore的近似可微函数，直接用到神经网络中；\n",
    "2. 设计更好的神经网络结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "1. Elad ET. Eban, Mariano Schain, Alan Mackey, Ariel Gordon, Rif A. Saurous, Gal Elidan. Scalable Learning of Non-Decomposable Objectives. AISTATS,2017\n",
    "\n",
    "2. Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le.Learning Transferable Architectures for Scalable Image Recognition.arXiv,2017\n",
    "\n",
    "3.  Shameem Puthiya Parambath, Nicolas Usunier,and Yves Grandvalet. Optimizing f-measures by cost-sensitive classification.NIPS，2123-2131, 2014.\n",
    "\n",
    "4.  Zachary Chase Lipton, Charles Elkan, and Balakrishnan Narayanaswamy. Thresholding classifiers to maximize f1 score. arXiv, 2014.\n",
    "\n",
    "5. Zachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy,Author information,Copyright and License information,DisclaimerOptimal Thresholding of Classifiers to Maximize F1 Measure,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4442797/\n",
    "\n",
    "6. Shameem A. Puthiya Parambath,Nicolas Usunier,Yves Grandvalet. Optimizing Pseudo-Linear Performance Measures:Application to F-measure. arXiv,2015\n",
    "\n",
    "7. 初赛A榜0.81的解决方案:https://tianchi.aliyun.com/forum/new_articleDetail.html?spm=5176.8366600.0.0.2717311fEdKTIF&raceId=231646&postsId=4508\n",
    "\n",
    "8. 李乡儒,刘中田,胡占义,吴福朝,赵永恒.巡天光谱分类前的预处理——流量标准化 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "744px",
    "left": "0px",
    "right": "1092.8px",
    "top": "67px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
