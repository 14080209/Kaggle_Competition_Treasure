{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#简介\" data-toc-modified-id=\"简介-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>简介</a></span></li><li><span><a href=\"#词频特征\" data-toc-modified-id=\"词频特征-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>词频特征</a></span><ul class=\"toc-item\"><li><span><a href=\"#一些概念\" data-toc-modified-id=\"一些概念-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>一些概念</a></span></li><li><span><a href=\"#数学解释\" data-toc-modified-id=\"数学解释-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>数学解释</a></span></li><li><span><a href=\"#基于词频的常见特征工程思路\" data-toc-modified-id=\"基于词频的常见特征工程思路-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span><font color=\"red\">基于词频的常见特征工程思路</font></a></span><ul class=\"toc-item\"><li><span><a href=\"#字典较小\" data-toc-modified-id=\"字典较小-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>字典较小</a></span></li><li><span><a href=\"#字典较大\" data-toc-modified-id=\"字典较大-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>字典较大</a></span></li></ul></li><li><span><a href=\"#纯词频特征的优缺点\" data-toc-modified-id=\"纯词频特征的优缺点-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>纯词频特征的优缺点</a></span><ul class=\"toc-item\"><li><span><a href=\"#优点\" data-toc-modified-id=\"优点-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>优点</a></span></li><li><span><a href=\"#缺点\" data-toc-modified-id=\"缺点-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>缺点</a></span></li></ul></li><li><span><a href=\"#Python包的使用例子\" data-toc-modified-id=\"Python包的使用例子-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Python包的使用例子</a></span></li></ul></li><li><span><a href=\"#TF-IDF特征\" data-toc-modified-id=\"TF-IDF特征-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TF-IDF特征</a></span><ul class=\"toc-item\"><li><span><a href=\"#IDF(Inverse-Document-Frequency)的定义\" data-toc-modified-id=\"IDF(Inverse-Document-Frequency)的定义-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>IDF(Inverse Document Frequency)的定义</a></span></li><li><span><a href=\"#TF-IDF计算\" data-toc-modified-id=\"TF-IDF计算-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>TF-IDF计算</a></span></li><li><span><a href=\"#例子\" data-toc-modified-id=\"例子-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>例子</a></span></li><li><span><a href=\"#Python包的使用例子\" data-toc-modified-id=\"Python包的使用例子-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Python包的使用例子</a></span><ul class=\"toc-item\"><li><span><a href=\"#CountVectorizer-+-TfidfVectorizer\" data-toc-modified-id=\"CountVectorizer-+-TfidfVectorizer-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;</span>CountVectorizer + TfidfVectorizer</a></span></li><li><span><a href=\"#TfidfVectorizer\" data-toc-modified-id=\"TfidfVectorizer-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;</span>TfidfVectorizer</a></span></li></ul></li><li><span><a href=\"#-基于TF-IDF的特征工程\" data-toc-modified-id=\"-基于TF-IDF的特征工程-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span><font color=\"red\"> 基于TF-IDF的特征工程</font></a></span></li><li><span><a href=\"#TF-IDF特征的优缺点\" data-toc-modified-id=\"TF-IDF特征的优缺点-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>TF-IDF特征的优缺点</a></span><ul class=\"toc-item\"><li><span><a href=\"#优点:\" data-toc-modified-id=\"优点:-3.6.1\"><span class=\"toc-item-num\">3.6.1&nbsp;&nbsp;</span>优点:</a></span></li><li><span><a href=\"#缺点\" data-toc-modified-id=\"缺点-3.6.2\"><span class=\"toc-item-num\">3.6.2&nbsp;&nbsp;</span>缺点</a></span></li></ul></li></ul></li><li><span><a href=\"#参考文献\" data-toc-modified-id=\"参考文献-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>参考文献</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 简介\n",
    "\n",
    "大数据趋势越来越火，人工智能技术的飞跃发展，我们之前很多难以解决的问题也渐渐可以被处理，并且取得了巨大的成功，例如文本类的数据，在20多年前几乎没几个人认为机器能理解文本中的信息，但是现在，随着很多新兴技术的发展，已经没多少人认为机器没法理解文中的信息了......如果从数据竞赛的趋势来看的话，我们也发现近期很多比赛都涉及到了文本的处理，而且这类比赛越来越多，奖金也非常诱人，不过作为一个小白，该如何开始处理文本类的问题呢？这篇Notebook着重介绍几种常见的文本特征的提取方法，并阐述他们之间的联系以及优缺点，方便大家在碰到不同问题时这些工具包的选取。本篇文章的内容主要包括下面几块内容：\n",
    "1. 词频特征（CountVectorizer）\n",
    "2. TF-IDF特征\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词频特征\n",
    "## 一些概念\n",
    "\n",
    "\n",
    "**1.词频（Term Frequency）**，顾名思义就是某个词出现的频率。\n",
    "\n",
    "一篇文章是由大量的单词所组成的，而每个单词出现的次数在一定程度上又可以从侧面反映该文章的内容，例如在谋篇文章中，\"love\"这个词出现的比较多,也就是说\"love\"对应的词频比较大，则我们可以猜测该文章很大可能属于情感类的文章。<br />\n",
    "\n",
    "**2.字典(Dictionary)**: 字典的理解也很简单，就是包含了所有词汇的一本小本子，对应到文本处理问题中，字典就是指包含了所有文件中**所有词汇**的一个字典(dict)，假设我们有两个文本文件，第一个文本是\"I love the the boy.\", 第二个文本文件内容是\"The girl is beautiful\",则这两个文本文件对应的字典就是[\"I\",\"love\",'the\",\"boy\",\"girl\",\"is\",\"beautiful\"],该词典中就包含了上述两个文本中的所有单词。<br />\n",
    "\n",
    "\n",
    "**3.词频向量(Term Vector)**: 向量上过高中的肯定都知道,数学上来说：一个$n$维度的向量可以表示为$\\bar{x} = (x_1,x_2,...,x_n)$, 其中$x_i$表示为第$i$个元素。而对应到文本处理中，向量的长度就是字典的大小，而每个文本则对应一个向量，例如上面的例子,字典大小是7,则第一个文本对应的向量表示就是[1,1,2,1,0,0,0],第二个文本对应的向量表示就是[0,0,1,0,1,1,1]，我们发现每个文本都被形式化成为了一个向量。<br />\n",
    "\n",
    "## 数学解释\n",
    "上面的概念介绍是一些简单的直观解释，因为在大量的建模问题中，我们需要给计算机输入的是数值特征，而不是直接的文本信息，所以此处我们将上面的一些文本信息表示为数学的形式。\n",
    "\n",
    "假设我们的字典大小为$N$,即我们所有的文本由$N$个单词组成,则我们每一个文本$(doc_x)$对应的向量为$doc_x = (fre_1, fre_2,...,fre_N)$,$fre_i$表示第$i$个词汇在文件$doc_x$中出现的次数,$fre_i = \\Sigma_{j \\in doc_x}  1(j =i )$，表示单词$i$出现的次数。\n",
    "\n",
    "\n",
    "\n",
    "## <font color=red>基于词频的常见特征工程思路</font>\n",
    "由上面的介绍我们知道每一个文本文件都被<font color=red>宏观</font>上表示为了字典对应的向量,那么这样我们便可以对文本进行下面的特征工程。\n",
    "\n",
    "### 字典较小\n",
    "当字典较小的时候：<br />\n",
    "1. 我们可以直接把每个文件对应的向量当特征进行拼接。\n",
    "\n",
    "### 字典较大\n",
    "当字典较大的时候,我们没法直接进行拼接操作,此时我们可以：<br />\n",
    "1. 对向量提取统计特征,包括均值,方差,skewness之类的特征，以此捕获文本中词汇的分布情况的信息;\n",
    "2. 对向量进行聚类，来捕获文件之间的相似性;\n",
    "3. 提取K近邻特征;\n",
    "4. 统计向量中的top5的高频词汇的id\n",
    "5. 其他\n",
    "\n",
    "## 纯词频特征的优缺点\n",
    "\n",
    "\n",
    "### 优点\n",
    "1. 从上面的分析我们可以发现,词频对应的文本特征简单而且易于理解，能够从宏观的角度捕获文本的信息。\n",
    "2. 能提取很多有用的信息特征，聚类特征和统计特征往往能对模型带来一定的提升;\n",
    "\n",
    "### 缺点\n",
    "1. 词频特征往往会受到停止词汇的影响(stop words)，例如\"the,a\"出现次数往往较多,这在聚类的时候如果选用了错误的聚类距离,例如l2距离等，则往往难以获得较好的聚类效果,所以需要细心的进行停止词汇的删选;\n",
    "2. 受文本大小的影响,如果文章比较长,则词汇较多,文本较短,词汇则会较少,如果需要解决此类问题，可以采用下面的策略:\n",
    "> **对文本进行归一化 + 将文本的大小作为特征(弥补文章较长的信息)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python包的使用例子\n",
    "下面是一个python的例子，很简单，此处就不再进行解释了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入工具包\n",
    "from sklearn.feature_extraction.text import CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化，并引入停止词汇\n",
    "vectorizer = CountVectorizer(stop_words=set(['the', 'six', 'less', 'being', 'indeed', 'over', 'move', 'anyway', 'four', 'not', 'own', 'through', 'yourselves']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集 & 测试集\n",
    "train_set = [\"The sky is blue.\", \"The sun is bright.\"]\n",
    "test_set = (\"The sun in the sky is bright.\", \"We can see the shining sun, the bright sun.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blue': 0, 'bright': 1, 'is': 2, 'sky': 3, 'sun': 4}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取词汇\n",
    "vectorizer.fit_transform(train_set)\n",
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 4)\t2\n"
     ]
    }
   ],
   "source": [
    "# 获取向量表示\n",
    "smatrix = vectorizer.transform(test_set)\n",
    "print(smatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF特征\n",
    "上一小节介绍了词频特征的相关信息,词频特征可以从宏观的方面表示文本的信息，但是词频方法因为将频繁的词汇的作用放大了，例如常见的\"I\",'the\"等，将稀有的词汇,例如\"garden\",\"tiger\"的作用缩减了，而这些单词却有着极为重要的信息量，所以词频特征往往很难捕获一些重点的细节信息。\n",
    "\n",
    "本节我们就介绍一种经典的缓解此类问题的方法,也是我们最基本的TF-IDF特征提取方法。** TF-IDF从全局（所有文件）和局部（单个文件）的角度来解决上述问题**，TF-IDF可以更好地给出某个单词对于某个文件的重要性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF(Inverse Document Frequency)的定义\n",
    "某一个单词的idf的常用计算公式为： <font color=red>$ idf(t) = log \\frac{|D|}{1 + \\{d: t \\in d\\}}$,</font>\n",
    "\n",
    "- $t$: 表示一个单词; $d$ 表示某一个文件; $\\{d: t \\in d\\}$表示文件中出现单词t的文件数量;\n",
    "- 其中$|D|$表示所有的文件数量, \n",
    "\n",
    "这边我们加1是为了防止分母为0，此处的log可以修改，我们认为一个单词出现在1个文件中并不会带来10倍的重要性（相比于只出现在10个单词只出现在一个文件中），所以**此处用了log来进行平滑，当然可以换成其他的平滑方式**。\n",
    "\n",
    "\n",
    "## TF-IDF计算\n",
    "上面我们给了IDF的定义以及它的合理性解释，此处我们给出TF-IDF的定义以及含义。TF-IDF的计算公式为：<font color=red>$ tf$-$idf(t) = tf(t,d) * idf(t)$,</font>，其中：\n",
    "- <font color=red>$ idf(t) = log \\frac{|D|}{1 + \\{d: t \\in d\\}}$,</font>\n",
    "- <font color=red>$ tf(t,d) = \\Sigma_{x \\in d} fr(x,t)$,</font> 其中,$fr(x,t) = 1$ when $x =t$, else 0.\n",
    "\n",
    "所以TF-IDF直观上就是<font color=blue>一个单词在某个文本中出现的次数（TF,局部）</font>以及该<font color=blue>单词在所有文本中出现的次数的计算得到的权重(IDF,全局)</font>的乘积。\n",
    "\n",
    "## 例子\n",
    "假设训练集中有两个文本,d1 & d2，其中：<br />\n",
    "d1: The sky is blue.<br />\n",
    "d2: The sun is bright.\n",
    "\n",
    "测试集中有两个文本d3 & d4,其中:<br />\n",
    "d3: The sun in the sky is bright.<br />\n",
    "d4: We can see the shining sun, the bright sun.\n",
    "\n",
    "1. 我们将The，is作为stop words删除,并得到我们的字典:['blue','sun','bright','sky']\n",
    "2. 获得我们测试集的TF表示,此处为：\n",
    "![](./pic/1.png) \n",
    "3. 计算得到所有的idf值:\n",
    "![](./pic/2_1.png) \n",
    "![](./pic/2_2.png) \n",
    "![](./pic/2_3.png) \n",
    "![](./pic/2_4.png) \n",
    "于是我们的idf为：\n",
    "![](./pic/3.png)\n",
    "将idf对角化：\n",
    "![](./pic/4.png)\n",
    "4. 计算TF-IDF值\n",
    "![](./pic/5.png)\n",
    "![](./pic/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python包的使用例子\n",
    "我们将idf的计算转化为python包的例子,注意python中的idf计算公式略有不同，具体地已经标注在了下面\n",
    "### CountVectorizer + TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'sky': 2, 'blue': 0, 'sun': 3, 'bright': 1}\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集中的字典\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "train_set = (\"The sky is blue.\", \"The sun is bright.\")\n",
    "test_set = (\"The sun in the sky is bright.\",\"We can see the shining sun, the bright sun.\")\n",
    "count_vectorizer = CountVectorizer(stop_words=['is','The','the'])\n",
    "count_vectorizer.fit_transform(train_set)\n",
    "print( \"Vocabulary:\", count_vectorizer.vocabulary_)\n",
    "# Vocabulary: {'blue': 0, 'sun': 1, 'bright': 2, 'sky': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1]\n",
      " [0 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# 得到词频矩阵\n",
    "freq_term_matrix = count_vectorizer.transform(test_set)\n",
    "print(freq_term_matrix.todense() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF: [ 2.09861229  1.          1.40546511  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# 注意：在最新的sklearn中idf是\n",
    "# smooth_idf = Talse, log(N / df) + 1，所以只要同时出现的话,就是1;  \n",
    "# smooth_idf = True, idf(d, t) = log [ (1 + n) / (1 + df(d, t)) ] + 1.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(norm=\"l2\")\n",
    "tfidf.fit(freq_term_matrix)\n",
    "print(\"IDF:\", tfidf.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.50154891  0.70490949  0.50154891]\n",
      " [ 0.          0.4472136   0.          0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_matrix = tfidf.transform(freq_term_matrix)\n",
    "print(tf_idf_matrix.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "上面的形式可以直接用TfidfVectorizer对文本进行处理,处理的过程是类似的,具体地，大家可以查看相应的工具包即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red> 基于TF-IDF的特征工程</font>\n",
    "\n",
    "基于TF-IDF的特征工程和词频部分的类似，不同之处在于换成了新的向量。当然最新的kaggle上也会对TF-IDF特征关于label直接做oof特征,这个也是在做数据竞赛时可以选择的一点."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF特征的优缺点\n",
    "\n",
    "### 优点:\n",
    "1. TF-IDF特征在很大程度上缓解了之前的词频特征的缺点，降低了频繁词汇的特征重要性，提高了出现次数少的词汇的重要性,使得新的特征更加关注出现次数少的特征。\n",
    "2. 在很多文本分类等的数据集上的表现效果非常好.\n",
    "\n",
    "### 缺点\n",
    "1. 忽略了文章的内容，词汇之间的联系,虽然可以通过N-Gram的方式进行缓解,但是依然没有从本质上解决该问题;\n",
    "2. idf的作为单词的权重的合理性,因为idf的计算方式可以有很多,sklearn中的是一种选择,但是并非是最合理的."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "1. https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "2. Machine Learning :: Text feature extraction (tf-idf) – Part I：http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/\n",
    "3. Machine Learning :: Text feature extraction (tf-idf) – Part II：http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/\n",
    "4. How to process textual data using TF-IDF in Python: https://medium.freecodecamp.org/how-to-process-textual-data-using-tf-idf-in-python-cd2bbc0a94a3\n",
    "5. TfidfVectorizer:http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "6. http://www.tfidf.com/\n",
    "7. TF-IDF的实现:https://code.google.com/archive/p/tfidf/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
